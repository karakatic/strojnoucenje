{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Domov"},{"location":"pages/onama/","text":"O nama fnkdslfasd","title":"Avtorja"},{"location":"pages/onama/#o-nama","text":"fnkdslfasd","title":"O nama"},{"location":"pages/knjiga/01_uvod/","text":"Uvod Strojno u\u010denje, umetna inteligenca, podatkovna znanost, podatkovno rudarjenje in velepodatki (ali kar veliki podatki). V zadnjih letih so to precej uporabljeni izrazi, ki jih zasledimo v novicah o revolucionarnih premikih na podro\u010djih obdelave podatkov, razpoznave slik ter videoposnetkov in obdelave besedila. S temi izrazi podjetniki zvito opi\u0161ejo svoje storitve in produkte, da jim dvignejo vrednost ali jih la\u017eje prodajo. S temi izrazi razvijalci programske opreme opi\u0161ejo svoje izdelke, ko so se ti zmo\u017eni prilagajati na razli\u010dne situacije. Pa gre res za revolucionarne metode ali le za trenutno modo oziroma modne besede (angl. buzzwords )? So algoritmi strojnega u\u010denja res nekaj, kar izra\u017ea inteligentnost ra\u010dunalnika, ali pa gre le za kombinacijo if stavkov in statisti\u010dnih metod? \u017dal ni enovitega odgovora. Inteligentnega in samostojnega stroja, kot so prikazani v nekaterih filmskih uspe\u0161nicah, ne moremo pri\u010dakovati \u0161e nekaj \u010dasa, \u010de sploh kdaj. Vsekakor pa imajo pristopi strojnega u\u010denja vrednost, kar ka\u017ee tudi vztrajnost zanimanja in njihove uporabe tako v industriji kakor tudi raziskovalni sferi. Strojno u\u010denje je danes klju\u010den sestavni del \u0161tevilnih komercialnih informacijskih sistemov in raziskovalnih projektov na razli\u010dnih podro\u010djih - od medicinske diagnoze in zdravljenja 1 , avtomatskega trgovanja z vrednostnimi papirji 2 , samovoze\u010dih avtomobilov 3 , pa do priporo\u010dil na dru\u017ebenih omre\u017ejih in spletnih trgovinah 4 . Mnogi menijo, da je uporaba strojnega u\u010denja primerna za uporabo le v velikih podjetjih z obse\u017enimi raziskovalnimi skupinami in globokimi \u017eepi. S to knjigo ti avtorja \u017eeliva pokazati, kako enostavna je uporaba strojnega u\u010denja, \u010de le \u017ee pozna\u0161 osnove programiranja. Kot na drugih podro\u010djih, tudi pri strojnem u\u010denju obstajajo tako kompleksni in napredni pristopi, kakor tudi enostavni. Za razumevanje najbolj naprednih pristopov (na primer globokega u\u010denja (angl. deep learning )) res potrebujemo \u017ee kar nekaj semestrov matematike. Po drugi strani pa razumevanje najenostavnej\u0161ih pristopov zahteva le izku\u0161nje iz programiranja. Seveda se najve\u010d govori o najbolj naprednih metodah, ki terjajo ekipo dvestotih podatkovnih znanstvenikov (kot se imenujemo) in za dve ko\u0161arkarski dvorani velik ra\u010dunalnik. V resnici pa ve\u010dji del informacijskih sistemov, ki so podprti s strojnim u\u010denjem, uporablja zelo elementarne tehnike. Vsakdo pa \u017ee ne potrebuje naprednega algoritma, ki identificira \u010dloveka iz treh pikslov. V ve\u010dji meri so za obogatitev obstoje\u010dih sistemov dovolj enostavni pristopi, kar je razvidno iz uporabe umetne inteligence v praksi. Nova storitev, katere razvijalci poudarjajo, da je podprta z naprednimi pristopi umetne inteligence, res nima le nekaj pogojnih if -ov in osnovnih opisno statisti\u010dnih izra\u010dunov, kljub temu pa navadno ni tako kompleksna, kot se mogo\u010de zdi. Komu je ta knjiga namenjena? Ta knjiga slu\u017ei kot uvod v podro\u010dje strojnega u\u010denja. Namenjena je tistim, ki \u017ee znajo programirati - idealno v programskem jeziku Python, saj so primeri prikazani z uporabo tega jezika. Zakaj Python? Ker je to eden izmed najbolj uporabljenih programskih jezikov za namen uporabe strojnega u\u010denja. Knjiga se osredoto\u010da na uporabo Pythona in knji\u017enice scikit-learn , ki je osnovna knji\u017enica za uporabo strojnega u\u010denja. Seveda se ne gre izogniti uporabi knji\u017enic NumPy in Matplotlib oz. pandas in seaborn - ampak poudarek vsekakor ni na teh. Zavestno je bila narejena odlo\u010ditev, da se knjiga preve\u010d ne osredoto\u010da na matemati\u010dni vidik strojnega u\u010denja. Dr\u017ei, matemati\u010dne formule so \u0161e vedno prisotne, ampak le do te mere, da pomagajo (ne pa ovirajo) pri razumevanju posameznih pristopov. Knjiga se osredoto\u010di le na eno tehniko strojnega u\u010denja - na klasifikacijo. Klasifikacija se predstavi na le enem, po mnenju avtorjev, najbolj intuitivnem algoritmu klasifikacije, imenovanemu k najbli\u017ejih sosedov. Ta algoritem je predstavljen nekoliko bolj podrobno, s \u010dimer se ponazori, da ima vsak pristop strojnega u\u010denja svoje posebnosti, ki ga naredijo bodisi primernega ali neprimernega za dan problem. Komu ta knjiga ni namenjena? V knjigi avtorja okvirno predstaviva razli\u010dne tehnike in podro\u010dja strojnega u\u010denja - podrobnosti pa izpustiva. \u010ce te zanima regresija, katera izmed tehnik nenadzorovanega u\u010denja ali delo z globokimi nevronskimi mre\u017eami, ta knjiga ni zate. Vsak dober podatkovni znanstvenik je najprej spoznaval temelje strojnega u\u010denja ter se \u0161ele potem posvetil naprednej\u0161im tehnikam. \u010cemu nam bo napredna globoka nevronska mre\u017ea, \u010de pa je ne znamo pravilno ovrednotiti? Knjiga je napisana tako, da bo samostojno nadaljevanje u\u010denja kar se da enostavno. V knjigi niso omenjene vse mo\u017ene knji\u017enice strojnega u\u010denja v Pythonu ali mo\u017enosti uporabe strojnega u\u010denja v drugih programskih jezikih. \u010cetudi te delo na strojnem u\u010denju v Pythonu zanese k uporabi PyTorcha ali TensorFlowa , je scikit-learn osnova, brez katere ne gre. \u010cetudi te bosta delodajalec ali lastna radovednost v prihodnosti usmerila v uporabo strojnega u\u010denja v drugih jezikih, so tudi ostale knji\u017enice strojnega u\u010denja spisane po zgledu knji\u017enice scikit-learn . Po povr\u0161nem pregledu knjige bi kdo morda pri\u0161el do zaklju\u010dka, da knjiga pokrije snov, ki bi jo lahko predstavili v eni objavi na blogu. Dr\u017ei - \u010de bi snov, predstavljeno v tej knjigi, strnili v programsko kodo, to ne bi bil kompleksen informacijski sistem, temve\u010d le dalj\u0161a Python skripta. Namen knjige ni, da predstavi \u010dim ve\u010d razli\u010dnih na\u010dinov uporabe knji\u017enice scikit-learn in predela vse mo\u017ene algoritme v tej - temu namre\u010d slu\u017ei dokumentacija te knji\u017enice. Skozi branje se od bralca te knjige pri\u010dakuje, da vsak korak (oziroma vsako vrstico kode) razume - kaj se zgodi in \u010demu je namenjena. Skozi poglobljeno razumevanje ti avtorja \u017eeliva predati sposobnost nadaljnjega samostojnega u\u010denja - kaj je pomembno pri dolo\u010denem pristopu, kako se uporabi ta pristop, kako nastavitve vplivajo na rezultate in tako naprej. Povedano druga\u010de, s knjigo te avtorja \u017eeliva nau\u010diti samostojnega u\u010denja snovi strojnega u\u010denja. Struktura knjige in kje za\u010deti Uvodu sledijo \u0161tiri poglavja. Poglavje 2 govori o vzpostavitvi okolja, ki bo primerno za uporabo knji\u017enic, ki jih knjiga vklju\u010duje, za zagon primerov iz knjige in za re\u0161evanje prakti\u010dnih nalog. \u010ce ima\u0161 Python okolje \u017ee vzpostavljeno na svojem ra\u010dunalniku, lahko to poglavje presko\u010di\u0161. TODO Poglavje 3 za\u010dne s splo\u0161no razlago definicije strojnega u\u010denja in vzpostavi vzporednice tega z na\u0161im (\u010dlove\u0161kim) u\u010denjem novega znanja. Ve\u010dji del poglavja je namenjen predstavitvi glavne tehnike strojnega u\u010denja v tej knjigi - klasifikaciji podatkov. Opisi in definicije gredo od najenostavnej\u0161e razlage na primerih pa do formalne matemati\u010dne definicije pojmov. To poglavje presko\u010di, \u010de si z osnovnimi tehnikami \u017ee seznanjen/-a in bi se \u017eelel/-a hitro posvetiti programiranju. Poglavje 4 obravnava izbran algoritem klasifikacije v tej knjigi - k najbli\u017ejih sosedov. Algoritem je najprej predstavljen neodvisno od programiranja na na\u010din, ki ti bo omogo\u010dal, da ga zna\u0161 re\u0161iti tudi na roko. Predstavljene so nekatere nastavitve tega algoritma, s \u010dimer se predstavi pomembnost poznavanja podrobnosti algoritma, da je uporaba tega kar se da u\u010dinkovita. Opisu in definicijam pa sledijo primeri v programski kodi. Ti primeri so napisani tako, da se lahko neposredno uporabijo pri testiranju in spoznavanju. Za utrjevanje znanja iz tega poglavja so prilo\u017eene tudi \u0161tiri naloge - dve preverjata teoreti\u010dno razumevanje in se re\u0161ita na roko, dve pa sta programerskega tipa. Tega poglavja nikar ne presko\u010di, saj predstavlja osnovo, brez katere ne gre. Poglavje 5 pa se dotakne pomembnega podro\u010dja ovrednotenja kakovosti uporabljenih tehnik strojnega u\u010denja. Strojno u\u010denje nam prinese dodatno vrednost le, \u010de deluje pravilno. Kako pa izmerimo, \u010de deluje pravilno? V tem poglavju so predstavljene razli\u010dne metrike kakovosti klasifikacijskih modelov in pristopi, k pravilni vzpostavi testnega okolja (in podatkov) za vrednotenje modelov. Ker je ovrednotenje kakovosti klasifikacijskih modelov klju\u010dnega pomena za razvoj uporabnih modelov, je tudi to poglavje priporo\u010dljivo za za\u010detnike. Dobrodo\u0161el oziroma dobrodo\u0161la v svet strojnega u\u010denja. Holzinger, A., Langs, G., Denk, H., Zatloukal, K. and M\u00fcller, H., 2019. Causability and explainability of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(4), p.e1312. \u21a9 Fister, D., Mun, J.C., Jagri\u010d, V. and Jagri\u010d, T., 2019. Deep learning for stock market trading: a superior trading strategy?. Neural Network World, 29(3), pp.151-171. \u21a9 Del Ser, J., Osaba, E., Sanchez-Medina, J.J. and Fister, I., 2019. Bioinspired computational intelligence and transportation systems: a long road ahead. IEEE Transactions on Intelligent Transportation Systems, 21(2), pp.466-495. \u21a9 Bello-Orgaz, G., Jung, J.J. and Camacho, D., 2016. Social big data: Recent achievements and new challenges. Information Fusion, 28, pp.45-59. \u21a9","title":"Uvod"},{"location":"pages/knjiga/01_uvod/#uvod","text":"Strojno u\u010denje, umetna inteligenca, podatkovna znanost, podatkovno rudarjenje in velepodatki (ali kar veliki podatki). V zadnjih letih so to precej uporabljeni izrazi, ki jih zasledimo v novicah o revolucionarnih premikih na podro\u010djih obdelave podatkov, razpoznave slik ter videoposnetkov in obdelave besedila. S temi izrazi podjetniki zvito opi\u0161ejo svoje storitve in produkte, da jim dvignejo vrednost ali jih la\u017eje prodajo. S temi izrazi razvijalci programske opreme opi\u0161ejo svoje izdelke, ko so se ti zmo\u017eni prilagajati na razli\u010dne situacije. Pa gre res za revolucionarne metode ali le za trenutno modo oziroma modne besede (angl. buzzwords )? So algoritmi strojnega u\u010denja res nekaj, kar izra\u017ea inteligentnost ra\u010dunalnika, ali pa gre le za kombinacijo if stavkov in statisti\u010dnih metod? \u017dal ni enovitega odgovora. Inteligentnega in samostojnega stroja, kot so prikazani v nekaterih filmskih uspe\u0161nicah, ne moremo pri\u010dakovati \u0161e nekaj \u010dasa, \u010de sploh kdaj. Vsekakor pa imajo pristopi strojnega u\u010denja vrednost, kar ka\u017ee tudi vztrajnost zanimanja in njihove uporabe tako v industriji kakor tudi raziskovalni sferi. Strojno u\u010denje je danes klju\u010den sestavni del \u0161tevilnih komercialnih informacijskih sistemov in raziskovalnih projektov na razli\u010dnih podro\u010djih - od medicinske diagnoze in zdravljenja 1 , avtomatskega trgovanja z vrednostnimi papirji 2 , samovoze\u010dih avtomobilov 3 , pa do priporo\u010dil na dru\u017ebenih omre\u017ejih in spletnih trgovinah 4 . Mnogi menijo, da je uporaba strojnega u\u010denja primerna za uporabo le v velikih podjetjih z obse\u017enimi raziskovalnimi skupinami in globokimi \u017eepi. S to knjigo ti avtorja \u017eeliva pokazati, kako enostavna je uporaba strojnega u\u010denja, \u010de le \u017ee pozna\u0161 osnove programiranja. Kot na drugih podro\u010djih, tudi pri strojnem u\u010denju obstajajo tako kompleksni in napredni pristopi, kakor tudi enostavni. Za razumevanje najbolj naprednih pristopov (na primer globokega u\u010denja (angl. deep learning )) res potrebujemo \u017ee kar nekaj semestrov matematike. Po drugi strani pa razumevanje najenostavnej\u0161ih pristopov zahteva le izku\u0161nje iz programiranja. Seveda se najve\u010d govori o najbolj naprednih metodah, ki terjajo ekipo dvestotih podatkovnih znanstvenikov (kot se imenujemo) in za dve ko\u0161arkarski dvorani velik ra\u010dunalnik. V resnici pa ve\u010dji del informacijskih sistemov, ki so podprti s strojnim u\u010denjem, uporablja zelo elementarne tehnike. Vsakdo pa \u017ee ne potrebuje naprednega algoritma, ki identificira \u010dloveka iz treh pikslov. V ve\u010dji meri so za obogatitev obstoje\u010dih sistemov dovolj enostavni pristopi, kar je razvidno iz uporabe umetne inteligence v praksi. Nova storitev, katere razvijalci poudarjajo, da je podprta z naprednimi pristopi umetne inteligence, res nima le nekaj pogojnih if -ov in osnovnih opisno statisti\u010dnih izra\u010dunov, kljub temu pa navadno ni tako kompleksna, kot se mogo\u010de zdi.","title":"Uvod"},{"location":"pages/knjiga/01_uvod/#komu-je-ta-knjiga-namenjena","text":"Ta knjiga slu\u017ei kot uvod v podro\u010dje strojnega u\u010denja. Namenjena je tistim, ki \u017ee znajo programirati - idealno v programskem jeziku Python, saj so primeri prikazani z uporabo tega jezika. Zakaj Python? Ker je to eden izmed najbolj uporabljenih programskih jezikov za namen uporabe strojnega u\u010denja. Knjiga se osredoto\u010da na uporabo Pythona in knji\u017enice scikit-learn , ki je osnovna knji\u017enica za uporabo strojnega u\u010denja. Seveda se ne gre izogniti uporabi knji\u017enic NumPy in Matplotlib oz. pandas in seaborn - ampak poudarek vsekakor ni na teh. Zavestno je bila narejena odlo\u010ditev, da se knjiga preve\u010d ne osredoto\u010da na matemati\u010dni vidik strojnega u\u010denja. Dr\u017ei, matemati\u010dne formule so \u0161e vedno prisotne, ampak le do te mere, da pomagajo (ne pa ovirajo) pri razumevanju posameznih pristopov. Knjiga se osredoto\u010di le na eno tehniko strojnega u\u010denja - na klasifikacijo. Klasifikacija se predstavi na le enem, po mnenju avtorjev, najbolj intuitivnem algoritmu klasifikacije, imenovanemu k najbli\u017ejih sosedov. Ta algoritem je predstavljen nekoliko bolj podrobno, s \u010dimer se ponazori, da ima vsak pristop strojnega u\u010denja svoje posebnosti, ki ga naredijo bodisi primernega ali neprimernega za dan problem.","title":"Komu je ta knjiga namenjena?"},{"location":"pages/knjiga/01_uvod/#komu-ta-knjiga-ni-namenjena","text":"V knjigi avtorja okvirno predstaviva razli\u010dne tehnike in podro\u010dja strojnega u\u010denja - podrobnosti pa izpustiva. \u010ce te zanima regresija, katera izmed tehnik nenadzorovanega u\u010denja ali delo z globokimi nevronskimi mre\u017eami, ta knjiga ni zate. Vsak dober podatkovni znanstvenik je najprej spoznaval temelje strojnega u\u010denja ter se \u0161ele potem posvetil naprednej\u0161im tehnikam. \u010cemu nam bo napredna globoka nevronska mre\u017ea, \u010de pa je ne znamo pravilno ovrednotiti? Knjiga je napisana tako, da bo samostojno nadaljevanje u\u010denja kar se da enostavno. V knjigi niso omenjene vse mo\u017ene knji\u017enice strojnega u\u010denja v Pythonu ali mo\u017enosti uporabe strojnega u\u010denja v drugih programskih jezikih. \u010cetudi te delo na strojnem u\u010denju v Pythonu zanese k uporabi PyTorcha ali TensorFlowa , je scikit-learn osnova, brez katere ne gre. \u010cetudi te bosta delodajalec ali lastna radovednost v prihodnosti usmerila v uporabo strojnega u\u010denja v drugih jezikih, so tudi ostale knji\u017enice strojnega u\u010denja spisane po zgledu knji\u017enice scikit-learn . Po povr\u0161nem pregledu knjige bi kdo morda pri\u0161el do zaklju\u010dka, da knjiga pokrije snov, ki bi jo lahko predstavili v eni objavi na blogu. Dr\u017ei - \u010de bi snov, predstavljeno v tej knjigi, strnili v programsko kodo, to ne bi bil kompleksen informacijski sistem, temve\u010d le dalj\u0161a Python skripta. Namen knjige ni, da predstavi \u010dim ve\u010d razli\u010dnih na\u010dinov uporabe knji\u017enice scikit-learn in predela vse mo\u017ene algoritme v tej - temu namre\u010d slu\u017ei dokumentacija te knji\u017enice. Skozi branje se od bralca te knjige pri\u010dakuje, da vsak korak (oziroma vsako vrstico kode) razume - kaj se zgodi in \u010demu je namenjena. Skozi poglobljeno razumevanje ti avtorja \u017eeliva predati sposobnost nadaljnjega samostojnega u\u010denja - kaj je pomembno pri dolo\u010denem pristopu, kako se uporabi ta pristop, kako nastavitve vplivajo na rezultate in tako naprej. Povedano druga\u010de, s knjigo te avtorja \u017eeliva nau\u010diti samostojnega u\u010denja snovi strojnega u\u010denja.","title":"Komu ta knjiga ni namenjena?"},{"location":"pages/knjiga/01_uvod/#struktura-knjige-in-kje-zaceti","text":"Uvodu sledijo \u0161tiri poglavja. Poglavje 2 govori o vzpostavitvi okolja, ki bo primerno za uporabo knji\u017enic, ki jih knjiga vklju\u010duje, za zagon primerov iz knjige in za re\u0161evanje prakti\u010dnih nalog. \u010ce ima\u0161 Python okolje \u017ee vzpostavljeno na svojem ra\u010dunalniku, lahko to poglavje presko\u010di\u0161. TODO Poglavje 3 za\u010dne s splo\u0161no razlago definicije strojnega u\u010denja in vzpostavi vzporednice tega z na\u0161im (\u010dlove\u0161kim) u\u010denjem novega znanja. Ve\u010dji del poglavja je namenjen predstavitvi glavne tehnike strojnega u\u010denja v tej knjigi - klasifikaciji podatkov. Opisi in definicije gredo od najenostavnej\u0161e razlage na primerih pa do formalne matemati\u010dne definicije pojmov. To poglavje presko\u010di, \u010de si z osnovnimi tehnikami \u017ee seznanjen/-a in bi se \u017eelel/-a hitro posvetiti programiranju. Poglavje 4 obravnava izbran algoritem klasifikacije v tej knjigi - k najbli\u017ejih sosedov. Algoritem je najprej predstavljen neodvisno od programiranja na na\u010din, ki ti bo omogo\u010dal, da ga zna\u0161 re\u0161iti tudi na roko. Predstavljene so nekatere nastavitve tega algoritma, s \u010dimer se predstavi pomembnost poznavanja podrobnosti algoritma, da je uporaba tega kar se da u\u010dinkovita. Opisu in definicijam pa sledijo primeri v programski kodi. Ti primeri so napisani tako, da se lahko neposredno uporabijo pri testiranju in spoznavanju. Za utrjevanje znanja iz tega poglavja so prilo\u017eene tudi \u0161tiri naloge - dve preverjata teoreti\u010dno razumevanje in se re\u0161ita na roko, dve pa sta programerskega tipa. Tega poglavja nikar ne presko\u010di, saj predstavlja osnovo, brez katere ne gre. Poglavje 5 pa se dotakne pomembnega podro\u010dja ovrednotenja kakovosti uporabljenih tehnik strojnega u\u010denja. Strojno u\u010denje nam prinese dodatno vrednost le, \u010de deluje pravilno. Kako pa izmerimo, \u010de deluje pravilno? V tem poglavju so predstavljene razli\u010dne metrike kakovosti klasifikacijskih modelov in pristopi, k pravilni vzpostavi testnega okolja (in podatkov) za vrednotenje modelov. Ker je ovrednotenje kakovosti klasifikacijskih modelov klju\u010dnega pomena za razvoj uporabnih modelov, je tudi to poglavje priporo\u010dljivo za za\u010detnike. Dobrodo\u0161el oziroma dobrodo\u0161la v svet strojnega u\u010denja. Holzinger, A., Langs, G., Denk, H., Zatloukal, K. and M\u00fcller, H., 2019. Causability and explainability of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(4), p.e1312. \u21a9 Fister, D., Mun, J.C., Jagri\u010d, V. and Jagri\u010d, T., 2019. Deep learning for stock market trading: a superior trading strategy?. Neural Network World, 29(3), pp.151-171. \u21a9 Del Ser, J., Osaba, E., Sanchez-Medina, J.J. and Fister, I., 2019. Bioinspired computational intelligence and transportation systems: a long road ahead. IEEE Transactions on Intelligent Transportation Systems, 21(2), pp.466-495. \u21a9 Bello-Orgaz, G., Jung, J.J. and Camacho, D., 2016. Social big data: Recent achievements and new challenges. Information Fusion, 28, pp.45-59. \u21a9","title":"Struktura knjige in kje za\u010deti"},{"location":"pages/knjiga/02_okolje/","text":"Vzpostavitev okolja Za namen prikaza prakti\u010dnih primerov bo v tej knjigi uporabljen programski jezik Python, saj je en izmed bolj priljubljenih programskih jezikov z odli\u010dnim naborom knji\u017enic strojnega u\u010denja. Za razumevanje primerov je vsekakor priporo\u010dljivo poznavanje tega programskega jezika ali vsaj splo\u0161no poznavanje enega izmed modernih programskih jezikov (Java, Perl, C#, R, C, C++). Sledijo navodila vzpostavitve okolja, primernega za izvedbo prakti\u010dnih primerov na svojem ra\u010dunalniku. Razvojno okolje Jupyter Notebook Vsi prakti\u010dni primeri v knjigi so prikazani tako, da se brez te\u017eave izvedejo v JupyterLab okolju. To razvojno okolje omogo\u010da pisanje Python programske kode v tako imenovanih Jupyter zvezkih (angl. Jupyter notebooks ). Ti zvezki se urejajo v poljubnem spletnem brskalniku in zato za urejanje ne potrebujejo namenskega orodja. Posebnost kode, zapisane v Jupyter zvezkih, je, da je ta v eni datoteki (zvezku) me\u0161ana s takoj\u0161njimi izpisi kode in navodili, ki jih razvijalci zapi\u0161emo sproti za razlago delovanja kode. Zvezki so razdeljeni na tako imenovane celice, kjer je celica bodisi navodilo ali pa programska koda z izpisom rezultata, kot je prikazano na spodnji sliki. Jupyter zvezek na spletni storitvi Google Colab. Namen takega razvoja je, da je programska koda z navodili enostavno deljiva in razumljiva. Prav tako razvoj v zvezku omogo\u010da inkrementalno zaganjanje kode, celico za celico - tako se za\u017eenejo le \u017eeleni deli kode in ne celotna datoteka. To je idealen na\u010din razvoja za u\u010denje in za raziskovanje. Jupyter zvezki so postali privzeti na\u010din razvoja na podro\u010dju podatkovne znanosti, saj zadostijo primarnemu namenu podatkovne znanosti - odkrivanju vzorcev in pregledu podatkov skozi zgodbo. Tako je vsaka celica s kodo in prilo\u017eeno celico navodil kar en del zgodbe, kjer celica z navodili opi\u0161e, kaj bo celica s kodo naredila ter povzame njene rezultate. Alternativnih razvojnih okolij je mnogo. Od popolnoma namenskih za programiranje v Pythonu, kot sta PyCharm in Spyder , pa do splo\u0161nih razvojnih okolij, kot sta Visual Studio Code in Atom . Uporaba obla\u010dnega okolja Prvi na\u010din uporabe Jupyter zvezkov za namen strojnega u\u010denja je uporaba ene izmed ponujenih storitev. V ta namen so na voljo \u0161tevilni ponudniki z bodisi zastonjskimi ali pla\u010dljivimi storitvami izvajanja Python kode v oblaku. Dober primer take storitve je Google Colab , ki vsem svojim uporabnikom ponuja kreacijo Jupyter zvezkov in zagon teh na Googlovih stre\u017enikih, do dolo\u010dene mere zastonj - v \u010dasu pisanja knjige je uporaba Google Colab zvezkov za namen zagona primerov in nalog bila brezpla\u010dna. Google Colab je prikazan prav na zgornji sliki. Konkurence na podro\u010dju obla\u010dnih zvezkov je ogromno in uporabnost teh se mese\u010dno spreminja ter je vezana na ceno storitve, dostopnost slovenskim razvijalcem in nabor funkcionalnosti. Preprosto spletno iskanje \"Jupyter Notebook service\"~ali \"Google Colab alternative\"~vrne \u0161tevilne rezultate. V \u010dasu pisanja knjige so med omembe vrednimi bile storitve drugih gigantov strojnega u\u010denja Microsoft Azure Machine Learning in Amazon SageMaker ter s strojno opremo radodarna Paperspace in Deepnote . Namestitev in uporaba lokalnega okolja Anaconda V tej sekciji bo predstavljeno, kako vzpostavimo primerno okolje na svojem ra\u010dunalniku. Da namestimo Python in vse potrebne knji\u017enice, se bomo zatekli k okolju Anaconda, ki vsebuje tako Python kot skupek \u0161tevilnih knji\u017enic, ki se uporabljajo za namen analize podatkov. Na uradni spletni strani okolja Anaconda poi\u0161\u010demo predel za prenos razli\u010dice, namenjene za posameznike. V \u010dasu pisanja knjige se ta razli\u010dica imenuje Individual Edition in je dosegljiva na uradni spletni strani ter prikazana spodaj. Spletna stran Python okolja Anaconda. Okolje Anaconda poleg programskega jezika Python namesti tudi \u0161tevilne knji\u017enice, namenjene za podatkovno analizo v tem programskem jeziku. Skozi knjigo bomo uporabili \u0161tevilne izmed teh: NumPy , ki se uporablja za upravljanje s podatki v matri\u010dnih in ve\u010d-dimenzionalnih poljih. Podrobnej\u0161ega dela s to knji\u017enico ne bo, je pa vseeno dobro, da se zavedamo, da je to ena izmed klju\u010dnih knji\u017enic, ko imamo opravek s podatki in analizo teh. Ta knji\u017enica je tudi sestavni del slede\u010de knji\u017enice. pandas je knji\u017enica, ki je namenjena za manipulacijo in analizo podatkov v tabelari\u010dni obliki. \u010ceprav v zaledju uporablja knji\u017enico NumPy , so dolo\u010dene operacije nad podatki poenostavljene. scikit-learn je knji\u017enica, ki vsebuje implementacije razli\u010dnih algoritmov, metrik in pristopov pred-procesiranja za strojno u\u010denje. Algoritem klasifikacije, ki bo predstavljen v tej knjigi, je povzet po implementaciji iz te knji\u017enice. Matplotlib , ki se uporablja za prikaz grafov razli\u010dnih vrst. Je zelo prilagodljiva, saj omogo\u010da velik nabor razli\u010dnih tipov vizualizacij podatkov in prilagoditve teh (tako stilno, kot vsebinsko). Je pa zaradi svoje prilagodljivost delo s to knji\u017enico nekoliko te\u017eje in zaradi tega uporabljamo ... seaborn , ki poenostavi prikaz grafov. V zaledju uporablja knji\u017enico Matplotlib , ampak preko svojih vmesnikov dolo\u010dene pogoste operacije pri risanju grafov poenostavi. Uporaba Jupyter zvezkov Za pisanje Jupyter zvezkov na lastnem ra\u010dunalniku je najprej potreben zagon okolja JupyterLab, za kar lahko uporabimo enega izmed dveh na\u010dinov. Prvi na\u010din je zagon JupyterLaba ro\u010dno iz komandne vrstice: 1. Za\u017eenemo komandno vrstico. V Windowsih sta to bodisi Command Prompt ali PowerShell. V Linux in MacOS operacijskih sistemih pa je komanda vrstica najve\u010dkrat pod imenom Terminal. 2. Za\u017eenemo ukaz jupyter lab . 3. Odpre se okno brskalnika in poka\u017ee se doma\u010da stran JupyterLab, kot je prikazano na spodnji sliki. Drugi na\u010din je preko uporabni\u0161kega vmesnika okolja Anaconda. Za\u017eenemo Anaconda Navigator. V seznamu ponujenih aplikacij najdimo JupyterLab in ga s pritiskom mo\u017enosti Launch za\u017eenemo. Odpre se okno brskalnika in poka\u017ee se doma\u010da stran JupyterLab, kot je prikazano na slede\u010di sliki. Doma\u010da stran JupyterLab okolja. Po \u017eelji naredimo novo mapo, kamor shranimo na\u0161e zvezke in druge podporne datoteke. V tej mapi ustvarimo novi zvezek, kot je prikazano slede\u010de. Kreacija novega zvezka v JupyterLab okolju. Odpre se novo okno s praznim zvezkom, kot je prikazano spodaj. Jupyter zvezke razvijamo preko spletnega vmesnika, kar je tudi razlog za prikaz spletnega brskalnika. Nov prazen Jupyter zvezek. Zgornja slika ima ozna\u010dene pomembne dele uporabni\u0161kega vmesnika. Oznaka 1 prikazuje naslov zvezka, kar s klikom na napis lahko spremenimo. Oznaka 2 prikazuje tip celice, ki je trenutno izbrana, oznaka 3 pa prikazuje trenutno izbrano in hkrati zaenkrat \u0161e edino celico. Markdown v Jupyter zvezkih Za za\u010detek spremenimo tip prve celice na Markdown , ki je stil zapisa navodil in druge vsebine v celico. V celico zapi\u0161emo slede\u010d Markdown zapis. # Prvi primer V __prvem__ primeru bomo: - Naredili prvi izpis `Python` kode. - Naredili prvi graf s pomo\u010djo _seaborn_ knji\u017enice. S pritiskom na gumb zagona, ki je na prej\u0161nji sliki 4 (ali s pritiskom Shift + Enter ), se izbrana celica izvede in izbere se (ter \u010de ne obstaja se tudi ustvari) naslednja celica. Ker je izbrana celica bila tipa Markdown, se je zapisana koda oblikovala po stilu Markdown zapisa. Osnovni ukazi za zapis Markdowna so slede\u010di. Z znakom #, \u010demur sledi presledek definiramo naslov. # za glavne naslove, ## za podnaslove, ### za tretji nivo naslovov, pa vse do \u0161estega nivoja naslovov z ###### . Besedilo lahko tudi poudarimo. Po\u0161evno zapisano besedilo: _besedilo_ ali *besedilo* ter krepko _ zapisano besedilo: __besedilo__ ali **besedilo** . Dodamo lahko tudi neurejen seznam, tako da vsak element seznama zapi\u0161emo v svoji vrstici, za\u010dnemo pa ga bodisi z znakom * ali z - . Sezname lahko gnezdimo - ugnezdene elemente zamaknemo s tabulatorjem. - Prvi element. * Drugi element. - Prvi ugnezden element. - Pa \u0161e drugi. Urejene sezname, kjer si elementi sledijo v vrstnem redu, pa ustvarimo tako, da pred elemente dodamo \u0161tevilko ali oznako vrstnega reda. Tudi take sezname lahko gnezdimo. \u0160tevilko vrstnega reda lahko podamo poljubno, saj ni potrebno, da so te v pravem vrstnem redu. 1. Prvi element. 2. Drugi element. 11. Prvi ugnezden element. 12. Pa \u0161e drugi. Besedilo v stilu kode pa v besedilo zapi\u0161emo med znakoma ` , ki ga na slovenski tipkovnici izberemo z AltGr + 7 . Spremenljivka `vrednost` in razred `podatki` . Tabelo vstavimo vrstico po vrstico, vsaka celica v vrstici se za\u010dne in kon\u010da z znakom | (na slovenski tipkovnici AltGr + W ). Tako je ena vrstica s tremi celicami zapisana na slede\u010d na\u010din. |Prva celica|Druga celica|Tretja celica| \u010crte med vrstice pa dodamo kar z znakom - namesto vsebine celice. Primer tabele s tremi stolpci in tremi vrsticami ter \u010drto med prvo in drugo vrstico izgleda slede\u010de. |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |-|-|-| |Druga vrstica|12|-42| |Tretja vrstica|-7|65| Za la\u017ejo razumevanje kode lahko to nekoliko oblikujemo s presledki. |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |----------------------------|-------------|--------------| |Druga vrstica |12 |-42 | |Tretja vrstica |-7 |65 | Hiperpovezave vstavljamo s slede\u010do kodo [Napis povezave](URL) , slike pa z zapisom ![Naziv slike](URL do datoteke slike) - razlika je v klicaju. [ Povezava do iskalnika Google ]( http://www.google.com/ ) ![ Primer slike ]( /imgs/slika.png ) \u010ce ne gre druga\u010de, pa lahko vsebino celic navodil oblikujemo tudi s preprosto HTML kodo. HTML in Markdown zapis lahko po \u017eelji me\u0161amo. Kon\u010den primer vsega skupaj bi zapisali s slede\u010do kodo, rezultat pa je prikazan na sliki pod kodo. ## Preizkus Markdown zapisa 1. Preizkusili smo oblikovanje pisav 1. __krepko__ in 2. _po\u0161evno_ pisavo 2. Pisanje seznamov - urejenih in neurejenih, - ter ugnezdenih 3. Dodajanje [ povezava ]( http://www.um.si/ ) 4. Dodajanje slik ![ FERI ]( https://feri.um.si/site/images/logo-feri.png ) 5. Pisanje v stilu `programske kode` 6. Dodajanje tabel |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |----------------------------|-------------|--------------| |Druga vrstica |12 |-42 | |Tretja vrstica |-7 |65 | 7. Tudi <a href=\"https://feri.um.si/\">HTML koda</a> deluje. Rezultat Markdown zapisa. Python koda v Jupyter zvezkih \u010ce ima celica vsebino tipa Code , pa se vsebina smatra kot Python koda - primerno je tudi obarvana, deluje avtomatsko dopolnjevanje kode (angl. code auto-completion ) in rezultati kode se izpi\u0161ejo kar takoj pod celico. Sledi primer zapisa programske kode v celico. V prvi vrstici se uvozi knji\u017enica NumPy ter se poimenuje kot np za nadaljnjo uporabo. Sledi izpis niza znakov z ukazom print() - izpis sledi kar pod celico, kot je prikazano na slede\u010di sliki. Tretja vrstica vsebuje komentar v kodi, ki opisuje zadnjo vrstico. Zadnja vrstica pa vsebuje klic metode rand() knji\u017enice NumPy , ki vrne polje naklju\u010dnih \u0161tevil po podanih velikostih (v primeru je podana velikost polja s tremi vrsticami in dvema stolpcema). Ker ta vrstica vsebuje tudi zadnji ukaz, ki vra\u010da rezultat, se rezultat tega izpi\u0161e tudi pod celico. import numpy as np print ( 'Sledi izpis polja naklju\u010dnih \u0161tevil s tremi vrsticami in dvema stolpcema.' ) # Rezultat zadnjega ukaza v celici se izpi\u0161e kot rezultat celice. np . random . rand ( 3 , 2 ) Sledi izpis polja naklju\u010dnih \u0161tevil s tremi vrsticami in dvema stolpcema. array(0.27687911, 0.19824952, 0.06910974, 0.7548379, 0.64441325, 0.11951306) Prvi preizkus zapisa Python kode v Jupyter zvezek. Preizkusimo \u0161e uporabo knji\u017enice pandas , ki se uvozi v prvi vrstici kode. Kot \u017ee omenjeno pri prvi predstavitvi knji\u017enice, ta v zaledju uporablja podatke v obliki NumPy polj. To dejstvo lahko izkoristimo pri kreaciji nove strukture podatkov - v pandas -u je to razpredelnica, poimenovana DataFrame . Slede\u010da koda ustvari polje naklju\u010dnih \u0161tevil s 100 vrsticami in dvema stolpcema, ki pa slu\u017ei pri kreaciji razpredelnice DataFrame . V DataFrame razpredelnici lahko stolpce in vrstice poimenujemo, kot je prikazano s podanim parametrom columns . Zadnja vrstica kli\u010de metodo head() na\u0161e instance data_df razreda DataFrame , ki vrne prvih pet vrstic te razpredelnice, kot tudi ka\u017ee slika pod kodo. import numpy as np import pandas as pd data_np = np . random . rand ( 100 , 2 ) data_df = pd . DataFrame ( data_np , columns = [ 'Prvi stolpec' , 'Drugi stolpec' ]) data_df . head () Prvi stolpec Drugi stolpec 0 0.753358 0.029272 1 0.901894 0.846576 2 0.492876 0.533067 3 0.943245 0.538149 4 0.567161 0.537613 Preizkus delovanja knji\u017enice pandas . Sledi preizkus izrisa grafov s knji\u017enicama seaborn in Matplotlib . Slede\u010da koda prikazuje izris grafa raztrosa (angl. scatter plot ) za prej ustvarjeno razpredelnico data_df v obeh knj\u017enicah. seaborn Matplotlib import seaborn as sns sns . scatterplot ( x = data_df [ 'Prvi stolpec' ], y = data_df [ 'Drugi stolpec' ]) Preizkus izrisa grafa raztrosa s knji\u017enico seaborn . import matplotlib.pyplot as plt plt . scatter ( x = data_df [ 'Prvi stolpec' ], y = data_df [ 'Drugi stolpec' ]) Preizkus izrisa grafa raztrosa s knji\u017enico Matplotlib . Na prvi pogled se zdi uporaba seaborn in Matplotlib knji\u017enic podobna, kar tudi v resnici je. Nekatere tehnike grafi\u010dne predstavitve podatkov je s knji\u017enico seaborn la\u017eje narediti, medtem, ko je pri izrisu grafov Matplotlib mnogo bolj prilagodljiv. Sedaj bomo preizkusili \u0161e knji\u017enico scikit-learn , ki vsebuje razli\u010dne pristope, metode ter tehnike strojnega u\u010denja in predprocesiranja podatkov. Preden se lotimo uporabe metod strojnega u\u010denja, bomo tokrat preizkusili nalaganje prilo\u017eenih podatkov v knji\u017enici scikit-learn . Ena izmed prilo\u017eenih je podatkovna zbirka Iris , ki vsebuje podatke o cveto\u010dih rastlinah perunikah. V prvi vrstici se najprej nalo\u017ei modul knji\u017enice scikit-learn , v drugi vrstici pa se nalo\u017ei podatkovna zbirka Iris ter se shrani v spremenljivko iris_vse . Ker smo pri klicu metode podali as_frame=True , bodo rezultati v obliki pandas razpredelnice DataFrame . \u010ce tega ne bi podali, pa bi rezultat podatkov bil v obliki NumPy polja. Shranjeni podatki v spremenljivki iris_vse imajo ve\u010d vrednosti: vrednost data nam vrne podatke o ro\u017eah (vi\u0161ine in \u0161irine \u010da\u0161nih in cvetnih listov). Vrednost target pa nam vrne vrednosti razredov te podatkovne zbirke - za kateri tip perunike gre. Zadnja vrstica kode izpi\u0161e prvih pet vrstic podatkov. from sklearn.datasets import load_iris iris_vse = load_iris ( as_frame = True ) iris_podatki = iris_vse . data iris_razredi = iris_vse . target iris_podatki . head () sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 Za konec preizkusimo z izrisom grafa raztrosa, kjer bodo pike vsake izmed instanc pobarvane glede na podan razred te instance. sns . scatterplot ( x = 'sepal length (cm)' , y = 'sepal width (cm)' , data = iris_podatki , hue = iris_razredi ) Graf raztrosa podatkovne zbirke Iris . Tokrat smo za parameter data podali kar razpredelnico iris_podatki , ki je pandas DataFrame . Parametra x in y smo tokrat zapisali kot niz znakov - imena stolpcev iz podane razpredelnice. S parametrom hue dolo\u010dimo, kako se oznake na grafu pobarvajo. Lahko bi podali tudi ime stolpca (ki pa ga tokrat v razpredelnici data nimamo), ali pa kot lo\u010deno spremenljivko razpredelnice s temi podatki (kot smo naredili tokrat z iris_razredi ). Nalaganje podatkov Da lahko izvedemo analize in uporabimo algoritme strojnega u\u010denja, pa potrebujemo podatke. Slede\u010da sekcija pregleda, kako nalo\u017eimo podatke v na\u0161 Python program oziroma Jupyter zvezek. Nalaganje prostodostopnih podatkov \u017de v prej\u0161nji sekciji smo pokazali, kako lahko dostopamo do standardnih podatkovnih zbirk, ki se mnogokrat uporabljajo v procesu u\u010denja. To smo storili s pomo\u010djo knji\u017enice scikit-learn , saj so preko te knji\u017enice \u017ee name\u0161\u010dene \u0161tevilne podatkovne zbirke. Skozi celotno knjigo bomo najve\u010dkrat uporabili podatkovno zbirko Iris , v nalogah pa bodo uporabljene tudi Wine in Breast cancer . V vseh primerih gre za podatkovne zbirke, namenjene klasifikaciji. Nabor podatkovnih zbirk, ki so prilo\u017eene tej knji\u017enici, je dostopen na slede\u010dem naslovu za enostavne in majhne zbirke ter na tem naslovu za zbirke iz realnega sveta. Podatkovne zbirke iz scikit-learn vrnejo objekt s slede\u010dimi vrednostmi: data je razpredelnica s podatki vseh instanc, target je polje z re\u0161itvami vseh instanc, feature_names je seznam imen vseh atributov, ki so v data ter target_names je seznam imen re\u0161itev (\u010de gre za razrede). \u010ce nam prilo\u017eene prostodostopne podatkovne zbirke niso dovolj, lahko uporabimo poljubno podatkovno zbirko iz portala OpenML.org . Ta portal igra vlogo repozitorija za podatkovne zbirke. Te lahko uporabniki portala prosto nalagajo na portal in tudi do njih dostopajo. Knji\u017enica scikit-learn ponuja vmesnik za neposredno nalaganje teh datotek s pomo\u010djo metode fetch_openml , kateri podamo ime zbirke, kot ka\u017ee spodnja koda. from sklearn.datasets import fetch_openml podatki = fetch_openml ( name = 'ecoli' ) print ( podatki . feature_names ) ['mcg', 'gvh', 'lip', 'chg', 'aac', 'alm1', 'alm2'] Tudi pri nalaganju podatkovnih zbirk iz OpenML.org lahko fetch_openml metodi povemo, da \u017eelimo razpredelnico DataFrame . To storimo enako kot pri nalaganju prilo\u017eenih podatkovnih zbirk, s podajo as_frame=True . from sklearn.datasets import fetch_openml podatki = fetch_openml ( name = 'nursery' , as_frame = True ) print ( podatki . target . head ()) 0 recommend 1 priority 2 not_recom 3 recommend4 priority Nalaganje lastnih podatkov Podatke lahko v program oziroma zvezek nalo\u017eimo s pomo\u010djo knji\u017enice pandas , ki prepozna podatke v \u0161tevilnih razli\u010dnih formatih : tekstovne datoteke, kjer so podatki lo\u010deni z znaki (npr. z vejico v CSV, tabulatorjem ali drugim znakom), JSON format tekstovne datoteke, HTML spletne strani, iz katerih se podatki preberejo iz elementa <table> , Excel ali OpenDocument razpredelnice, Lastni\u0161ke SAS ali SPSS datoteke podatkov, podatkovne baze s SQL klicem ter lokalne odlo\u017ei\u0161\u010da (angl. clipboard ). Primer nalaganja iz CSV datoteke, kjer so vrednosti lo\u010dene s podpi\u010djem ; prikazuje spodnja koda. Vsak format ima svojo metodo za prebiranje datotek, saj so dolo\u010dene nastavitve formatu specifi\u010dne. Pri prebiranju iz tekstovnih datotek tako lahko podamo znak, ki lo\u010duje vrednosti s sep , znak lo\u010devanja decimalnih mest s dec , vrstico, ki predstavlja imena atributov, s header ali pa podamo ta imena kar v names . import pandas as pd podatki = pd . read_csv ( 'datoteka.txt' , sep = ';' , dec = ',' ) S pomo\u010djo tega je po prebiranju knjige mogo\u010de osvojeno znanje aplicirati na lastne podatke.","title":"Vzpostavitev okolja"},{"location":"pages/knjiga/02_okolje/#vzpostavitev-okolja","text":"Za namen prikaza prakti\u010dnih primerov bo v tej knjigi uporabljen programski jezik Python, saj je en izmed bolj priljubljenih programskih jezikov z odli\u010dnim naborom knji\u017enic strojnega u\u010denja. Za razumevanje primerov je vsekakor priporo\u010dljivo poznavanje tega programskega jezika ali vsaj splo\u0161no poznavanje enega izmed modernih programskih jezikov (Java, Perl, C#, R, C, C++). Sledijo navodila vzpostavitve okolja, primernega za izvedbo prakti\u010dnih primerov na svojem ra\u010dunalniku.","title":"Vzpostavitev okolja"},{"location":"pages/knjiga/02_okolje/#razvojno-okolje-jupyter-notebook","text":"Vsi prakti\u010dni primeri v knjigi so prikazani tako, da se brez te\u017eave izvedejo v JupyterLab okolju. To razvojno okolje omogo\u010da pisanje Python programske kode v tako imenovanih Jupyter zvezkih (angl. Jupyter notebooks ). Ti zvezki se urejajo v poljubnem spletnem brskalniku in zato za urejanje ne potrebujejo namenskega orodja. Posebnost kode, zapisane v Jupyter zvezkih, je, da je ta v eni datoteki (zvezku) me\u0161ana s takoj\u0161njimi izpisi kode in navodili, ki jih razvijalci zapi\u0161emo sproti za razlago delovanja kode. Zvezki so razdeljeni na tako imenovane celice, kjer je celica bodisi navodilo ali pa programska koda z izpisom rezultata, kot je prikazano na spodnji sliki. Jupyter zvezek na spletni storitvi Google Colab. Namen takega razvoja je, da je programska koda z navodili enostavno deljiva in razumljiva. Prav tako razvoj v zvezku omogo\u010da inkrementalno zaganjanje kode, celico za celico - tako se za\u017eenejo le \u017eeleni deli kode in ne celotna datoteka. To je idealen na\u010din razvoja za u\u010denje in za raziskovanje. Jupyter zvezki so postali privzeti na\u010din razvoja na podro\u010dju podatkovne znanosti, saj zadostijo primarnemu namenu podatkovne znanosti - odkrivanju vzorcev in pregledu podatkov skozi zgodbo. Tako je vsaka celica s kodo in prilo\u017eeno celico navodil kar en del zgodbe, kjer celica z navodili opi\u0161e, kaj bo celica s kodo naredila ter povzame njene rezultate. Alternativnih razvojnih okolij je mnogo. Od popolnoma namenskih za programiranje v Pythonu, kot sta PyCharm in Spyder , pa do splo\u0161nih razvojnih okolij, kot sta Visual Studio Code in Atom .","title":"Razvojno okolje Jupyter Notebook"},{"location":"pages/knjiga/02_okolje/#uporaba-oblacnega-okolja","text":"Prvi na\u010din uporabe Jupyter zvezkov za namen strojnega u\u010denja je uporaba ene izmed ponujenih storitev. V ta namen so na voljo \u0161tevilni ponudniki z bodisi zastonjskimi ali pla\u010dljivimi storitvami izvajanja Python kode v oblaku. Dober primer take storitve je Google Colab , ki vsem svojim uporabnikom ponuja kreacijo Jupyter zvezkov in zagon teh na Googlovih stre\u017enikih, do dolo\u010dene mere zastonj - v \u010dasu pisanja knjige je uporaba Google Colab zvezkov za namen zagona primerov in nalog bila brezpla\u010dna. Google Colab je prikazan prav na zgornji sliki. Konkurence na podro\u010dju obla\u010dnih zvezkov je ogromno in uporabnost teh se mese\u010dno spreminja ter je vezana na ceno storitve, dostopnost slovenskim razvijalcem in nabor funkcionalnosti. Preprosto spletno iskanje \"Jupyter Notebook service\"~ali \"Google Colab alternative\"~vrne \u0161tevilne rezultate. V \u010dasu pisanja knjige so med omembe vrednimi bile storitve drugih gigantov strojnega u\u010denja Microsoft Azure Machine Learning in Amazon SageMaker ter s strojno opremo radodarna Paperspace in Deepnote .","title":"Uporaba obla\u010dnega okolja"},{"location":"pages/knjiga/02_okolje/#namestitev-in-uporaba-lokalnega-okolja-anaconda","text":"V tej sekciji bo predstavljeno, kako vzpostavimo primerno okolje na svojem ra\u010dunalniku. Da namestimo Python in vse potrebne knji\u017enice, se bomo zatekli k okolju Anaconda, ki vsebuje tako Python kot skupek \u0161tevilnih knji\u017enic, ki se uporabljajo za namen analize podatkov. Na uradni spletni strani okolja Anaconda poi\u0161\u010demo predel za prenos razli\u010dice, namenjene za posameznike. V \u010dasu pisanja knjige se ta razli\u010dica imenuje Individual Edition in je dosegljiva na uradni spletni strani ter prikazana spodaj. Spletna stran Python okolja Anaconda. Okolje Anaconda poleg programskega jezika Python namesti tudi \u0161tevilne knji\u017enice, namenjene za podatkovno analizo v tem programskem jeziku. Skozi knjigo bomo uporabili \u0161tevilne izmed teh: NumPy , ki se uporablja za upravljanje s podatki v matri\u010dnih in ve\u010d-dimenzionalnih poljih. Podrobnej\u0161ega dela s to knji\u017enico ne bo, je pa vseeno dobro, da se zavedamo, da je to ena izmed klju\u010dnih knji\u017enic, ko imamo opravek s podatki in analizo teh. Ta knji\u017enica je tudi sestavni del slede\u010de knji\u017enice. pandas je knji\u017enica, ki je namenjena za manipulacijo in analizo podatkov v tabelari\u010dni obliki. \u010ceprav v zaledju uporablja knji\u017enico NumPy , so dolo\u010dene operacije nad podatki poenostavljene. scikit-learn je knji\u017enica, ki vsebuje implementacije razli\u010dnih algoritmov, metrik in pristopov pred-procesiranja za strojno u\u010denje. Algoritem klasifikacije, ki bo predstavljen v tej knjigi, je povzet po implementaciji iz te knji\u017enice. Matplotlib , ki se uporablja za prikaz grafov razli\u010dnih vrst. Je zelo prilagodljiva, saj omogo\u010da velik nabor razli\u010dnih tipov vizualizacij podatkov in prilagoditve teh (tako stilno, kot vsebinsko). Je pa zaradi svoje prilagodljivost delo s to knji\u017enico nekoliko te\u017eje in zaradi tega uporabljamo ... seaborn , ki poenostavi prikaz grafov. V zaledju uporablja knji\u017enico Matplotlib , ampak preko svojih vmesnikov dolo\u010dene pogoste operacije pri risanju grafov poenostavi.","title":"Namestitev in uporaba lokalnega okolja Anaconda"},{"location":"pages/knjiga/02_okolje/#uporaba-jupyter-zvezkov","text":"Za pisanje Jupyter zvezkov na lastnem ra\u010dunalniku je najprej potreben zagon okolja JupyterLab, za kar lahko uporabimo enega izmed dveh na\u010dinov. Prvi na\u010din je zagon JupyterLaba ro\u010dno iz komandne vrstice: 1. Za\u017eenemo komandno vrstico. V Windowsih sta to bodisi Command Prompt ali PowerShell. V Linux in MacOS operacijskih sistemih pa je komanda vrstica najve\u010dkrat pod imenom Terminal. 2. Za\u017eenemo ukaz jupyter lab . 3. Odpre se okno brskalnika in poka\u017ee se doma\u010da stran JupyterLab, kot je prikazano na spodnji sliki. Drugi na\u010din je preko uporabni\u0161kega vmesnika okolja Anaconda. Za\u017eenemo Anaconda Navigator. V seznamu ponujenih aplikacij najdimo JupyterLab in ga s pritiskom mo\u017enosti Launch za\u017eenemo. Odpre se okno brskalnika in poka\u017ee se doma\u010da stran JupyterLab, kot je prikazano na slede\u010di sliki. Doma\u010da stran JupyterLab okolja. Po \u017eelji naredimo novo mapo, kamor shranimo na\u0161e zvezke in druge podporne datoteke. V tej mapi ustvarimo novi zvezek, kot je prikazano slede\u010de. Kreacija novega zvezka v JupyterLab okolju. Odpre se novo okno s praznim zvezkom, kot je prikazano spodaj. Jupyter zvezke razvijamo preko spletnega vmesnika, kar je tudi razlog za prikaz spletnega brskalnika. Nov prazen Jupyter zvezek. Zgornja slika ima ozna\u010dene pomembne dele uporabni\u0161kega vmesnika. Oznaka 1 prikazuje naslov zvezka, kar s klikom na napis lahko spremenimo. Oznaka 2 prikazuje tip celice, ki je trenutno izbrana, oznaka 3 pa prikazuje trenutno izbrano in hkrati zaenkrat \u0161e edino celico.","title":"Uporaba Jupyter zvezkov"},{"location":"pages/knjiga/02_okolje/#markdown-v-jupyter-zvezkih","text":"Za za\u010detek spremenimo tip prve celice na Markdown , ki je stil zapisa navodil in druge vsebine v celico. V celico zapi\u0161emo slede\u010d Markdown zapis. # Prvi primer V __prvem__ primeru bomo: - Naredili prvi izpis `Python` kode. - Naredili prvi graf s pomo\u010djo _seaborn_ knji\u017enice. S pritiskom na gumb zagona, ki je na prej\u0161nji sliki 4 (ali s pritiskom Shift + Enter ), se izbrana celica izvede in izbere se (ter \u010de ne obstaja se tudi ustvari) naslednja celica. Ker je izbrana celica bila tipa Markdown, se je zapisana koda oblikovala po stilu Markdown zapisa. Osnovni ukazi za zapis Markdowna so slede\u010di. Z znakom #, \u010demur sledi presledek definiramo naslov. # za glavne naslove, ## za podnaslove, ### za tretji nivo naslovov, pa vse do \u0161estega nivoja naslovov z ###### . Besedilo lahko tudi poudarimo. Po\u0161evno zapisano besedilo: _besedilo_ ali *besedilo* ter krepko _ zapisano besedilo: __besedilo__ ali **besedilo** . Dodamo lahko tudi neurejen seznam, tako da vsak element seznama zapi\u0161emo v svoji vrstici, za\u010dnemo pa ga bodisi z znakom * ali z - . Sezname lahko gnezdimo - ugnezdene elemente zamaknemo s tabulatorjem. - Prvi element. * Drugi element. - Prvi ugnezden element. - Pa \u0161e drugi. Urejene sezname, kjer si elementi sledijo v vrstnem redu, pa ustvarimo tako, da pred elemente dodamo \u0161tevilko ali oznako vrstnega reda. Tudi take sezname lahko gnezdimo. \u0160tevilko vrstnega reda lahko podamo poljubno, saj ni potrebno, da so te v pravem vrstnem redu. 1. Prvi element. 2. Drugi element. 11. Prvi ugnezden element. 12. Pa \u0161e drugi. Besedilo v stilu kode pa v besedilo zapi\u0161emo med znakoma ` , ki ga na slovenski tipkovnici izberemo z AltGr + 7 . Spremenljivka `vrednost` in razred `podatki` . Tabelo vstavimo vrstico po vrstico, vsaka celica v vrstici se za\u010dne in kon\u010da z znakom | (na slovenski tipkovnici AltGr + W ). Tako je ena vrstica s tremi celicami zapisana na slede\u010d na\u010din. |Prva celica|Druga celica|Tretja celica| \u010crte med vrstice pa dodamo kar z znakom - namesto vsebine celice. Primer tabele s tremi stolpci in tremi vrsticami ter \u010drto med prvo in drugo vrstico izgleda slede\u010de. |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |-|-|-| |Druga vrstica|12|-42| |Tretja vrstica|-7|65| Za la\u017ejo razumevanje kode lahko to nekoliko oblikujemo s presledki. |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |----------------------------|-------------|--------------| |Druga vrstica |12 |-42 | |Tretja vrstica |-7 |65 | Hiperpovezave vstavljamo s slede\u010do kodo [Napis povezave](URL) , slike pa z zapisom ![Naziv slike](URL do datoteke slike) - razlika je v klicaju. [ Povezava do iskalnika Google ]( http://www.google.com/ ) ![ Primer slike ]( /imgs/slika.png ) \u010ce ne gre druga\u010de, pa lahko vsebino celic navodil oblikujemo tudi s preprosto HTML kodo. HTML in Markdown zapis lahko po \u017eelji me\u0161amo. Kon\u010den primer vsega skupaj bi zapisali s slede\u010do kodo, rezultat pa je prikazan na sliki pod kodo. ## Preizkus Markdown zapisa 1. Preizkusili smo oblikovanje pisav 1. __krepko__ in 2. _po\u0161evno_ pisavo 2. Pisanje seznamov - urejenih in neurejenih, - ter ugnezdenih 3. Dodajanje [ povezava ]( http://www.um.si/ ) 4. Dodajanje slik ![ FERI ]( https://feri.um.si/site/images/logo-feri.png ) 5. Pisanje v stilu `programske kode` 6. Dodajanje tabel |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |----------------------------|-------------|--------------| |Druga vrstica |12 |-42 | |Tretja vrstica |-7 |65 | 7. Tudi <a href=\"https://feri.um.si/\">HTML koda</a> deluje. Rezultat Markdown zapisa.","title":"Markdown v Jupyter zvezkih"},{"location":"pages/knjiga/02_okolje/#python-koda-v-jupyter-zvezkih","text":"\u010ce ima celica vsebino tipa Code , pa se vsebina smatra kot Python koda - primerno je tudi obarvana, deluje avtomatsko dopolnjevanje kode (angl. code auto-completion ) in rezultati kode se izpi\u0161ejo kar takoj pod celico. Sledi primer zapisa programske kode v celico. V prvi vrstici se uvozi knji\u017enica NumPy ter se poimenuje kot np za nadaljnjo uporabo. Sledi izpis niza znakov z ukazom print() - izpis sledi kar pod celico, kot je prikazano na slede\u010di sliki. Tretja vrstica vsebuje komentar v kodi, ki opisuje zadnjo vrstico. Zadnja vrstica pa vsebuje klic metode rand() knji\u017enice NumPy , ki vrne polje naklju\u010dnih \u0161tevil po podanih velikostih (v primeru je podana velikost polja s tremi vrsticami in dvema stolpcema). Ker ta vrstica vsebuje tudi zadnji ukaz, ki vra\u010da rezultat, se rezultat tega izpi\u0161e tudi pod celico. import numpy as np print ( 'Sledi izpis polja naklju\u010dnih \u0161tevil s tremi vrsticami in dvema stolpcema.' ) # Rezultat zadnjega ukaza v celici se izpi\u0161e kot rezultat celice. np . random . rand ( 3 , 2 ) Sledi izpis polja naklju\u010dnih \u0161tevil s tremi vrsticami in dvema stolpcema. array(0.27687911, 0.19824952, 0.06910974, 0.7548379, 0.64441325, 0.11951306) Prvi preizkus zapisa Python kode v Jupyter zvezek. Preizkusimo \u0161e uporabo knji\u017enice pandas , ki se uvozi v prvi vrstici kode. Kot \u017ee omenjeno pri prvi predstavitvi knji\u017enice, ta v zaledju uporablja podatke v obliki NumPy polj. To dejstvo lahko izkoristimo pri kreaciji nove strukture podatkov - v pandas -u je to razpredelnica, poimenovana DataFrame . Slede\u010da koda ustvari polje naklju\u010dnih \u0161tevil s 100 vrsticami in dvema stolpcema, ki pa slu\u017ei pri kreaciji razpredelnice DataFrame . V DataFrame razpredelnici lahko stolpce in vrstice poimenujemo, kot je prikazano s podanim parametrom columns . Zadnja vrstica kli\u010de metodo head() na\u0161e instance data_df razreda DataFrame , ki vrne prvih pet vrstic te razpredelnice, kot tudi ka\u017ee slika pod kodo. import numpy as np import pandas as pd data_np = np . random . rand ( 100 , 2 ) data_df = pd . DataFrame ( data_np , columns = [ 'Prvi stolpec' , 'Drugi stolpec' ]) data_df . head () Prvi stolpec Drugi stolpec 0 0.753358 0.029272 1 0.901894 0.846576 2 0.492876 0.533067 3 0.943245 0.538149 4 0.567161 0.537613 Preizkus delovanja knji\u017enice pandas . Sledi preizkus izrisa grafov s knji\u017enicama seaborn in Matplotlib . Slede\u010da koda prikazuje izris grafa raztrosa (angl. scatter plot ) za prej ustvarjeno razpredelnico data_df v obeh knj\u017enicah. seaborn Matplotlib import seaborn as sns sns . scatterplot ( x = data_df [ 'Prvi stolpec' ], y = data_df [ 'Drugi stolpec' ]) Preizkus izrisa grafa raztrosa s knji\u017enico seaborn . import matplotlib.pyplot as plt plt . scatter ( x = data_df [ 'Prvi stolpec' ], y = data_df [ 'Drugi stolpec' ]) Preizkus izrisa grafa raztrosa s knji\u017enico Matplotlib . Na prvi pogled se zdi uporaba seaborn in Matplotlib knji\u017enic podobna, kar tudi v resnici je. Nekatere tehnike grafi\u010dne predstavitve podatkov je s knji\u017enico seaborn la\u017eje narediti, medtem, ko je pri izrisu grafov Matplotlib mnogo bolj prilagodljiv. Sedaj bomo preizkusili \u0161e knji\u017enico scikit-learn , ki vsebuje razli\u010dne pristope, metode ter tehnike strojnega u\u010denja in predprocesiranja podatkov. Preden se lotimo uporabe metod strojnega u\u010denja, bomo tokrat preizkusili nalaganje prilo\u017eenih podatkov v knji\u017enici scikit-learn . Ena izmed prilo\u017eenih je podatkovna zbirka Iris , ki vsebuje podatke o cveto\u010dih rastlinah perunikah. V prvi vrstici se najprej nalo\u017ei modul knji\u017enice scikit-learn , v drugi vrstici pa se nalo\u017ei podatkovna zbirka Iris ter se shrani v spremenljivko iris_vse . Ker smo pri klicu metode podali as_frame=True , bodo rezultati v obliki pandas razpredelnice DataFrame . \u010ce tega ne bi podali, pa bi rezultat podatkov bil v obliki NumPy polja. Shranjeni podatki v spremenljivki iris_vse imajo ve\u010d vrednosti: vrednost data nam vrne podatke o ro\u017eah (vi\u0161ine in \u0161irine \u010da\u0161nih in cvetnih listov). Vrednost target pa nam vrne vrednosti razredov te podatkovne zbirke - za kateri tip perunike gre. Zadnja vrstica kode izpi\u0161e prvih pet vrstic podatkov. from sklearn.datasets import load_iris iris_vse = load_iris ( as_frame = True ) iris_podatki = iris_vse . data iris_razredi = iris_vse . target iris_podatki . head () sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 Za konec preizkusimo z izrisom grafa raztrosa, kjer bodo pike vsake izmed instanc pobarvane glede na podan razred te instance. sns . scatterplot ( x = 'sepal length (cm)' , y = 'sepal width (cm)' , data = iris_podatki , hue = iris_razredi ) Graf raztrosa podatkovne zbirke Iris . Tokrat smo za parameter data podali kar razpredelnico iris_podatki , ki je pandas DataFrame . Parametra x in y smo tokrat zapisali kot niz znakov - imena stolpcev iz podane razpredelnice. S parametrom hue dolo\u010dimo, kako se oznake na grafu pobarvajo. Lahko bi podali tudi ime stolpca (ki pa ga tokrat v razpredelnici data nimamo), ali pa kot lo\u010deno spremenljivko razpredelnice s temi podatki (kot smo naredili tokrat z iris_razredi ).","title":"Python koda v Jupyter zvezkih"},{"location":"pages/knjiga/02_okolje/#nalaganje-podatkov","text":"Da lahko izvedemo analize in uporabimo algoritme strojnega u\u010denja, pa potrebujemo podatke. Slede\u010da sekcija pregleda, kako nalo\u017eimo podatke v na\u0161 Python program oziroma Jupyter zvezek.","title":"Nalaganje podatkov"},{"location":"pages/knjiga/02_okolje/#nalaganje-prostodostopnih-podatkov","text":"\u017de v prej\u0161nji sekciji smo pokazali, kako lahko dostopamo do standardnih podatkovnih zbirk, ki se mnogokrat uporabljajo v procesu u\u010denja. To smo storili s pomo\u010djo knji\u017enice scikit-learn , saj so preko te knji\u017enice \u017ee name\u0161\u010dene \u0161tevilne podatkovne zbirke. Skozi celotno knjigo bomo najve\u010dkrat uporabili podatkovno zbirko Iris , v nalogah pa bodo uporabljene tudi Wine in Breast cancer . V vseh primerih gre za podatkovne zbirke, namenjene klasifikaciji. Nabor podatkovnih zbirk, ki so prilo\u017eene tej knji\u017enici, je dostopen na slede\u010dem naslovu za enostavne in majhne zbirke ter na tem naslovu za zbirke iz realnega sveta. Podatkovne zbirke iz scikit-learn vrnejo objekt s slede\u010dimi vrednostmi: data je razpredelnica s podatki vseh instanc, target je polje z re\u0161itvami vseh instanc, feature_names je seznam imen vseh atributov, ki so v data ter target_names je seznam imen re\u0161itev (\u010de gre za razrede). \u010ce nam prilo\u017eene prostodostopne podatkovne zbirke niso dovolj, lahko uporabimo poljubno podatkovno zbirko iz portala OpenML.org . Ta portal igra vlogo repozitorija za podatkovne zbirke. Te lahko uporabniki portala prosto nalagajo na portal in tudi do njih dostopajo. Knji\u017enica scikit-learn ponuja vmesnik za neposredno nalaganje teh datotek s pomo\u010djo metode fetch_openml , kateri podamo ime zbirke, kot ka\u017ee spodnja koda. from sklearn.datasets import fetch_openml podatki = fetch_openml ( name = 'ecoli' ) print ( podatki . feature_names ) ['mcg', 'gvh', 'lip', 'chg', 'aac', 'alm1', 'alm2'] Tudi pri nalaganju podatkovnih zbirk iz OpenML.org lahko fetch_openml metodi povemo, da \u017eelimo razpredelnico DataFrame . To storimo enako kot pri nalaganju prilo\u017eenih podatkovnih zbirk, s podajo as_frame=True . from sklearn.datasets import fetch_openml podatki = fetch_openml ( name = 'nursery' , as_frame = True ) print ( podatki . target . head ()) 0 recommend 1 priority 2 not_recom 3 recommend4 priority","title":"Nalaganje prostodostopnih podatkov"},{"location":"pages/knjiga/02_okolje/#nalaganje-lastnih-podatkov","text":"Podatke lahko v program oziroma zvezek nalo\u017eimo s pomo\u010djo knji\u017enice pandas , ki prepozna podatke v \u0161tevilnih razli\u010dnih formatih : tekstovne datoteke, kjer so podatki lo\u010deni z znaki (npr. z vejico v CSV, tabulatorjem ali drugim znakom), JSON format tekstovne datoteke, HTML spletne strani, iz katerih se podatki preberejo iz elementa <table> , Excel ali OpenDocument razpredelnice, Lastni\u0161ke SAS ali SPSS datoteke podatkov, podatkovne baze s SQL klicem ter lokalne odlo\u017ei\u0161\u010da (angl. clipboard ). Primer nalaganja iz CSV datoteke, kjer so vrednosti lo\u010dene s podpi\u010djem ; prikazuje spodnja koda. Vsak format ima svojo metodo za prebiranje datotek, saj so dolo\u010dene nastavitve formatu specifi\u010dne. Pri prebiranju iz tekstovnih datotek tako lahko podamo znak, ki lo\u010duje vrednosti s sep , znak lo\u010devanja decimalnih mest s dec , vrstico, ki predstavlja imena atributov, s header ali pa podamo ta imena kar v names . import pandas as pd podatki = pd . read_csv ( 'datoteka.txt' , sep = ';' , dec = ',' ) S pomo\u010djo tega je po prebiranju knjige mogo\u010de osvojeno znanje aplicirati na lastne podatke.","title":"Nalaganje lastnih podatkov"},{"location":"pages/knjiga/03_strojno_ucenje/","text":"Strojno u\u010denje Kam gremo po nasvet, ko se po\u010dutimo bolni? Se zate\u010demo k sosedu, sodelavki, prijatelju ali pa raje obi\u0161\u010demo zdravnika? Vsekakor je najbolje, da obi\u0161\u010demo zdravnika. Premislimo, zakaj. Kaj je prednost zdravnika v primerjavi z ostalimi na\u0161tetimi v danem problemu? Zdravnik ima najverjetneje mnogo ve\u010d znanja in izku\u0161enj pri diagnozi bolezni, saj se je kot \u0161tudent ve\u010d let u\u010dil prav o tej tematiki in v svoji delovni karieri \u017ee pridobil mnogotero izku\u0161enj. Tekom tega procesa pridobivanja znanja in izku\u0161enj tako predpostavimo, da se je zdravnik \u017ee usposobil za kakovostno spopadanje z danim problemom diagnoze bolezni. Model znanja Naslednji\u010d, ko sre\u010date zdravnika, ga kar povpra\u0161ajte o tem, kako se spopade z izzivom diagnoze pacienta na podlagi simptomov obolenj. Verjetno bo za\u010del s pregledom trenutnega in preteklega stanja pacienta. Naredil bo razli\u010dne meritve pacientovega telesa (telesna temperatura, krvni pritisk, hormonska slika, krvna slika, rentgen ...) in dodatno pregledal pacientovo kartoteko. Ob preu\u010devanju novih in preteklih meritev pacienta bo zdravnik najverjetneje \u017ee imel ustaljen postopek razmi\u0161ljanja pri postavitvi diagnoze. Recimo, da \u017eelimo analizirati delo na\u0161ega zdravnika in ga prosimo, da na kar se da razumljiv na\u010din opi\u0161e svoj proces odlo\u010danja. Najverjetneje zdravnik (ali strokovnjak drugih podro\u010dij) nima formalno dolo\u010denega procesa odlo\u010danja, ampak se pri tem zateka tudi k svoji intuiciji, ki jo je razvil po vseh letih \u0161olanja in izku\u0161enj. Pa vendar, \u010de bi se zdravnik odlo\u010dil zapisati svoj proces odlo\u010danja na papir, bi ta verjetno izgledal nekako tako: \u010ce je telesna temperature pacienta nad 40 \u00b0C, je pacient bolan . \u010ce je telesna temperature nad 38 \u00b0C in je sistoli\u010dni krvni pritisk nad 140 mHg, je pacient bolan . \u010ce je telesna temperature pod 38 \u00b0C, je pacient zdrav . Svoj proces diagnoze bi zdravnik zapisal v obliki pravil, katerim bi sledil bodisi po vrsti ali pa bi jih upo\u0161teval vse hkrati. Tem pravilom pravimo model znanja (angl. knowledge model ). Kateri drug zdravnik pa bi svoj proces odlo\u010danja mogo\u010de la\u017eje zapisal v obliki odlo\u010ditvenega drevesa. Model znanja v obliki odlo\u010ditvenega drevesa. Pri interpretaciji takega drevesa bi zdravnik za\u010del na vrhu (korenu) drevesa in se z odgovarjanjem na vpra\u0161anja v posameznem vozli\u0161\u010du drevesa pri pregledu posameznega pacienta premikal vse do listov drevesa. Listi pa mu povedo odgovor glede diagnoze pacienta. Nek tretji zdravnik pa bolj\u0161e razmi\u0161lja, ko svoj model znanja predstavi v obliki matemati\u010dnih formul. Slede\u010da slika prikazuje graf, kjer je na horizontalni X osi telesna temperatura pacienta, na vertikalni Y osi pa je krvni pritisk pacienta. Matemati\u010dna formula deli prostor na dva dela - na prostor, kjer se nahajajo zdravi in na prostor, kjer se nahajajo bolni pacienti. Model znanja v obliki matemati\u010dne funkcije. Zadnji zdravnik pa bi deloval popolnoma druga\u010de kot prej\u0161nji trije. Ta ne bi znal na papir zapisati svojega modela znanja, saj se odlo\u010da po druga\u010dnem postopku - pregleda svoje prej\u0161nje paciente iz bogate dvajsetletne kariere in novega pacienta primerja z njimi. Ko najde primere \u017ee diagnosticiranih pacientov, ki so novemu pacientu najbolj podobni, preprosto pogleda, kak\u0161na je bila njihova diagnoza in temu novemu poda mnenje o njegovem stanju bolezni, ki je skladna s prej\u0161njimi (podobnimi) pacienti. Ta proces je prikazan slede\u010de. Model znanja v obliki sprotne primerjave z \u017ee re\u0161enimi primeri. Vsi \u0161tirje opisani na\u010dini odlo\u010danja zdravnikov v resnici prikazujejo razli\u010dne na\u010dine predstavitve modela znanja. Teh je v realnosti \u0161e mnogo ve\u010d, tekom te knjige pa bomo podrobneje spoznali zadnji na\u010din zapisa modela znanja - primerjavo novih primerov s starimi, \u017ee re\u0161enimi. Ekspertni sistem Gradnjo modela znanja zdravnika vizualno prikazuje spodnja slika. Najverjetneje preberejo zdravniki tekom \u0161tudija medicine in delovne kariere mnogo knjig, znanstvenih in strokovnih \u010dlankov ter pridobijo mnogotere izku\u0161nje. To ka\u017ee zgornji del slike. Rezultat takega procesa je model znanja (seveda ne formalno zapisan). Ob pregledu novega pacienta uporabijo ta model znanja, da pridejo do kon\u010dne diagnoze pacienta, kar je ponazorjeno s spodnjim delom slike. Poenostavljen prikaz procesa gradnje in uporabe modela znanja. Seveda lahko zdravnikov model znanja prepi\u0161emo v programsko kodo in tako uporabimo njegov model znanja v vsakdanjih informacijskih sistemih, kot ka\u017ee slede\u010da slika. To je standardni postopek pri gradnji ekspertnih sistemov (angl. expert systems ) tj. informacijskih sistemov, ki uporabijo model znanja, ki so jih zgradili strokovnjaki (na primer zdravniki). Z digitalizacijo modela znanja pridobimo zmo\u017enost hitrej\u0161e uporabe modela znanja in posledi\u010dnega odlo\u010danja, saj ra\u010dunalnik lahko tak model znanja uporabi tudi milijonkrat v sekundi. Proces uporabe modela znanja v ekspertnem sistemu. Pri ekspertnem sistemu \u0161e vedno ne govorimo o umetni inteligenci ali strojnem u\u010denju, saj ra\u010dunalnik ni sodeloval pri procesu u\u010denja (gradnje modela znanja), ampak je le neumen stroj, ki sledi modelu znanja, zapisanem v obliki if in for stavkov. Inteligentni sistem Premislimo, kako bi lahko v prej\u0161njem procesu vklju\u010dili ra\u010dunalnik v proces gradnje modela znanja. Namesto da \u010dlovek postane strokovnjak s pomo\u010djo preu\u010devanja literature in nabiranja izku\u0161enj, bomo tokrat uporabili kar ra\u010dunalnik za gradnjo modela. Ampak kako? Ra\u010dunalnik ne razume prebranih knjig in ne more pridobivati izku\u0161enj. Tukaj pa uporabimo enak pristop u\u010denja, kot ga uporabijo ljudje v situacijah, kjer se u\u010dijo s pomo\u010djo opazovanja - pri delu opazujejo strokovnjake in sku\u0161ajo ugotoviti, kako strokovnjaki delajo, da lahko to kasneje posnemajo. Podobno pot uberemo, ko \u017eelimo pri gradnji modela uporabiti ra\u010dunalnik kar ka\u017ee slika spodaj. Namesto iz knjig in izku\u0161enj, ra\u010dunalnik razbere vzorce iz dela strokovnjakov, ki pa je v tem primeru sestavljeno iz \u017ee re\u0161enih problemov. \u010ce ra\u010dunalniku podamo kartoteke \u017ee diagnosticiranih pacientov, bo ta lahko iz teh kartotek razbral vzorce, ki so zna\u010dilni za bolne paciente in vzorce, ki so zna\u010dilni za zdrave paciente. Seveda, enako kot vajenec ne bo postal mojster pri enournem opazovanju strokovnjaka pri delu, tako tudi ra\u010dunalnik ne uspe najti vzorcev iz le nekaj primerov \u017ee diagnosticiranih pacientov. Koliko podatkov pa bo dovolj? \u010ce je delo mojstra zelo zapleteno, bo vajenec potreboval nekaj let opazovanja, da bo ugotovil pravi na\u010din dela tega strokovnjaka. \u010ce pa vajencu ka\u017eemo, kako nalepiti obli\u017e na rano, pa bo vajenec vzorec za reprodukcijo videnega odkril \u017ee po nekaj minutah. Podobno je tudi pri ra\u010dunalniku. Koli\u010dina potrebnih re\u0161enih primerov je odvisna od koli\u010dine in kompleksnosti vzorcev - ve\u010d kot je vzorcev in bolj so ti kompleksni, ve\u010d podatkov bo ra\u010dunalnik potreboval, da bo vzorce prepoznal. Ra\u010dunalni\u0161ki algoritem, ki iz podatkov razbira vzorce, imenujemo algoritem strojnega u\u010denja (angl. machine learning algorithm ). Ta algoritem je zadol\u017een, da namesto \u010dloveka ustvari model znanja, ki ga kasneje lahko pri svojem delu uporabita tako \u010dlovek, kakor tudi ra\u010dunalnik. Sistem, ki vklju\u010duje algoritem strojnega u\u010denja za gradnjo modela znanja, pa imenujemo inteligentni sistem (angl. _intelligent system). Proces u\u010denja in uporabe modela znanja v inteligentnem sistemu. Podatki Podatki, uporabljeni v algoritmu strojnega u\u010denja za namen kreacije modela znanja, so zdru\u017eeni v podatkovne mno\u017eice (angl. datasets ) in so lahko v obliki preproste strukturirane tekstovne datoteke ali podatkovne baze poljubne strukture (relacijske, objektne ali dokumentne podatkovne baze). Najenostavnej\u0161a struktura podatkov, namenjenih za strojno u\u010denje ima obliko, ki je prikazana v spodnji tabeli. Vsako vrstico v tekstovnem dokumentu ali vsak primerek podatkovne baze imenujemo u\u010dna instanca ali u\u010dni primerek (angl. learning example ali learning instance ). Vsaka instanca je opisana z mno\u017eico karakteristik imenovanih atributi (angl. features } ali tudi neodvisne spremenljivke oziroma zna\u010dilnice . Prostor atributov (angl. feature space } \\(F\\) je vektor vseh atributov. Vi\u0161ina Te\u017ea Starost \u2026 181 92 45 178 71 27 168 73 65 \u2026 Strojno u\u010denje Strojno u\u010denje (angl. machine learning } zajema tehnike, kjer se ra\u010dunalnik nau\u010di re\u0161evanja specifi\u010dnih in ozko usmerjenih nalog iz podatkov - pravimo, da odlo\u010ditve strojnega u\u010denja temeljijo na podatkih. Tehnike strojnega u\u010denja uvr\u0161\u010damo v krovno podro\u010dje umetne inteligence (angl. artificial intelligence ), ki pa pokriva mnogo \u0161ir\u0161e raziskovalno podro\u010dje. Podro\u010dje, povezano s strojnim u\u010denjem, je tako imenovano podatkovno rudarjenje (angl. data mining ), kjer s pomo\u010djo razli\u010dnih tehnik, med drugimi tudi s strojnim u\u010denjem, obdelujemo in preu\u010dujemo podatke ter posku\u0161amo iz njih razbrati vzorce in posledi\u010dno novo znanje. Izraz podatkovno rudarjenje uporabljamo, ko se sre\u010damo s problemom uporabe samih metod strojnega u\u010denja kot orodij za re\u0161evanje drugih problemov in ne s samo implementacijo teh. Strojno u\u010denje delimo na \u0161tiri podro\u010dja glede na stopnjo nadzora nad u\u010denjem 1 : Nadzorovano u\u010denje (angl. supervised learning ) se uporablja, ko \u017eelimo, da se ra\u010dunalnik nau\u010di klasificirati (razvr\u0161\u010dati) podatke v vnaprej dolo\u010dene razrede ali jim pripisovati \u0161tevilske vrednosti. Temu re\u010demo nadzorovano u\u010denje, ker se stroj u\u010di na re\u0161enih podatkih (z znanimi razredi ali vrednostmi) in ker lahko nadzorujemo kakovost dobljenih modelov znanja. Pri nadzorovanem u\u010denju imamo dve nalogi, ki ju mora stroj opravljati: regresijo in klasifikacijo. Regresija (angl. regression ) se uporablja, ko se ra\u010dunalnik na podlagi podanih karakteristik (neodvisnih spremenljivk) nau\u010di napovedovanja numeri\u010dnih vrednosti (odvisne spremenljivke). Primer takega problema bi bil napovedovanje cene delnice ali koli\u010dine de\u017eja glede na znane podatke. Z regresijo se v tej knjigi ne bomo ukvarjali, zato se v podrobnej\u0161e razlage ne bomo spu\u0161\u010dali. Klasifikacija (angl. classification ) pa se uporablja, ko se ra\u010dunalnik iz re\u0161enih podatkov nau\u010di te razvr\u0161\u010dati v vnaprej dolo\u010dene razrede. Primer smo \u017ee omenjali, ko smo govorili o diagnozi pacientov in ga bomo podrobneje opisali v nadaljevanju. Nenadzorovano u\u010denje (angl. unsupervised learning ) uporabimo, ko \u017eelimo odkriti \u0161e neznane povezave med podatki in strukturo teh podatkov. V tem primeru na\u0161i podatki ne vsebujejo re\u0161itve, saj re\u0161itev \u0161e ne poznamo, in posledi\u010dno kakovosti takih modelov ne moremo nadzirati. Nenadzorovano u\u010denje ima ve\u010d nalog, ki se jih stroj nau\u010di: gru\u010denje in sprememba strukture podatkov. Gru\u010denje (angl. clustering ) je tehnika, kjer ra\u010dunalnik najde vzorce, ki povezujejo podatke v gru\u010de, in tako najde doslej neznane povezave med podatki. Definicija teh gru\u010d ni vnaprej znana, a v vsakem primeru ra\u010dunalnik uporabi eno izmed tehnik, da gru\u010de zdru\u017eujejo podobne in povezane podatke skupaj. \u0160tevilo gru\u010d je lahko vnaprej dolo\u010deno ali pa se odlo\u010ditev o \u0161tevilu gru\u010d prepusti ra\u010dunalniku. Primer gru\u010denja je iskanje profilov strank v trgovini, kjer imajo stranke v isti gru\u010di podobne nakupovalne navade. Spreminjanje in preu\u010devanje strukture podatkov pa zdru\u017euje tehnike, ki se ukvarjajo s transformacijo, preslikavo, zdru\u017eevanjem in selekcijo posameznih karakteristik iz podatkov. Podrobneje teh tehnik ne bomo obravnavali v okviru te knjige. Delno nadzorovano u\u010denje (angl. semi-supervised learning ) je srednja pot med nadzorovanim in nenadzorovanim u\u010denjem. Pri tej tehniki \u0161e vedno klasificiramo podatke v vnaprej podane razrede, pri \u010demer si pomagamo z novimi karakteristikami podatkov, ki pa so rezultat nenadzorovanega u\u010denja, ali pa uporabljamo le delno ozna\u010dene podatke (na primer, \u010de imamo v u\u010dni mno\u017eici le bolne paciente). Tipi\u010dna uporaba delno nadzorovanega u\u010denja je iskanje anomalij, kjer poznamo samo lastnosti normalnih podatkov in iz tega sklepamo, kaj je normalno - vse, kar je druga\u010dno, pa je anomalija. Okrepitveno u\u010denje (angl. reinforcement learning ) je raz\u0161irjeno nadzorovano ali nenadzorovano u\u010denje, kjer se ra\u010dunalnik u\u010di na podlagi nagrad ali kazni glede na izide u\u010denja. Pri nagrajevanju in kaznovanju lahko sodeluje \u010dlovek ali pa je nagrada podeljena ra\u010dunsko. V primeru sodelovanja \u010dloveka, ta poda dodatne informacije o samih podatkih, ali pa v iterativnem postopku poda mnenje o kakovosti modela. Tega pristopa strojnega u\u010denja v okviru te knjige ne bomo obravnavali. Jiawei Han, M.K. and Pei, J., 2011. Data mining: concepts and techniques: concepts and techniques. \u21a9","title":"Strojno u\u010denje"},{"location":"pages/knjiga/03_strojno_ucenje/#strojno-ucenje","text":"Kam gremo po nasvet, ko se po\u010dutimo bolni? Se zate\u010demo k sosedu, sodelavki, prijatelju ali pa raje obi\u0161\u010demo zdravnika? Vsekakor je najbolje, da obi\u0161\u010demo zdravnika. Premislimo, zakaj. Kaj je prednost zdravnika v primerjavi z ostalimi na\u0161tetimi v danem problemu? Zdravnik ima najverjetneje mnogo ve\u010d znanja in izku\u0161enj pri diagnozi bolezni, saj se je kot \u0161tudent ve\u010d let u\u010dil prav o tej tematiki in v svoji delovni karieri \u017ee pridobil mnogotero izku\u0161enj. Tekom tega procesa pridobivanja znanja in izku\u0161enj tako predpostavimo, da se je zdravnik \u017ee usposobil za kakovostno spopadanje z danim problemom diagnoze bolezni.","title":"Strojno u\u010denje"},{"location":"pages/knjiga/03_strojno_ucenje/#model-znanja","text":"Naslednji\u010d, ko sre\u010date zdravnika, ga kar povpra\u0161ajte o tem, kako se spopade z izzivom diagnoze pacienta na podlagi simptomov obolenj. Verjetno bo za\u010del s pregledom trenutnega in preteklega stanja pacienta. Naredil bo razli\u010dne meritve pacientovega telesa (telesna temperatura, krvni pritisk, hormonska slika, krvna slika, rentgen ...) in dodatno pregledal pacientovo kartoteko. Ob preu\u010devanju novih in preteklih meritev pacienta bo zdravnik najverjetneje \u017ee imel ustaljen postopek razmi\u0161ljanja pri postavitvi diagnoze. Recimo, da \u017eelimo analizirati delo na\u0161ega zdravnika in ga prosimo, da na kar se da razumljiv na\u010din opi\u0161e svoj proces odlo\u010danja. Najverjetneje zdravnik (ali strokovnjak drugih podro\u010dij) nima formalno dolo\u010denega procesa odlo\u010danja, ampak se pri tem zateka tudi k svoji intuiciji, ki jo je razvil po vseh letih \u0161olanja in izku\u0161enj. Pa vendar, \u010de bi se zdravnik odlo\u010dil zapisati svoj proces odlo\u010danja na papir, bi ta verjetno izgledal nekako tako: \u010ce je telesna temperature pacienta nad 40 \u00b0C, je pacient bolan . \u010ce je telesna temperature nad 38 \u00b0C in je sistoli\u010dni krvni pritisk nad 140 mHg, je pacient bolan . \u010ce je telesna temperature pod 38 \u00b0C, je pacient zdrav . Svoj proces diagnoze bi zdravnik zapisal v obliki pravil, katerim bi sledil bodisi po vrsti ali pa bi jih upo\u0161teval vse hkrati. Tem pravilom pravimo model znanja (angl. knowledge model ). Kateri drug zdravnik pa bi svoj proces odlo\u010danja mogo\u010de la\u017eje zapisal v obliki odlo\u010ditvenega drevesa. Model znanja v obliki odlo\u010ditvenega drevesa. Pri interpretaciji takega drevesa bi zdravnik za\u010del na vrhu (korenu) drevesa in se z odgovarjanjem na vpra\u0161anja v posameznem vozli\u0161\u010du drevesa pri pregledu posameznega pacienta premikal vse do listov drevesa. Listi pa mu povedo odgovor glede diagnoze pacienta. Nek tretji zdravnik pa bolj\u0161e razmi\u0161lja, ko svoj model znanja predstavi v obliki matemati\u010dnih formul. Slede\u010da slika prikazuje graf, kjer je na horizontalni X osi telesna temperatura pacienta, na vertikalni Y osi pa je krvni pritisk pacienta. Matemati\u010dna formula deli prostor na dva dela - na prostor, kjer se nahajajo zdravi in na prostor, kjer se nahajajo bolni pacienti. Model znanja v obliki matemati\u010dne funkcije. Zadnji zdravnik pa bi deloval popolnoma druga\u010de kot prej\u0161nji trije. Ta ne bi znal na papir zapisati svojega modela znanja, saj se odlo\u010da po druga\u010dnem postopku - pregleda svoje prej\u0161nje paciente iz bogate dvajsetletne kariere in novega pacienta primerja z njimi. Ko najde primere \u017ee diagnosticiranih pacientov, ki so novemu pacientu najbolj podobni, preprosto pogleda, kak\u0161na je bila njihova diagnoza in temu novemu poda mnenje o njegovem stanju bolezni, ki je skladna s prej\u0161njimi (podobnimi) pacienti. Ta proces je prikazan slede\u010de. Model znanja v obliki sprotne primerjave z \u017ee re\u0161enimi primeri. Vsi \u0161tirje opisani na\u010dini odlo\u010danja zdravnikov v resnici prikazujejo razli\u010dne na\u010dine predstavitve modela znanja. Teh je v realnosti \u0161e mnogo ve\u010d, tekom te knjige pa bomo podrobneje spoznali zadnji na\u010din zapisa modela znanja - primerjavo novih primerov s starimi, \u017ee re\u0161enimi.","title":"Model znanja"},{"location":"pages/knjiga/03_strojno_ucenje/#ekspertni-sistem","text":"Gradnjo modela znanja zdravnika vizualno prikazuje spodnja slika. Najverjetneje preberejo zdravniki tekom \u0161tudija medicine in delovne kariere mnogo knjig, znanstvenih in strokovnih \u010dlankov ter pridobijo mnogotere izku\u0161nje. To ka\u017ee zgornji del slike. Rezultat takega procesa je model znanja (seveda ne formalno zapisan). Ob pregledu novega pacienta uporabijo ta model znanja, da pridejo do kon\u010dne diagnoze pacienta, kar je ponazorjeno s spodnjim delom slike. Poenostavljen prikaz procesa gradnje in uporabe modela znanja. Seveda lahko zdravnikov model znanja prepi\u0161emo v programsko kodo in tako uporabimo njegov model znanja v vsakdanjih informacijskih sistemih, kot ka\u017ee slede\u010da slika. To je standardni postopek pri gradnji ekspertnih sistemov (angl. expert systems ) tj. informacijskih sistemov, ki uporabijo model znanja, ki so jih zgradili strokovnjaki (na primer zdravniki). Z digitalizacijo modela znanja pridobimo zmo\u017enost hitrej\u0161e uporabe modela znanja in posledi\u010dnega odlo\u010danja, saj ra\u010dunalnik lahko tak model znanja uporabi tudi milijonkrat v sekundi. Proces uporabe modela znanja v ekspertnem sistemu. Pri ekspertnem sistemu \u0161e vedno ne govorimo o umetni inteligenci ali strojnem u\u010denju, saj ra\u010dunalnik ni sodeloval pri procesu u\u010denja (gradnje modela znanja), ampak je le neumen stroj, ki sledi modelu znanja, zapisanem v obliki if in for stavkov.","title":"Ekspertni sistem"},{"location":"pages/knjiga/03_strojno_ucenje/#inteligentni-sistem","text":"Premislimo, kako bi lahko v prej\u0161njem procesu vklju\u010dili ra\u010dunalnik v proces gradnje modela znanja. Namesto da \u010dlovek postane strokovnjak s pomo\u010djo preu\u010devanja literature in nabiranja izku\u0161enj, bomo tokrat uporabili kar ra\u010dunalnik za gradnjo modela. Ampak kako? Ra\u010dunalnik ne razume prebranih knjig in ne more pridobivati izku\u0161enj. Tukaj pa uporabimo enak pristop u\u010denja, kot ga uporabijo ljudje v situacijah, kjer se u\u010dijo s pomo\u010djo opazovanja - pri delu opazujejo strokovnjake in sku\u0161ajo ugotoviti, kako strokovnjaki delajo, da lahko to kasneje posnemajo. Podobno pot uberemo, ko \u017eelimo pri gradnji modela uporabiti ra\u010dunalnik kar ka\u017ee slika spodaj. Namesto iz knjig in izku\u0161enj, ra\u010dunalnik razbere vzorce iz dela strokovnjakov, ki pa je v tem primeru sestavljeno iz \u017ee re\u0161enih problemov. \u010ce ra\u010dunalniku podamo kartoteke \u017ee diagnosticiranih pacientov, bo ta lahko iz teh kartotek razbral vzorce, ki so zna\u010dilni za bolne paciente in vzorce, ki so zna\u010dilni za zdrave paciente. Seveda, enako kot vajenec ne bo postal mojster pri enournem opazovanju strokovnjaka pri delu, tako tudi ra\u010dunalnik ne uspe najti vzorcev iz le nekaj primerov \u017ee diagnosticiranih pacientov. Koliko podatkov pa bo dovolj? \u010ce je delo mojstra zelo zapleteno, bo vajenec potreboval nekaj let opazovanja, da bo ugotovil pravi na\u010din dela tega strokovnjaka. \u010ce pa vajencu ka\u017eemo, kako nalepiti obli\u017e na rano, pa bo vajenec vzorec za reprodukcijo videnega odkril \u017ee po nekaj minutah. Podobno je tudi pri ra\u010dunalniku. Koli\u010dina potrebnih re\u0161enih primerov je odvisna od koli\u010dine in kompleksnosti vzorcev - ve\u010d kot je vzorcev in bolj so ti kompleksni, ve\u010d podatkov bo ra\u010dunalnik potreboval, da bo vzorce prepoznal. Ra\u010dunalni\u0161ki algoritem, ki iz podatkov razbira vzorce, imenujemo algoritem strojnega u\u010denja (angl. machine learning algorithm ). Ta algoritem je zadol\u017een, da namesto \u010dloveka ustvari model znanja, ki ga kasneje lahko pri svojem delu uporabita tako \u010dlovek, kakor tudi ra\u010dunalnik. Sistem, ki vklju\u010duje algoritem strojnega u\u010denja za gradnjo modela znanja, pa imenujemo inteligentni sistem (angl. _intelligent system). Proces u\u010denja in uporabe modela znanja v inteligentnem sistemu.","title":"Inteligentni sistem"},{"location":"pages/knjiga/03_strojno_ucenje/#podatki","text":"Podatki, uporabljeni v algoritmu strojnega u\u010denja za namen kreacije modela znanja, so zdru\u017eeni v podatkovne mno\u017eice (angl. datasets ) in so lahko v obliki preproste strukturirane tekstovne datoteke ali podatkovne baze poljubne strukture (relacijske, objektne ali dokumentne podatkovne baze). Najenostavnej\u0161a struktura podatkov, namenjenih za strojno u\u010denje ima obliko, ki je prikazana v spodnji tabeli. Vsako vrstico v tekstovnem dokumentu ali vsak primerek podatkovne baze imenujemo u\u010dna instanca ali u\u010dni primerek (angl. learning example ali learning instance ). Vsaka instanca je opisana z mno\u017eico karakteristik imenovanih atributi (angl. features } ali tudi neodvisne spremenljivke oziroma zna\u010dilnice . Prostor atributov (angl. feature space } \\(F\\) je vektor vseh atributov. Vi\u0161ina Te\u017ea Starost \u2026 181 92 45 178 71 27 168 73 65 \u2026","title":"Podatki"},{"location":"pages/knjiga/03_strojno_ucenje/#strojno-ucenje_1","text":"Strojno u\u010denje (angl. machine learning } zajema tehnike, kjer se ra\u010dunalnik nau\u010di re\u0161evanja specifi\u010dnih in ozko usmerjenih nalog iz podatkov - pravimo, da odlo\u010ditve strojnega u\u010denja temeljijo na podatkih. Tehnike strojnega u\u010denja uvr\u0161\u010damo v krovno podro\u010dje umetne inteligence (angl. artificial intelligence ), ki pa pokriva mnogo \u0161ir\u0161e raziskovalno podro\u010dje. Podro\u010dje, povezano s strojnim u\u010denjem, je tako imenovano podatkovno rudarjenje (angl. data mining ), kjer s pomo\u010djo razli\u010dnih tehnik, med drugimi tudi s strojnim u\u010denjem, obdelujemo in preu\u010dujemo podatke ter posku\u0161amo iz njih razbrati vzorce in posledi\u010dno novo znanje. Izraz podatkovno rudarjenje uporabljamo, ko se sre\u010damo s problemom uporabe samih metod strojnega u\u010denja kot orodij za re\u0161evanje drugih problemov in ne s samo implementacijo teh. Strojno u\u010denje delimo na \u0161tiri podro\u010dja glede na stopnjo nadzora nad u\u010denjem 1 : Nadzorovano u\u010denje (angl. supervised learning ) se uporablja, ko \u017eelimo, da se ra\u010dunalnik nau\u010di klasificirati (razvr\u0161\u010dati) podatke v vnaprej dolo\u010dene razrede ali jim pripisovati \u0161tevilske vrednosti. Temu re\u010demo nadzorovano u\u010denje, ker se stroj u\u010di na re\u0161enih podatkih (z znanimi razredi ali vrednostmi) in ker lahko nadzorujemo kakovost dobljenih modelov znanja. Pri nadzorovanem u\u010denju imamo dve nalogi, ki ju mora stroj opravljati: regresijo in klasifikacijo. Regresija (angl. regression ) se uporablja, ko se ra\u010dunalnik na podlagi podanih karakteristik (neodvisnih spremenljivk) nau\u010di napovedovanja numeri\u010dnih vrednosti (odvisne spremenljivke). Primer takega problema bi bil napovedovanje cene delnice ali koli\u010dine de\u017eja glede na znane podatke. Z regresijo se v tej knjigi ne bomo ukvarjali, zato se v podrobnej\u0161e razlage ne bomo spu\u0161\u010dali. Klasifikacija (angl. classification ) pa se uporablja, ko se ra\u010dunalnik iz re\u0161enih podatkov nau\u010di te razvr\u0161\u010dati v vnaprej dolo\u010dene razrede. Primer smo \u017ee omenjali, ko smo govorili o diagnozi pacientov in ga bomo podrobneje opisali v nadaljevanju. Nenadzorovano u\u010denje (angl. unsupervised learning ) uporabimo, ko \u017eelimo odkriti \u0161e neznane povezave med podatki in strukturo teh podatkov. V tem primeru na\u0161i podatki ne vsebujejo re\u0161itve, saj re\u0161itev \u0161e ne poznamo, in posledi\u010dno kakovosti takih modelov ne moremo nadzirati. Nenadzorovano u\u010denje ima ve\u010d nalog, ki se jih stroj nau\u010di: gru\u010denje in sprememba strukture podatkov. Gru\u010denje (angl. clustering ) je tehnika, kjer ra\u010dunalnik najde vzorce, ki povezujejo podatke v gru\u010de, in tako najde doslej neznane povezave med podatki. Definicija teh gru\u010d ni vnaprej znana, a v vsakem primeru ra\u010dunalnik uporabi eno izmed tehnik, da gru\u010de zdru\u017eujejo podobne in povezane podatke skupaj. \u0160tevilo gru\u010d je lahko vnaprej dolo\u010deno ali pa se odlo\u010ditev o \u0161tevilu gru\u010d prepusti ra\u010dunalniku. Primer gru\u010denja je iskanje profilov strank v trgovini, kjer imajo stranke v isti gru\u010di podobne nakupovalne navade. Spreminjanje in preu\u010devanje strukture podatkov pa zdru\u017euje tehnike, ki se ukvarjajo s transformacijo, preslikavo, zdru\u017eevanjem in selekcijo posameznih karakteristik iz podatkov. Podrobneje teh tehnik ne bomo obravnavali v okviru te knjige. Delno nadzorovano u\u010denje (angl. semi-supervised learning ) je srednja pot med nadzorovanim in nenadzorovanim u\u010denjem. Pri tej tehniki \u0161e vedno klasificiramo podatke v vnaprej podane razrede, pri \u010demer si pomagamo z novimi karakteristikami podatkov, ki pa so rezultat nenadzorovanega u\u010denja, ali pa uporabljamo le delno ozna\u010dene podatke (na primer, \u010de imamo v u\u010dni mno\u017eici le bolne paciente). Tipi\u010dna uporaba delno nadzorovanega u\u010denja je iskanje anomalij, kjer poznamo samo lastnosti normalnih podatkov in iz tega sklepamo, kaj je normalno - vse, kar je druga\u010dno, pa je anomalija. Okrepitveno u\u010denje (angl. reinforcement learning ) je raz\u0161irjeno nadzorovano ali nenadzorovano u\u010denje, kjer se ra\u010dunalnik u\u010di na podlagi nagrad ali kazni glede na izide u\u010denja. Pri nagrajevanju in kaznovanju lahko sodeluje \u010dlovek ali pa je nagrada podeljena ra\u010dunsko. V primeru sodelovanja \u010dloveka, ta poda dodatne informacije o samih podatkih, ali pa v iterativnem postopku poda mnenje o kakovosti modela. Tega pristopa strojnega u\u010denja v okviru te knjige ne bomo obravnavali. Jiawei Han, M.K. and Pei, J., 2011. Data mining: concepts and techniques: concepts and techniques. \u21a9","title":"Strojno u\u010denje"},{"location":"pages/knjiga/04_klasifikacija/","text":"Klasifikacija Osrednja metoda strojnega u\u010denja te knjige je metoda klasifikacije , kjer se ra\u010dunalnik nau\u010di klasificirati instance v vnaprej dolo\u010dene razrede. Z regresijo napovemo \u0161tevilske vrednosti in so odlo\u010ditve zvezne, pri klasifikaciji pa napovedujemo nominalne vrednosti - diskretne razrede. Klasifikacija se uporablja v primerih, kot so razpoznava vzorcev na slikah, prepre\u010devanje prevar, zaznavanje neza\u017eelene po\u0161te in diagnosticiranje bolezni. \u010ce ra\u010dunalnik podatke deli v dva razreda, govorimo o binarni klasifikaciji , \u010de pa ra\u010dunalnik klasificira v ve\u010d razredov, pa imamo opravka z ve\u010drazredno klasifikacijo . 1 Obstaja na tiso\u010de razli\u010dnih algoritmov klasifikacije, ki jih v grobem delimo glede na to, v kak\u0161ni obliki shranijo model znanja 1 2 : matemati\u010dne formule in porazdelitve (logisti\u010dna regresija, naivni Bayesov klasifikator, metoda podpornih vektorjev), odlo\u010ditvena drevesa (CART, C4.5, ID3, evolucijska drevesa), odlo\u010ditvena pravila (RIPPER, PART, evolucijska pravila), umetne nevronske mre\u017ee (konvolucijske, rekurzivne) in klasifikatorji na podlagi podobnosti ( k najbli\u017ejih sosedov ). Nekatere metode zgradijo model znanja, ki se uporablja pri klasifikaciji novih instanc, ne da bi bil potreben vpogled v prej podane podatke. Takim metodam pravimo metode takoj\u0161njega u\u010denja (angl. eager learning ), saj zgradijo model znanja takoj in ga kasneje ne prilagajajo. Nasprotno, pa nekatere metode, kot na primer k najbli\u017ejih sosedov, ne ustvarijo u\u010dnega modela, ampak za vsako klasifikacijo nove instance ponovno naredijo pregled \u017ee prej podanih podatkov. Te metode uporabljajo leno u\u010denje (angl. lazy learning ), saj se u\u010dijo sproti po potrebi. Prav ta pristop bomo kasneje pregledali pri preizkusu na\u0161ega klasifikatorja k najbli\u017ejih sosedov. Matemati\u010dna definicija pojmov V nadaljevanju bomo za metodo klasifikacije uporabljali slede\u010do definicijo instanc. Ena instanca je par \\((x_i,y_i)\\) , kjer je \\(x_i\\) vektor vrednosti (atributov) te instance, \\(y_i\\) pa je dejanski razred instance (skalarna vrednost). U\u010dna mno\u017eica \\(X\\) je definirana kot mno\u017eica vseh instanc, na katerih se algoritem u\u010di, sestavljena je iz \\(n\\) instanc in je definirana, kot prikazuje spodnja ena\u010dba. \\[\\begin{align*} \\begin{split} X &= \\left\\lbrace \\left( x_1,y_1 \\right),\\left( x_2,y_2 \\right), \\dots ,\\left( x_n,y_n \\right) \\right\\rbrace \\\\ x_i &= (x_i^1,x_i^2, \\dots, x_i^l) \\\\ y_i &\\in \\left\\lbrace razred_1, razred_2, \\dots , razred_k \\right\\rbrace \\\\ n &= \\text{\u0161tevilo instanc} \\\\ l &= \\text{\u0161tevilo atributov} \\\\ k &= \\text{\u0161tevilo razredov} \\end{split} \\end{align*}\\] Vrednost \\(x_i\\) je vektor atributov \\((x_i^1,x_i^2, \\dots, x_i^l)\\) velikosti \\(l\\) , ki je definiran v prostoru atributov \\(F\\) , razred instance \\(y_i\\) pa je vrednost iz nabora vseh mo\u017enih razredov v velikosti \\(k\\) . Cilj algoritmov klasifikacije je, da ob dani u\u010dni mno\u017eici \\(X\\) najdejo slede\u010do funkcijo: \\[\\begin{align*} h: x_i \\rightarrow y_i \\end{align*}\\] za vsak \\(i \\in \\left[ 1,n\\right]\\) , tako da bo model znanja klasifikacije \\(h(x_i)\\) dovolj dober napovedovalec razreda \\(y_i\\) . Proces klasifikacije prikazuje slede\u010da slika, kjer iz podatkovne mno\u017eice \\(X\\) algoritem klasifikacije ustvari model znanja klasifikacije \\(h\\) . Ta model preslika vhode instance \\(x^1, x^2, \\dots, x^l\\) v razred \\(y\\) .. Splo\u0161ni proces klasifikacije. Ob pregledu splo\u0161nega podro\u010dja strojnega u\u010denja in klasifikacije pa zdaj sledi preizkus prvega klasifikatorja. Naredili bomo teoreti\u010dni pregled klasifikatorja k najbli\u017ejih sosedov in prikazali prakti\u010dni primer njegove uporabe. Friedman, J., Hastie, T. and Tibshirani, R., 2001. The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics. \u21a9 \u21a9 Kotsiantis, S.B., Zaharakis, I. and Pintelas, P., 2007. Supervised machine learning: A review of classification techniques. Emerging artificial intelligence applications in computer engineering, 160(1), pp.3-24. \u21a9","title":"Klasifikacija"},{"location":"pages/knjiga/04_klasifikacija/#klasifikacija","text":"Osrednja metoda strojnega u\u010denja te knjige je metoda klasifikacije , kjer se ra\u010dunalnik nau\u010di klasificirati instance v vnaprej dolo\u010dene razrede. Z regresijo napovemo \u0161tevilske vrednosti in so odlo\u010ditve zvezne, pri klasifikaciji pa napovedujemo nominalne vrednosti - diskretne razrede. Klasifikacija se uporablja v primerih, kot so razpoznava vzorcev na slikah, prepre\u010devanje prevar, zaznavanje neza\u017eelene po\u0161te in diagnosticiranje bolezni. \u010ce ra\u010dunalnik podatke deli v dva razreda, govorimo o binarni klasifikaciji , \u010de pa ra\u010dunalnik klasificira v ve\u010d razredov, pa imamo opravka z ve\u010drazredno klasifikacijo . 1 Obstaja na tiso\u010de razli\u010dnih algoritmov klasifikacije, ki jih v grobem delimo glede na to, v kak\u0161ni obliki shranijo model znanja 1 2 : matemati\u010dne formule in porazdelitve (logisti\u010dna regresija, naivni Bayesov klasifikator, metoda podpornih vektorjev), odlo\u010ditvena drevesa (CART, C4.5, ID3, evolucijska drevesa), odlo\u010ditvena pravila (RIPPER, PART, evolucijska pravila), umetne nevronske mre\u017ee (konvolucijske, rekurzivne) in klasifikatorji na podlagi podobnosti ( k najbli\u017ejih sosedov ). Nekatere metode zgradijo model znanja, ki se uporablja pri klasifikaciji novih instanc, ne da bi bil potreben vpogled v prej podane podatke. Takim metodam pravimo metode takoj\u0161njega u\u010denja (angl. eager learning ), saj zgradijo model znanja takoj in ga kasneje ne prilagajajo. Nasprotno, pa nekatere metode, kot na primer k najbli\u017ejih sosedov, ne ustvarijo u\u010dnega modela, ampak za vsako klasifikacijo nove instance ponovno naredijo pregled \u017ee prej podanih podatkov. Te metode uporabljajo leno u\u010denje (angl. lazy learning ), saj se u\u010dijo sproti po potrebi. Prav ta pristop bomo kasneje pregledali pri preizkusu na\u0161ega klasifikatorja k najbli\u017ejih sosedov.","title":"Klasifikacija"},{"location":"pages/knjiga/04_klasifikacija/#matematicna-definicija-pojmov","text":"V nadaljevanju bomo za metodo klasifikacije uporabljali slede\u010do definicijo instanc. Ena instanca je par \\((x_i,y_i)\\) , kjer je \\(x_i\\) vektor vrednosti (atributov) te instance, \\(y_i\\) pa je dejanski razred instance (skalarna vrednost). U\u010dna mno\u017eica \\(X\\) je definirana kot mno\u017eica vseh instanc, na katerih se algoritem u\u010di, sestavljena je iz \\(n\\) instanc in je definirana, kot prikazuje spodnja ena\u010dba. \\[\\begin{align*} \\begin{split} X &= \\left\\lbrace \\left( x_1,y_1 \\right),\\left( x_2,y_2 \\right), \\dots ,\\left( x_n,y_n \\right) \\right\\rbrace \\\\ x_i &= (x_i^1,x_i^2, \\dots, x_i^l) \\\\ y_i &\\in \\left\\lbrace razred_1, razred_2, \\dots , razred_k \\right\\rbrace \\\\ n &= \\text{\u0161tevilo instanc} \\\\ l &= \\text{\u0161tevilo atributov} \\\\ k &= \\text{\u0161tevilo razredov} \\end{split} \\end{align*}\\] Vrednost \\(x_i\\) je vektor atributov \\((x_i^1,x_i^2, \\dots, x_i^l)\\) velikosti \\(l\\) , ki je definiran v prostoru atributov \\(F\\) , razred instance \\(y_i\\) pa je vrednost iz nabora vseh mo\u017enih razredov v velikosti \\(k\\) . Cilj algoritmov klasifikacije je, da ob dani u\u010dni mno\u017eici \\(X\\) najdejo slede\u010do funkcijo: \\[\\begin{align*} h: x_i \\rightarrow y_i \\end{align*}\\] za vsak \\(i \\in \\left[ 1,n\\right]\\) , tako da bo model znanja klasifikacije \\(h(x_i)\\) dovolj dober napovedovalec razreda \\(y_i\\) . Proces klasifikacije prikazuje slede\u010da slika, kjer iz podatkovne mno\u017eice \\(X\\) algoritem klasifikacije ustvari model znanja klasifikacije \\(h\\) . Ta model preslika vhode instance \\(x^1, x^2, \\dots, x^l\\) v razred \\(y\\) .. Splo\u0161ni proces klasifikacije. Ob pregledu splo\u0161nega podro\u010dja strojnega u\u010denja in klasifikacije pa zdaj sledi preizkus prvega klasifikatorja. Naredili bomo teoreti\u010dni pregled klasifikatorja k najbli\u017ejih sosedov in prikazali prakti\u010dni primer njegove uporabe. Friedman, J., Hastie, T. and Tibshirani, R., 2001. The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics. \u21a9 \u21a9 Kotsiantis, S.B., Zaharakis, I. and Pintelas, P., 2007. Supervised machine learning: A review of classification techniques. Emerging artificial intelligence applications in computer engineering, 160(1), pp.3-24. \u21a9","title":"Matemati\u010dna definicija pojmov"},{"location":"pages/knjiga/05_knn/","text":"K najbli\u017ejih sosedov Za\u010deli bomo s pregledom delovanja klasifikacije na podlagi podobnosti med instancami. Prvo vpra\u0161anje, ki se nam poraja, je, kako sploh dolo\u010dimo podobnost med instancami? Oziroma povedano druga\u010de, kdaj sta si dve instanci podobni in kdaj ne? Intuitivno si ljudje naredimo prvi vtis glede na podobnost z \u017ee poznanimi koncepti. Vidimo nov avto? Ta je podoben na\u0161emu avtu doma - torej je najverjetneje hiter, porabi veliko goriva in ni primeren za vo\u017enjo po slabi cesti. Izbira novih \u010devljev s primerjavo. Kaj pa, \u010de izbiramo nove zimske \u010devlje izmed nabora \u010devljev, ki jih trgovina ponuja? Vse \u010devlje v ponudbi primerjamo na podlagi izku\u0161enj z zimskimi \u010devlji, ki smo si jih lastili v preteklosti. \u017delimo si nove \u010devlje, ki so \u010dim bolj podobni prej\u0161njim, na \u017ealost obrabljenim. V\u0161e\u010d nam je bila njihova barva, prav tako nas niso \u017eulili pri dalj\u0161i hoji, na ledu nam nikoli ni drselo pa tudi za na fakulteto so bili primerni. Izmed vseh ponujenih \u010devljev najdemo najbli\u017eje na\u0161im prej\u0161njim - to ljudje naredimo intuitivno, hitro in brez nepotrebnih kalkulacij. Kako pa pripravimo ra\u010dunalnik, da bo videl podobnosti med predstavljenimi koncepti? Z izra\u010dunom razdalje med dvema konceptoma. Bolj sta si dve stvari podobni, manj\u0161a je razdalja med njima, ter obratno - manj sta si dve stvari podobni, ve\u010dja je razdalja med njima. Ra\u010dunanje razdalj Poglejmo si primer primerjave dveh \u010devljev. Primerjava dveh \u010devljev. Ta primer preslikajmo v tabelo, kjer je prva vrstica namenjena atributom prvih \u010devljev, druga vrstica je namenjena atributom drugih \u010devljev, v tretji vrstici pa so razlike med njima. Vi\u0161ina Te\u017ea Vodoodporni Barva \u010cevlji A 17 231 da rjavi \u010cevlji B 9 119 ne modri Razlika 8 112 ? ? Primer ra\u010dunanja razdalje med dvema \u010devljema. Razlike med \u0161tevilskimi atributi je enostavno izra\u010dunati; preprosto vzamemo vrednost \u0161tevilskih atributov prve instance in od\u0161tejemo vrednost atributov druge instance. V primeru kategori\u010dnih atributov pa ni mo\u017eno dolo\u010diti, kateri kategoriji sta si bli\u017eji in kateri sta si dlje. Ko moramo izra\u010dunati razdaljo med dvema kategorijama, preprosto uporabimo naslednje pravilo: \u010ce sta kategoriji enaki, je razdalja med njima 0. \u010ce sta kategoriji razli\u010dni, je razdalja med njima 1. Knji\u017enica scikit-learn pa ne razlikuje med tipi spremenljivk - vse spremenljivke obravnava kot \u0161tevilske oziroma razmernostne. Za na\u0161i dve kategori\u010dni spremenljivki vodoodpornosti in barve \u010devljev to predstavlja te\u017eavo, saj v trenutni obliki nista zapisani v obliki \u0161tevil. Indikacijski atributi Za uporabo podatkov s knji\u017enico scikit-learn je podatke potrebno prilagoditi tako, da kategori\u010dne atribute spremenimo v \u0161tevilske. Napa\u010den pristop bi bil dolo\u010ditev \u0161tevila vsaki kategoriji. Pri barvi \u010devljev bi tako barva \u010drna postala \u0161tevilo 0, barva modra \u0161tevilo 1, barva rjava \u0161tevilo 2, in tako naprej. Ta pristop \u0161e vedno ni pravilen, saj algoritmi strojnega u\u010denja najve\u010dkrat operirajo s takimi vrednostmi - naklju\u010dna dolo\u010ditev \u0161tevilk kategorijam pa bi izra\u010dune pokvarila. Pri ra\u010dunanju razdalj bi tako razdalja med \u010drnimi in rjavimi \u010devlji bila 2, med \u010drnimi in modrimi pa le 1. To je nesmiselno, saj barv ne moremo postaviti v vrstni red. Posledi\u010dno se transformacije kategori\u010dnih atributov v \u0161tevilske atribute lotimo s pomo\u010djo kreacije indikacijskih atributov (angl. dummy attribute ). Iz enega kategori\u010dnega atributa tako nastane ve\u010d novih \u0161tevilskih atributov. Za vsako kategorijo enega kategori\u010dnega atributa tako nastane svoj \u0161tevilski atribut. \u010ce so \u010devlji lahko treh razli\u010dnih barv (\u010drni, modri, rjavi), potem nastanejo trije novi atributi: Barva (\u010drni) , Barva (modri) in Barva (rjavi) . Indikacijski atribut ima lahko le dve vrednosti: vrednost 0, \u010de kategorija ne dr\u017ei za instanco ter vrednost 1, \u010de kategorija dr\u017ei za instanco. Poglejmo si tabelo podatkov obeh \u010devljev po transformaciji kategori\u010dnih atributov v indikacijske atribute. Zdaj je primerjava mnogo bolj smiselna. Prav tako je opazno, da obstaja razlika tako v vodoodpornosti \u010devljev, kakor v barvi. Vi\u0161ina Te\u017ea Vod. Vod. Barva Barva Barva (da) (ne) (modri) (rjavi) (\u010drni) \u010cevlji A 17 231 1 0 0 0 1 \u010cevlji B 9 119 0 1 0 1 0 Razlika 8 112 1 -1 0 -1 1 Primer ra\u010dunanja razdalje z indikacijskimi atributi. Kreacija indikacijskih atributov v Pythonu Najenostavnej\u0161i postopek kreacije indikacijskih atributov je s pomo\u010djo pandas knji\u017enice. Struktura podatkov DataFrame ima \u017ee zabele\u017een tip vsakega stolpca, kar poenostavi avtomatsko transformacijo kategori\u010dnih atributov v indikacijske. Slede\u010da koda prikazuje kreacijo podatkov, iz katerih se bodo kreirali indikacijski atributi. import pandas as pd cevljiA = [ 17 , 231 , 'da' , 'rjavi' ] cevljiB = [ 9 , 119 , 'ne' , 'modri' ] cevljiC = [ 12 , 143 , 'ne' , '\u010drni' ] cevljiD = [ 8 , 112 , 'ne' , 'rjavi' ] cevljiE = [ 11 , 198 , 'da' , 'modri' ] cevljiF = [ 15 , 245 , 'da' , '\u010drni' ] # Z zdru\u017eitvijo instanc ustvarimo DataFrame podatki = pd . DataFrame ([ cevljiA , cevljiB , cevljiC , cevljiD , cevljiE , cevljiF ], columns = [ 'Vi\u0161ina' , 'Te\u017ea' , 'Vodood' , 'Barva' ], index = [ 'cevlji A' , 'cevlji B' , 'cevlji C' , 'cevlji D' , 'cevlji E' , 'cevlji F' ]) print ( podatki ) Vi\u0161ina Te\u017ea Vodood Barva cevlji A 17 231 da rjavi cevlji B 9 119 ne modri cevlji C 12 143 ne \u010drni cevlji D 8 112 ne rjavi cevlji E 11 198 da modri cevlji F 15 245 da \u010drni S pregledom vrednosti dtypes podatkov lahko ugotovimo kak\u0161nega tipa je posamezen atribut (stolpec). podatki . dtypes Vi\u0161ina int64 Te\u017ea int64 Vodood object Barva object dtype: object Atributa Vodood in Barva sta tipa object , kar pomeni, da sta obravnavana kot kategori\u010dni spremenljivki. Pred uporabo teh podatkov v procesu strojnega u\u010denja s knji\u017enico scikit-learn je potrebno spremeniti vse atribute object v \u0161tevilske. Knji\u017enica pandas ponuja metodo get_dummies() , ki pregleda podane podatke tipa DataFrame in vse atribute tipa object spremeni v indikacijske atribute. Izvorna spremenljivka se ne spremeni, temve\u010d se podatki z indikacijskimi atributi vrnejo kot rezultat metode. Pregled tipa atributov poka\u017ee, da so zdaj vsi atributi \u0161tevilskega tipa, s \u010dimer zadostimo uporabi v kombinaciji s scikit-learn knji\u017enico. # Vse kategori\u010dne (object) atribute spremenimo v indikacijske podatki_z_indikacijskimi = pd . get_dummies ( podatki ) podatki_z_indikacijskimi . dtypes Vi\u0161ina int64 Te\u017ea int64 Vodood_da uint8 Vodood_ne uint8 Barva_modri uint8 Barva_rjavi uint8 Barva_\u010drni uint8 dtype: object Pregled vsebine novih podatkov prika\u017ee novonastale indikacijske atribute. print ( podatki_z_indikacijskimi ) Vi\u0161ina Te\u017ea Vodood_da Vodovod_ne Barva_modri Barva_rjavi Barva_\u010drni cevlji A 17 231 1 0 0 1 0 cevlji B 9 119 0 1 1 0 0 cevlji C 12 143 0 1 0 0 1 cevlji D 8 112 0 1 0 1 0 cevlji E 11 198 1 0 1 0 0 cevlji F 15 245 1 0 0 0 1 Skupna razdalja Pri vmesnih izra\u010dunih razdalj \u0161e vedno ne pridemo do kon\u010dne skupne razdalje med dvema instancama. Za agregacijo vmesnih razdalj v eno mero razdalje pa lahko izberemo ve\u010d razli\u010dnih pristopov. Slede\u010da slika prikazuje ve\u010d vrst razdalj, ki bodo opisane v slede\u010dih sekcijah. Razli\u010dne razdalje med dvema to\u010dkama. Evklidska razdalja Evklidsko razdaljo med dvema to\u010dkama v ravnini je definiral matematik Evklid. Deluje po principu Pitagorovega izreka, kjer se razdalja med dvema to\u010dkama oz. instancama ( \\(x_1\\) in \\(x_2\\) ) izra\u010duna kot koren vsote kvadratov vseh dimenzij. \\[\\begin{align*} \\begin{split} d_E\\left(x_1,x_2\\right)&=\\sqrt{\\left( x_1^{(1)}-x_2^{(1)}\\right)^2 + \\left( x_1^{(2)}-x_2^{(2)}\\right)^2 + \\dots \\left( x_1^{(l)}-x_2^{(l)}\\right)^2} \\\\ &=\\sqrt{\\sum \\nolimits _{i=1}^{l} \\left( x_1^{(i)}-x_2^{(i)}\\right)^2 } \\end{split} \\end{align*}\\] kjer je \\(l\\) \u0161tevilo atributov posamezne instance. Mahattanska razdalja Zelo pogosto pa nas zanima manhattanska razdalja, ki si zgled za ra\u010dunanje razdalje med dvema to\u010dkama vzame po postavitvi cest na otoku Manhattan, kot ka\u017ee slika spodaj. V ve\u010djem delu otoka so ceste postavljene vzdol\u017e otoka (avenije) in pre\u010dno po otoku (ulice). Pri ra\u010dunanju razdalje od ene to\u010dke do druge je tako potrebno v obzir vzeti dol\u017eine vseh cest med dvema to\u010dkama, pri tem pa ni mo\u017eno kraj\u0161ati poti z diagonalami. Razdalje na Manhattnu. Pri izra\u010dunu manhattanske razdalje tako ni najkraj\u0161a pot predstavljena kot diagonalna in najkraj\u0161a daljica med dvema instancama, ampak kot vsota vseh daljic, ki poteka vzdol\u017e vseh osi. \\[\\begin{align*} \\begin{split} d_M\\left(x_1,x_2\\right)&=\\left| x_1^{(1)}-x_2^{(1)}\\right| + \\left| x_1^{(2)}-x_2^{(2)}\\right| + \\dots \\left| x_1^{(l)}-x_2^{(l)}\\right| \\\\ &=\\sum \\nolimits _{i=1}^{l} \\left| x_1^{(i)}-x_2^{(i)}\\right| \\end{split} \\end{align*}\\] Kosinusna razdalja Z manhattansko in evklidsko razdaljo pa pridemo do te\u017eav, ko imamo meritve, ki lahko imajo tako negativne kot pozitivne vrednosti. Primer take meritve bi bil letni zaslu\u017eek podjetja, saj je ta lahko tudi negativen (podjetje je na letni ravni imelo izgubo). Za primer vzemimo tri podjetja in njihove letne zaslu\u017eke ter spremembe dele\u017ea pokritega trga od prej\u0161njega leta, kot je na sliki. Primerjava evklidske, manhattanske in kosinusne razdalje. Tako evklidska kakor tudi manhattanska razdalja pravita, da sta podjetji A in C bli\u017eje kot pa podjetji A in B. \\[\\begin{align*} \\begin{split} d_E\\left(A,B\\right)&>d_E\\left(A,C\\right)\\\\ d_M\\left(A,B\\right)&>d_M\\left(A,C\\right)\\\\ d_C\\left(A,B\\right)&<d_M\\left(A,C\\right) \\end{split} \\end{align*}\\] To je v nasprotju z na\u0161o intuicijo: podjetje C je namre\u010d imelo izgubo v letnem prihodku in negativno spremembo pokritosti trga. Podjetji A in B pa sta imeli tako dobi\u010dek, kakor tudi se je njun dele\u017e pokritega trga pove\u010dal v primerjavi z lanskim letom. V takih situacijah sta evklidska in manhattanska razdalja neprimerni ter se uporabi kosinusna razdalja. Pri kosinusni razdalji je pomembna (1) smer vektorja posamezne instance ter (2) njegova dol\u017eina. Razliko med smerjo dveh vektorjev merimo s kotom \\(\\alpha\\) med vektorjema (instancama), ta pa je proporcionalna kosinusu kotov. Za instanci \\(x_1\\) in \\(x_2\\) velja slede\u010d izra\u010dun kosinusne razdalje. \\[\\begin{align*} \\begin{split} d_C\\left(x_1,x_2\\right)&=1- \\frac { x_1 \\cdot x_2}{||x_1|| \\cdot ||x_2||} \\\\ &=1-\\frac { \\sum _{i=1}^{l} x_1^{(i)} * x_2^{(i)}}{\\sqrt{\\sum _{i=1}^{l} x_1^{i~2}} * \\sqrt{\\sum _{i=1}^{l} x_2^{i~2}}} \\end{split} \\end{align*}\\] Izbira na\u010dina izra\u010duna razdalje je odvisna od problema. Najbolj intuitivna je vsekakor evklidska razdalja. Ve\u010dji je nabor atributov \\(l\\) , bolj je primerna manhattanska razdalja v primerjavi z evklidsko 1 . Kosinusna razdalja pa je primerna, ko nas bolj kot razdalje zanimajo podobnosti med instancami. Klasifikator k najbli\u017ejih sosedov Pri nekaterih algoritmih klasifikacije ne nastane u\u010dni model, ampak se klasifikator odlo\u010da vsakokrat ob pogledu v \u017ee klasificirane instance. Taka vrsta u\u010denja se imenuje leno u\u010denje in algoritem klasifikacije k najbli\u017ejih sosedov (angl. k nearest neighbors ) je tipi\u010dni primer lenega algoritma u\u010denja. Sledi pregled delovanja tega algoritma k najbli\u017ejih sosedov. Delovanje k najbli\u017ejih sosedov Algoritem k najbli\u017ejih sosedov ste\u010de po slede\u010dem postopku 2 , 3 . Za podano instanco \\(x_N\\) , ki jo \u017eelimo klasificirati, algoritem izra\u010duna razdalje do vseh \u017ee klasificiranih instanc. \u017de klasificirane instance razvrsti nara\u0161\u010dajo\u010de glede na razdaljo do instance \\(x_N\\) . V nadaljnji obravnavi upo\u0161teva le \\(k\\) prvih instanc v nara\u0161\u010dajo\u010dem seznamu - \\(k\\) najbli\u017ejih sosedov. Izra\u010duna pogostosti razredov iz nabora \\(k\\) najbli\u017ejih instanc kot je na sliki spodaj. Najpogostej\u0161i razred (modus) vrne kot rezultat klasifikacije instance \\(x_N\\) . \u010ce se ve\u010d razredov pojavlja z najve\u010djo pogostostjo, se izbere naklju\u010den razred izmed vseh najpogostej\u0161ih. Klasifikacija instance (moder trikotnik) s pomo\u010djo k najbli\u017ejih sosedov ob razli\u010dnih nastavitvah parametra k. Pri ra\u010dunanju razdalj je uporabljena evklidska razdalja. Sist. krvni tlak Tel. temp. Bolan Evklidska Manhattanska (mmHg) (\u00b0C) d rang d rang 82 38,0 Da 28,0 17 28,7 17 85 36,4 Da 25,1 16 27,3 16 89 36,7 Da 21,1 14 23,0 14 94 38,2 Da 16,0 11 16,5 11 95 38,2 Da 15,0 9 15,5 9 95 37,6 Ne 15,0 10 16,1 10 100 36,6 Ne 10,2 7 12,1 7 104 35,5 Ne 6,8 5 9,2 6 108 35,7 Ne 3,6 3 5,0 3 108 38,4 Da 2,0 2 2,3 2 109 39,4 Da 1,2 1 1,7 1 113 36,4 Ne 3,8 4 5,3 4 119 38,7 Da 9,0 6 9,0 5 124 37,5 Ne 14,1 8 15,2 8 128 37,8 Ne 18,0 12 18,9 12 129 38,8 Da 19,0 13 19,1 13 135 39,4 Da 25,0 15 25,7 15 140 36,6 Ne 30,1 18 32,1 18 145 36,0 Da 35,1 19 37,7 19 147 36,5 Da 37,1 20 39,2 20 U\u010dne instance. Sist. krvni tlak Tel. temp. Bolan (mmHg) (\u00b0C) 110 38,7 ? Nova instanca. Primer klasifikacije pacientov Sledi primer izra\u010dunov algoritma k najbli\u017ejih sosedov po korakih na primeru diagnoze bolezni pacientov. Imamo u\u010dno mno\u017eico, kjer merimo sistoli\u010dni krvni tlak pacientov in njihovo telesno temperaturo. Med kartotekami imamo \u017ee zabele\u017eene podatke 20 prej\u0161njih pacientov in njihove diagnoze, ki so jih postavili zdravniki. Instance in nov pacient so prikazani v zgornji tabeli. V tabeli je prav tako podan izra\u010dun razdalj, evklidske in manhattanske. Izra\u010dun obeh razdalj med prvo instanco mno\u017eice in novo instanco pacienta poteka po slede\u010dem postopku. \\[\\begin{align*} \\begin{split} d_E\\left(x_N,x_1\\right)&=\\sqrt{\\left( 110-82\\right)^2 + \\left( 38,7 - 38,0\\right)^2}\\\\ &=\\sqrt{\\left(28\\right)^2 + \\left(0,7\\right)^2}\\\\ &=\\sqrt{784 + 0,49} =\\sqrt{784,49} = 28,0 \\\\ \\\\ d_M\\left(x_N,x_1\\right)&=\\left| 110-82\\right| + \\left| 38,7 - 38,0\\right|\\\\ &=\\left| 28\\right| + \\left| 0,7\\right|\\\\ &=28 + 0,7 =28,7 \\end{split} \\end{align*}\\] Razvidno je, da razlika v krvnem tlaku prevladuje pri izra\u010dunu razdalje. Do tega pride zaradi razli\u010dnih enot, posledica pa je, da razdalje atributov z ve\u010djimi \u0161tevilskimi vrednostmi prevladujejo nad atributi manj\u0161ih \u0161tevilskih vrednosti. \u010ce \u017eelimo prispevek atributov pri ra\u010dunanju skupne razdalje poenotiti, se je potrebno lotiti standardizacije podatkov, s \u010dimer postavimo vse meritve na enako zalogo vrednosti. Standardizacija podatkov Standardizacija (angl. standardization ) je postopek transformacije podatkov na tak na\u010din, da bodo ti po transformaciji imeli povpre\u010dje enako 0 in standardni odklon enak 1. Za standardizacijo podatkov \\(x\\) rabimo njihovo povpre\u010dje \\(\\mu\\) in standardni odklon \\(\\rho\\) , ki je definiran slede\u010de za vseh \\(n\\) instanc. \\[\\begin{align*} \\begin{split} {\\rho} &= \\sqrt{\\frac{(x_1-\\mu)^2+(x_1-\\mu)^2+\\dots+(x_n-\\mu)^2}{n}} \\end{split} \\end{align*}\\] Standardizirane vrednosti \\(z\\) izra\u010dunamo iz izvornih podatkov \\(x\\) slede\u010de. \\[\\begin{align*} \\begin{split} z &= \\frac{x-\\mu}{\\rho} \\end{split} \\end{align*}\\] Vsak atribut \\(x^1\\) , \\(x^2\\) , \\(\\dots\\) , \\(x^l\\) standardiziramo lo\u010deno - transformirano z njegovim povpre\u010djem in standardnim odklonom. Po standardizaciji vseh atributov (tudi indikacijskih) imajo vsi atributi povpre\u010dje v vrednosti 0 in standardni odklon 1. Razdalje posameznih atributov med instancami bodo tako v podobnem intervalu. Za standardizacijo podatkov pacientov tako potrebujemo podatke povpre\u010dja in standardnega odklona instanc, kar prikazuje spodnja ena\u010dba. \\[\\begin{align*} \\begin{split} \\mu\\left(\\text{Sistoli\u010dni krvni pritisk}\\right)&= 112,45\\\\ \\rho\\left(\\text{Sistoli\u010dni krvni pritisk}\\right)&= 19,58\\\\ \\mu\\left(\\text{Telesna temperatura}\\right)&= 37,42\\\\ \\rho\\left(\\text{Telesna temperatura}\\right)&= 1,17\\\\ \\end{split} \\end{align*}\\] Tabela prikazuje vrednosti atributov po standardizaciji. Sist. krvni tlak Tel. temp. Bolan Evklidska Manhattanska (mmHg) (\u00b0C) d rang d rang -1,55 0,49 Da 1,55 11 2,03 11 -1,40 -0,87 Da 2,34 15 3,24 17 -1,20 -0,61 Da 2,01 14 2,78 15 -0,94 0,66 Da 0,92 5 1,24 6 -0,89 0,66 Da 0,88 4 1,19 5 -0,89 0,15 Ne 1,21 8 1,70 8 -0,64 -0,70 Ne 1,86 12 2,30 13 -0,43 -1,64 Ne 2,74 19 3,03 16 -0,23 -1,47 Ne 2,56 17 2,66 14 -0,23 0,84 Da 0,28 1 0,36 1 -0,18 1,69 Da 0,60 3 0,65 3 0,03 -0,87 Ne 1,97 13 2,11 12 0,33 1,09 Da 0,46 2 0,46 2 0,59 0,07 Ne 1,25 9 1,74 9 0,79 0,32 Ne 1,20 7 1,69 7 0,85 1,18 Da 0,97 6 1,06 4 1,15 1,69 Da 1,41 10 1,87 10 1,41 -0,70 Ne 2,36 16 3,32 18 1,66 -1,21 Da 2,91 20 4,09 20 1,76 -0,78 Da 2,66 18 3,76 19 Standardizirane u\u010dne instance. Sist. krvni tlak Tel. temp. Bolan (mmHg) (\u00b0C) -0,13 1,09 ? Standardizirana nova instanca. Podatki po standardizaciji ka\u017eejo druga\u010dno sliko. Vrstni red najbli\u017ejih instanc podani instanci se spremeni. Instanca, ki je v tabeli ozna\u010dena, je pred standardizacijo bila \u010detrta najbli\u017eja podani instanci. Pred standardizacijo je namre\u010d bila razlika v temperaturi majhna v primerjavi z razliko v krvnem tlaku. Po standardizaciji pa je razlika v temperaturi mnogo ve\u010dja, kot razlika pri krvnem tlaku. To je prav, saj sta bili pred standardizacijo zalogi vrednosti obeh meritev druga\u010dni. Realni obseg telesne temperature \u010dloveka je pribli\u017eno med 35 in 41 \u00b0C, kar predstavlja maksimalno razliko 6 enot. \u0160est enot razlike pri krvnem tlaku pa ni niti pribli\u017eno realnemu obsegu sistoli\u010dnega krvnega tlaka, ki se lahko giblje v intervalu med 80 in 180 mmHg z maksimalno razliko kar 100 enot. Po standardizaciji se krvni tlak giblje med \\(-1,55\\) ter \\(1,76\\) z maksimalno razliko \\(3,31\\) . To je v skladu z obsegom telesne temperature, ki se po standardizaciji giblje med \\(-1,64\\) ter \\(1,69\\) z maksimalno razliko \\(3,33\\) . Standardizacija je le en postopek tehnike, ki jo imenujemo normalizacija podatkov . Alternative standardizaciji so min-max skaliranje , centriranje , rangiranje in drugi. Vsak pristop strojnega u\u010denja ni ob\u010dutljiv na intervale oz. skalo atributov. Med take \u0161tejemo algoritme kreacije odlo\u010ditvenih dreves in naivnega Bayesa. Dobra praksa je, da pri postopku podatkovnega rudarjenja podatke pred obdelavo vedno normaliziramo, saj se s tem izognemo morebitnemu zavajanju algoritmov. Negativna plat normalizacije podatkov pa je, da ti niso najve\u010dkrat enostavno interpretabilni. Kakor ka\u017eejo standardizirane vrednosti v zgornji tabeli, so vrednosti telesne temperature nesmiselne. To postane \u0161e posebej problemati\u010dno v primeru, ko gradimo model znanja, ki temelji na pravilih (odlo\u010ditvena pravila ali odlo\u010ditvena drevesa), saj bodo vrednosti v pravilih standardizirane. Standardizacija v Pythonu Sledi primer, kjer za za\u010detek najprej pregledamo povpre\u010dne vrednosti in standardne odklone atributov pred standardizacijo. from sklearn.datasets import load_iris # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) print ( 'Povpre\u010dja pred standardizacijo:' ) print ( podatki . data . mean ( axis = 0 )) print ( 'Standardni odkloni pred standardizacijo:' ) print ( podatki . data . std ( axis = 0 )) Povpre\u010dja pred standardizacijo: sepal length (cm) 5.843333 sepal width (cm) 3.057333 petal length (cm) 3.758000 petal width (cm) 1.199333 dtype: float64 Standardni odkloni pred standardizacijo: sepal length (cm) 0.828066 sepal width (cm) 0.435866 petal length (cm) 1.765298 petal width (cm) 0.762238 dtype: float64 Sedaj pa te podatke standardiziramo in izpi\u0161emo povpre\u010dja ter standardne odklone novih vrednosti. Knji\u017enica scikit-learn ponuja kar nekaj na\u010dinov transformacije podatkov. Za standardizacijo se uporablja razred StandardScaler . S klicem metode fit se izra\u010dunata povpre\u010dje in standardni odklon iz podanih podatkov. Za transformacijo podatkov, pa se uporabi klic metode transform , kateri podamo podatke, ki jih \u017eelimo transformirati. from sklearn.preprocessing import StandardScaler # Inicializacija standardizatorja std = StandardScaler () # Izra\u010dunamo povpre\u010dja in standardne odklone atributov std . fit ( podatki . data ) # Standardizacija podatkov stand_podatki = std . transform ( podatki . data ) print ( 'Povpre\u010dja po standardizaciji:' ) print ( stand_podatki . mean ( axis = 0 )) print ( 'Standardni odkloni po standardizaciji:' ) print ( stand_podatki . std ( axis = 0 )) ovpre\u010dja po standardizaciji: -4.73695157e-16 -7.81597009e-16 -4.26325641e-16 -4.73695157e-16 Standardni odkloni po standardizaciji: 1. 1. 1. 1. Pregled rezultatov povpre\u010dij ka\u017ee, da so te zelo blizu vrednosti 0 ( \\(10^{-15}\\) ), standardni odkloni pa so enaki 1. \u010ce \u017eelimo proces izra\u010duna povpre\u010dij in standardnih odklonov ter proces transformacije podatkov zdru\u017eiti, lahko uporabimo metodo fit_transform , ki na podanih podatkih izra\u010duna vmesne vrednosti in jih vrne transformirane. stand_podatki = std . fit_transform ( podatki . data ) Dolo\u010ditev razreda podane instance Po izra\u010dunu razdalj in dolo\u010ditvi rangov glede na bli\u017eino do nove instance sledi dolo\u010ditev razreda (klasifikacija) nove instance. Pri tem procesu igra pomembno vlogo dolo\u010ditev vrednosti parametra \\(k\\) , ki nam pove, koliko najbli\u017ejih instanc upo\u0161tevamo pri klasifikaciji. Vseh \\(k\\) najbli\u017ejih instanc bo namre\u010d glasovalo za razred nove instance - vsaka instanca bo dala glas za razred, kateremu le-ta pripada. \u010ce je parameter \\(k=1\\) , potem vzamemo le najbli\u017ejo instanco in bo novi instanci dodeljen razred te instance. \u010ce se nave\u017eemo na primer iz tabele zgoraj, je pri izra\u010dunu obeh razdalj najbli\u017eja ista instanca - ta je razreda Da kar pomeni, da tudi novo instanco klasificiramo v razred Da . Tabeli spodaj prikazujeta rezultate klasifikacije pri razli\u010dnih nastavitvah - pri standardiziranih in nestandardiziranih podatkih, pri razli\u010dnih vrednostih \\(k\\) in pri evklidski ter manhattanski razdalji. Evklidska Manhattanska k Da Ne Rezultat Da Ne Rezultat 2 2 0 Da 2 0 Da 3 2 1 Da 2 1 Da 4 2 2 Da/Ne 2 2 Da/Ne 5 2 3 Ne 3 2 Da Klasifikacija s k najbli\u017ejih sosedov pri razli\u010dnih nastavitvah na nestandardiziranih podatkih. Evklidska Manhattanska k Da Ne Rezultat Da Ne Rezultat 2 2 0 Da 2 0 Da 3 3 0 Da 3 0 Da 4 4 0 Da 4 0 Da 5 5 0 Da 5 0 Da Klasifikacija s k najbli\u017ejih sosedov pri razli\u010dnih nastavitvah na standardiziranih podatkih. Zgornja tabela ka\u017ee zanimive rezultate. Najprej poglejmo nestandardizirane podatke. Tako pri evklidski, kot tudi pri manhattanski razdalji, se z ve\u010danjem \u0161tevila najbli\u017ejih instanc, ki se upo\u0161tevajo pri klasifikaciji, ve\u010da tudi negotovost, saj iz razreda Da pri upo\u0161tevanju le ene najbli\u017eje instance ( \\(k=1\\) ) preidemo do negotovosti, ko upo\u0161tevamo \u0161tiri najbli\u017eje instance ( \\(k=4\\) ), pa vse do spremembe odlo\u010ditve v razred Ne , ko upo\u0161tevamo pet najbli\u017ejih instanc ( \\(k=5\\) ) pri evklidski razdalji. Ti rezultati nam ka\u017eejo tudi razliko med evklidsko in manhattansko razdaljo, saj se odlo\u010ditve kon\u010dnega razreda instance ne skladajo pri \\(k=5\\) . Hkrati pa se moramo soo\u010diti \u0161e z negotovostjo pri \\(k=4\\) . Ko pridemo do neodlo\u010denega izida, se algoritem odlo\u010di za en naklju\u010den razred v vodstvu (tisti, ki ima manj\u0161i indeks), kar pa ni vedno najbolj\u0161a odlo\u010ditev. \u010ce bi ro\u010dno \u017eeleli izni\u010diti mo\u017enosti za neodlo\u010dene izide in naklju\u010dne odlo\u010ditve med njimi, bi algoritem preprosto zagnali \u0161e na drugih nastavitvah vrednosti \\(k\\) in pogledali, kak\u0161en je kon\u010den razred tedaj. Ko imamo opravek z binarno klasifikacijo (delitev instanc v dva razreda), pa se neodlo\u010denih izidov lahko znebimo z liho vrednostjo \\(k\\) . Po drugi strani pa ob pregledu rezultatov po standardizaciji vidimo ve\u010djo stabilnost, saj obstaja konsenz pri vseh nastavitvah \\(k\\) in pri obeh tipih razdalje. Ta pristop se tako izka\u017ee kot bolj robusten na manj\u0161e spremembe, pa tudi konceptualno je primernej\u0161i, saj vsi atributi instanc enakovredno vplivajo na odlo\u010ditev klasifikacije. Uporaba k najbli\u017ejih sosedov v Pythonu Algoritem k najbli\u017ejih sosedov je v knji\u017enici scikit-learn implementiran z razredom KNeighborsClassifier . \u017de pri inicializaciji primerka tega razreda dolo\u010dimo \\(k\\) \u0161tevilo najbli\u017ejih sosedov s parametrom n_neighbors in na\u010din ra\u010dunanja razdalje s parametrom metric . from sklearn.neighbors import KNeighborsClassifier klasif = KNeighborsClassifier ( n_neighbors = 3 , metric = 'manhattan' ) klasif . fit ( X_u , y_u ) napovedi = klasif . predict ( X_t ) Pri ra\u010dunanju razdalje lahko uporabimo evklidsko razdaljo z euclidean , mahattansko z manhattan in kosinusno razdaljo s cosine . S klicem metode fit in podajo podatkov instanc X_u in njihovih razredov y_u te shranimo in bodo slu\u017eili za izra\u010dun najbli\u017ejih sosedov. Metodo predict pa kli\u010demo s podajo mno\u017eice instanc X_t , ki jih \u017eelimo klasificirati. To je tudi standardni postopek uporabe drugih algoritmov klasifikacije, regresije in gru\u010denja v knji\u017enici scikit-learn : Nastavitve definiramo v konstruktorju. Model znanja zgradimo s fit(podatki_instanc, resitve_instanc) . Model znanja uporabimo s predict(podatki_novih_instanc) . Algoritmi pa lahko imajo tudi sebi specifi\u010dne metode. V primeru k najbli\u017ejih sosedov je njemu posebna metoda kneighbors , kateri podamo instance, za katere i\u0161\u010demo najbli\u017eje sosede, ter parameter n_neighbors , s katerim povemo, koliko najbli\u017ejih sosedov i\u0161\u010demo iz nabora vseh instanc podanih \u017ee prej v metodi fit . Rezultat sta dva seznama: seznam razdalj od izbranih instanc do najbli\u017ejih sosedov v nara\u0161\u010dajo\u010dem vrstnem redu glede na razdaljo, ter seznam indeksov najbli\u017ejih instanc, ponovno od najbli\u017ejega naprej. klasif . fit ( X_u , y_u ) razdalje , sosedi = klasif . kneighbors ( nove_instance , n_neighbors = 3 ) Sledi pregled uporabe klasifikacijskega algoritma k najbli\u017ejih sosedov za namen iskanja petih najbli\u017ejih sosedov eni instanci. Najprej s slede\u010do kodo nalo\u017eimo instance podatkovne mno\u017eice Iris , iz nje izberemo instanco v vrstici z indeksom 133 ter jo odstranimo iz podatkovne mno\u017eice. from sklearn.datasets import load_iris # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Izberemo eno instanco izbrana = 133 X_izbrana = podatki . data . iloc [ izbrana ,:] y_izbrana = podatki . target . iloc [ izbrana ] # Izbrano instanco izlo\u010dimo iz ostalih podatkov X_ostali = podatki . data . drop ( izbrana , axis = 0 ) y_ostali = podatki . target . drop ( izbrana ) Izbira vrednosti iz polja poteka s pomo\u010djo klica iloc[vrstica, stolpec] , kamor podamo indeks vrstice in indeks stolpca. \u010ce \u017eelimo izbrati celotno vrsto, podamo namesto indeksa kar dvopi\u010dje : . Tako s podatki.data.iloc[:, 12] izberemo stolpec z indeksom 12, s klicem podatki.data.iloc[8, :] pa izberemo vrstico z indeksom 8. \u010ce \u017eelimo izbrati ve\u010d vrednosti, pa namesto \u0161tevila na mestu vrstice in stolpca podamo polje indeksov. S klicem podatki.data.iloc[[2, 5, 8], :] izberemo vrstice s temi indeksi. In obratno, s klicem podatki.data.iloc[:, [3, 6, 9]] se vrnejo stolpci z indeksi 3, 6 in 9. Pri izbiri vrednosti iz vektorja podamo le eno vrednost, saj ima ta le eno dimenzijo. Tako nam klic podatki.target.iloc[[2, 5, 8]] vrne podatke z indeksi 2, 5 in 8. Z metodo podatki.data.drop(izbrana, axis=0) odstranimo instanco z indeksom izbrana iz podatkov podatki.data . Parameter axis dolo\u010da, po kateri osi izbri\u0161emo podatke - \u010de je podana 0, izbri\u0161emo vrstico, \u010de pa 1, pa izbri\u0161emo stolpec. Pri izbrisu iz podatki.target parametra axis ni potrebno podati, saj so podatki v obliki vektorja, ki ima le eno dimenzijo. Vizualizacija podatkov S pomo\u010djo knji\u017enice seaborn lahko enostavno vizualiziramo instance. Z dvema klicema metode scatterplot se en na drugega izri\u0161eta dva grafa raztrosa. S parametroma x in y podamo podatke, ki naj so izrisani na teh oseh. import seaborn as sns x_os , y_os = 0 , 1 # Izri\u0161emo ostale instance sns . scatterplot ( x = X_ostali . iloc [:, x_os ], y = X_ostali . iloc [:, y_os ], hue = podatki . target_names [ y_ostali ], palette = 'colorblind' ) # Izri\u0161emo eno izbrano instanco sns . scatterplot ( x = [ X_izbrana . iloc [ x_os ]], y = [ X_izbrana . iloc [ y_os ]], hue = [ 'Neznan' ], style = [ 'Neznan' ], markers = { 'Neznan' : '^' }) Instance podatkovne zbirke Iris. Horizontalna x os predstavlja prvi atribut, vertikalna y os pa drugi atribut podatkovne mno\u017eice. Barve lo\u010dijo razrede instanc, s trikotnikom pa je ozna\u010dena instanca, ki jo \u017eelimo klasificirati. Pomanjkljivost grafi\u010dne predstavitve podatkov je, da lahko izri\u0161emo instance glede na omejeno \u0161tevilo njihovih atributov. V na\u0161em primeru imamo dvodimenzionalen graf, kjer smo posamezne osi dolo\u010dili s spremenljivkama x_os in y_os . Parameter hue prejme podatke, ki bodo narisane ozna\u010dbe delili glede na barvo - v na\u0161em primeru so to podatki o razredu. Parameter style pa prejme podatke, ki dolo\u010dajo stil ozna\u010dbe, ki jih definiramo s parametrom markers . Barve ozna\u010db dolo\u010damo s podajanjem teme v parameter palette . U\u010denje in uporaba modela znanja U\u010denje modela in napoved razreda instance na indeksu 133 poteka na slede\u010d na\u010din. Najprej s konstruktorjem dolo\u010dimo vrednost \\(k\\) na pet najbli\u017ejih sosedov po izra\u010dunu manhattanske razdalje. Temu sledi klic metode fit , kateri podamo podatke instanc X_ostali in njihove razrede y_ostali . Ker metoda predict pri\u010dakuje ve\u010d instanc, dodamo instanco X_izbrana najprej v seznam in ta seznam podamo v klic metode predict([X_izbrana]) . \u010ce X_izbrana ne bi bil vektor, ampak polje, bi klic metode potekal brez ovijanja v seznam predict(X_izbrana) . from sklearn.neighbors import KNeighborsClassifier # Inicializiramo klasifikator knn = KNeighborsClassifier ( n_neighbors = 5 , metric = 'manhattan' ) # Shranimo instance za primerjavo knn . fit ( X_ostali , y_ostali ) # Napovemo razred izbrane instance napoved = knn . predict ([ X_izbrana ]) print ( f 'KNN je napovedal, da je instanca razreda { podatki . target_names [ napoved ] } .' ) print ( f 'Ta instanca je dejansko razreda { podatki . target_names [ y_izbrana ] } .' ) KNN je napovedal, da je instanca razreda ['versicolor']. Ta instanca je dejansko razreda virginica. Klasifikator je napovedal napa\u010den razred za to instanco. Preglejmo katerih pet instanc je po izra\u010dunu manhattanske razdalje najbli\u017eje na\u0161i izbrani instanci iz vrstice z indeksom 133. # Izbrani instanci najdemo pet najbli\u017ejih sosedov razdalje , sosedi = knn . kneighbors ([ X_izbrana ], n_neighbors = 5 ) print ( f 'Pet najbli\u017ejih: { sosedi } ' ) print ( f 'Razdalje od najbli\u017ejih do izbrane: { razdalje } ' ) Pet najbli\u017ejih: [[ 72 83 123 126 54]] Razdalje od najbli\u017ejih do izbrane: [[0.5 0.5 0.6 0.7 0.7]] Pet najbli\u017ejih instanc po manhattanski razdalji so instance z indeksi 72, 83, 123, 126 in 54. Rezultati razdalj pa so kar manhattanske razdalje teh sosedov do podane instance. Vpliv nastavitev na rezultate Poglejmo, kako se spremeni seznam petih najbli\u017ejih instanc, ko spreminjamo na\u010din izra\u010duna razdalje med instancami. for razdalja in [ 'euclidean' , 'manhattan' , 'cosine' ]: knn = KNeighborsClassifier ( n_neighbors = 5 , metric = razdalja ) knn . fit ( X_ostali , y_ostali ) razdalje , najblizje = knn . kneighbors ([ X_izbrana ], n_neighbors = 5 ) print ( f 'Najbli\u017eje instance po { razdalja } so { najblizje } ' ) print ( f 'Razdalje so { razdalje } ' ) Najbli\u017eje instance po euclidean so [[ 83 72 123 126 127]] Razdalje so [[0.33166248 0.36055513 0.37416574 0.43588989 0.45825757]] Najbli\u017eje instance po manhattan so [[ 72 83 123 126 54]] Razdalje so [[0.5 0.5 0.6 0.7 0.7]] Najbli\u017eje instance po cosine so [[125 129 90 131 83]] Razdalje so [[0.0001158 0.00022532 0.00033682 0.00034546 0.00039085]] Izra\u010dun petih najbli\u017ejih sosedov po evklidski in manhattanski razdalji vrne podobne rezultate. Instanca z indeksom 83 je najbli\u017eja izbrani po izra\u010dunu evklidske razdalje in je druga najbli\u017eja po manhattanski razdalji - mesto si izmenja z instanco 72. Tretje in \u010detrto mesto sta v obeh razdaljah zasedli instanci 123 in 125. Peto mesto ima v primeru evklidske razdalje instanca 127, v primeru manhattanske pa instanca 54. \u010ce sta instanci 127 in 54 druga\u010dnega razreda, lahko ta razlika vpliva na razred izbrane instance. Seznam najbli\u017ejih instanc po izra\u010dunu kosinusne razdalje pa je skorajda popolnoma druga\u010den od ostalih dveh - le instanca 83 je v vseh treh seznamih. Vse ostale \u0161tiri instance so v seznamu kosinusne razdalje druge v primerjavi s seznamoma evklidske in manhattanske razdalje. Ti rezultati nam ka\u017eejo, kako pomembna je odlo\u010ditev glede na\u010dina izra\u010duna razdalje. Slede\u010da slika ka\u017ee delitev obmo\u010dja razredov glede na na\u010din ra\u010dunanja razdalje. Pri kreaciji slike sta bila upo\u0161tevana prva dva atributa in vrednost \u0161tevila sosedov k je bila nastavljena na 5. Slika ka\u017ee, da ve\u010djih razlik med evklidsko in manhattansko razdaljo pri tej podatkovni mno\u017eici in nastavitvi k ni. Delitev obmo\u010dij je relativno jasna, z nekoliko ve\u010djim prekrivanjem v sredini, kjer so si instance razredov versicolor in virginica zelo podobne. Po drugi strani je razvidno, da delitev po izra\u010dunu glede na kosinusno razdaljo ni primerna za podano podatkovno mno\u017eico, saj sta obmo\u010dji versicolor in virginica preve\u010d prepleteni in je klasifikacija instanc v tem obmo\u010dju nestabilna. Delitev na obmo\u010dja razredov glede na na\u010din ra\u010dunanja razdalj med instancami. Poglejmo si \u0161e, kako vrednost k vpliva na kon\u010dno klasifikacijo izbrane instance. for k in [ 1 , 3 , 5 , 8 , 10 ]: knn = KNeighborsClassifier ( n_neighbors = k , metric = 'euclidean' ) knn . fit ( X_ostali , y_ostali ) napoved = knn . predict ([ X_izbrana ]) print ( f 'KNN z k= { k } je napovedal, da je izbrana instanca razreda { podatki . target_names [ napoved ] } ' ) KNN z k=1 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=3 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=5 je napovedal, da je izbrana instanca razreda ['virginica'] KNN z k=8 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=10 je napovedal, da je izbrana instanca razreda ['virginica'] Pri spreminjanju \u0161tevila najbli\u017ejih sosedov ne vidimo konsenza. Razred izbrane instance z indeksom 133 se namre\u010d spreminja iz (napa\u010dnega) razreda versicolor ob glasovanju enega, treh in osmih najbli\u017ejih sosedov, v (pravilen) razred virginica ob glasovanju petih ali desetih najbli\u017ejih sosedov. Ponovno je razvidno, da nastavitev igra vlogo pri napovedih algoritma. Vrednost k se najve\u010dkrat dolo\u010di po preizku\u0161anju, vsekakor pa mora biti vrednost smiselna (\u010de je k prevelik, bo v resnici algoritem vrnil najpogostej\u0161i razred). Preve\u010d optimiziranja nastavitev za namen bolj\u0161e klasifikacije ene instance je nesmiselno. Instanca indeksa 133 je bila izbrana namenoma, saj se napovedi njenega razreda zelo spreminjajo ob druga\u010dnih nastavitvah. Ve\u010dji del instanc dobi enako napoved razreda, ne glede na tip razdalje in k . To nam pove ve\u010d o tej instanci kot pa o samem algoritmu. Mogo\u010de je ta nekoliko nenavadna, ali pa je bila \u017ee v osnovi (s strani ekspertov) klasificirana napa\u010dno. Iz tega sledi, da je smiselno gledati rezultate in kakovost klasifikacije na ve\u010d instancah, ne le na eni - kaj pa \u010de je ta izbrana nenavadno. V naslednjem poglavju bomo spoznali na\u010dine ovrednotenja kakovosti klasifikacije in proces pravilne izbire instanc, s katerimi testiramo izbrani algoritem klasifikacije in njegovih nastavitev. Spodnja slika prikazuje razli\u010dna obmo\u010dja razredov glede na razli\u010dne vrednosti \u0161tevila sosedov k . Pri kreaciji slike sta bila ponovno upo\u0161tevana le prva dva atributa podatkov in ra\u010dunanje evklidskih razdalj. Iz slike je razvidno, da majhne vrednosti k prinesejo kar nekaj majhnih podro\u010dij klasifikacije. Po drugi strani pa je razvidno, da nastavitev \u0161tevila sosedov na 10 nekoliko pokvari delitev obmo\u010dja. Delitev na obmo\u010dja razredov glede na \u0161tevilo najbli\u017ejih sosedov k: 1, 3, 5, 8 in 10. Vpliv standardizacije podatkov na rezultate Poglejmo si, \u010de se klasifikacija kaj spremeni, \u010de uporabimo standardizirane podatke. from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import StandardScaler # Standardiziramo podatke standardizator = StandardScaler () standardizator . fit ( X_ostali ) X_ostali_s = standardizator . transform ( X_ostali ) X_izbrana_s = standardizator . transform ([ X_izbrana ]) for razdalja in [ 'euclidean' , 'manhattan' , 'cosine' ]: for k in [ 1 , 3 , 5 ]: knn = KNeighborsClassifier ( n_neighbors = k , metric = razdalja ) knn . fit ( X_ostali_s , y_ostali ) nap = knn . predict ( X_izbrana_s ) print ( f 'KNN metric= { razdalja } , k= { k } je napovedal razred { podatki . target_names [ nap ] } ' ) KNN metric=euclidean, k=1 je napovedal razred ['versicolor'] KNN metric=euclidean, k=3 je napovedal razred ['versicolor'] KNN metric=euclidean, k=5 je napovedal razred ['versicolor'] KNN metric=manhattan, k=1 je napovedal razred ['versicolor'] KNN metric=manhattan, k=3 je napovedal razred ['versicolor'] KNN metric=manhattan, k=5 je napovedal razred ['versicolor'] KNN metric=cosine, k=1 je napovedal razred ['versicolor'] KNN metric=cosine, k=3 je napovedal razred ['virginica'] KNN metric=cosine, k=5 je napovedal razred ['versicolor'] Objekt razreda StandardScaler najprej nau\u010dimo, kaj so povpre\u010dja in standardni odkloni podatkov s pomo\u010djo klica fit . Kasneje to uporabilo za transformacijo (standardizacijo) tako ostalih podatkov X_ostali , kakor tudi izbrane instance X_izbrana . Pri tem uporabimo klic metode transform . Napovedi so mnogo bolj robustne na spreminjanje nastavitev klasifikacijskega algoritma, ko imamo opravek s standardiziranimi podatki. Namre\u010d, najpogosteje napovedan razred je bil versicolor . \u010ceprav je napoved razreda napa\u010dna, imamo raje robustne napovedi, kot pa take, ki so preve\u010d odvisne od nastavitev algoritma. Enovit primer klasifikacije ve\u010d instanc Sledi enovit primer klasifikacije ve\u010d instanc iz Iris podatkovne zbirke, kjer so podatki tudi standardizirani. from sklearn.neighbors import KNeighborsClassifier from sklearn.datasets import load_iris import numpy as np from sklearn.preprocessing import StandardScaler # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Izberemo ve\u010d instanc izbrane = [ 1 , 31 , 61 , 91 , 121 ] X_izbrane = podatki . data . iloc [ izbrane ,:] y_izbrane = podatki . target . iloc [ izbrane ] # Izbrano instanco izlo\u010dimo iz ostalih podatkov X_ostali = podatki . data . drop ( izbrane , axis = 0 ) y_ostali = podatki . target . drop ( izbrane ) # Standardiziramo podatke standardizator = StandardScaler () standardizator . fit ( X_ostali ) X_ostali_s = standardizator . transform ( X_ostali ) X_izbrane_s = standardizator . transform ( X_izbrane ) # Zgradimo klasifikator in napovedmo razrede knn = KNeighborsClassifier ( n_neighbors = 3 , metric = 'euclidean' ) knn . fit ( X_ostali_s , y_ostali ) napovedi = knn . predict ( X_izbrane_s ) for i , napoved , dejansko in zip ( izbrane , napovedi , y_izbrane ): print ( f 'Instanca { i } je klasificirana kot { podatki . target_names [ napoved ] } dejansko pa je { podatki . target_names [ dejansko ] } .' ) Instanca 1 je klasificirana kot setosa dejansko pa je setosa. Instanca 31 je klasificirana kot setosa dejansko pa je setosa. Instanca 61 je klasificirana kot versicolor dejansko pa je versicolor. Instanca 91 je klasificirana kot versicolor dejansko pa je versicolor. Instanca 121 je klasificirana kot virginica dejansko pa je virginica. Aggarwal, C.C., Hinneburg, A. and Keim, D.A., 2001, January. On the surprising behavior of distance metrics in high dimensional space. In International conference on database theory (pp. 420-434). Springer, Berlin, Heidelberg. \u21a9 Aha, D.W., Kibler, D. and Albert, M.K., 1991. Instance-based learning algorithms. Machine learning, 6(1), pp.37-66. \u21a9 Goldberger, J., Hinton, G.E., Roweis, S. and Salakhutdinov, R.R., 2004. Neighbourhood components analysis. Advances in neural information processing systems, 17. \u21a9","title":"K najbli\u017ejih sosedov"},{"location":"pages/knjiga/05_knn/#k-najblizjih-sosedov","text":"Za\u010deli bomo s pregledom delovanja klasifikacije na podlagi podobnosti med instancami. Prvo vpra\u0161anje, ki se nam poraja, je, kako sploh dolo\u010dimo podobnost med instancami? Oziroma povedano druga\u010de, kdaj sta si dve instanci podobni in kdaj ne? Intuitivno si ljudje naredimo prvi vtis glede na podobnost z \u017ee poznanimi koncepti. Vidimo nov avto? Ta je podoben na\u0161emu avtu doma - torej je najverjetneje hiter, porabi veliko goriva in ni primeren za vo\u017enjo po slabi cesti. Izbira novih \u010devljev s primerjavo. Kaj pa, \u010de izbiramo nove zimske \u010devlje izmed nabora \u010devljev, ki jih trgovina ponuja? Vse \u010devlje v ponudbi primerjamo na podlagi izku\u0161enj z zimskimi \u010devlji, ki smo si jih lastili v preteklosti. \u017delimo si nove \u010devlje, ki so \u010dim bolj podobni prej\u0161njim, na \u017ealost obrabljenim. V\u0161e\u010d nam je bila njihova barva, prav tako nas niso \u017eulili pri dalj\u0161i hoji, na ledu nam nikoli ni drselo pa tudi za na fakulteto so bili primerni. Izmed vseh ponujenih \u010devljev najdemo najbli\u017eje na\u0161im prej\u0161njim - to ljudje naredimo intuitivno, hitro in brez nepotrebnih kalkulacij. Kako pa pripravimo ra\u010dunalnik, da bo videl podobnosti med predstavljenimi koncepti? Z izra\u010dunom razdalje med dvema konceptoma. Bolj sta si dve stvari podobni, manj\u0161a je razdalja med njima, ter obratno - manj sta si dve stvari podobni, ve\u010dja je razdalja med njima.","title":"K najbli\u017ejih sosedov"},{"location":"pages/knjiga/05_knn/#racunanje-razdalj","text":"Poglejmo si primer primerjave dveh \u010devljev. Primerjava dveh \u010devljev. Ta primer preslikajmo v tabelo, kjer je prva vrstica namenjena atributom prvih \u010devljev, druga vrstica je namenjena atributom drugih \u010devljev, v tretji vrstici pa so razlike med njima. Vi\u0161ina Te\u017ea Vodoodporni Barva \u010cevlji A 17 231 da rjavi \u010cevlji B 9 119 ne modri Razlika 8 112 ? ? Primer ra\u010dunanja razdalje med dvema \u010devljema. Razlike med \u0161tevilskimi atributi je enostavno izra\u010dunati; preprosto vzamemo vrednost \u0161tevilskih atributov prve instance in od\u0161tejemo vrednost atributov druge instance. V primeru kategori\u010dnih atributov pa ni mo\u017eno dolo\u010diti, kateri kategoriji sta si bli\u017eji in kateri sta si dlje. Ko moramo izra\u010dunati razdaljo med dvema kategorijama, preprosto uporabimo naslednje pravilo: \u010ce sta kategoriji enaki, je razdalja med njima 0. \u010ce sta kategoriji razli\u010dni, je razdalja med njima 1. Knji\u017enica scikit-learn pa ne razlikuje med tipi spremenljivk - vse spremenljivke obravnava kot \u0161tevilske oziroma razmernostne. Za na\u0161i dve kategori\u010dni spremenljivki vodoodpornosti in barve \u010devljev to predstavlja te\u017eavo, saj v trenutni obliki nista zapisani v obliki \u0161tevil.","title":"Ra\u010dunanje razdalj"},{"location":"pages/knjiga/05_knn/#indikacijski-atributi","text":"Za uporabo podatkov s knji\u017enico scikit-learn je podatke potrebno prilagoditi tako, da kategori\u010dne atribute spremenimo v \u0161tevilske. Napa\u010den pristop bi bil dolo\u010ditev \u0161tevila vsaki kategoriji. Pri barvi \u010devljev bi tako barva \u010drna postala \u0161tevilo 0, barva modra \u0161tevilo 1, barva rjava \u0161tevilo 2, in tako naprej. Ta pristop \u0161e vedno ni pravilen, saj algoritmi strojnega u\u010denja najve\u010dkrat operirajo s takimi vrednostmi - naklju\u010dna dolo\u010ditev \u0161tevilk kategorijam pa bi izra\u010dune pokvarila. Pri ra\u010dunanju razdalj bi tako razdalja med \u010drnimi in rjavimi \u010devlji bila 2, med \u010drnimi in modrimi pa le 1. To je nesmiselno, saj barv ne moremo postaviti v vrstni red. Posledi\u010dno se transformacije kategori\u010dnih atributov v \u0161tevilske atribute lotimo s pomo\u010djo kreacije indikacijskih atributov (angl. dummy attribute ). Iz enega kategori\u010dnega atributa tako nastane ve\u010d novih \u0161tevilskih atributov. Za vsako kategorijo enega kategori\u010dnega atributa tako nastane svoj \u0161tevilski atribut. \u010ce so \u010devlji lahko treh razli\u010dnih barv (\u010drni, modri, rjavi), potem nastanejo trije novi atributi: Barva (\u010drni) , Barva (modri) in Barva (rjavi) . Indikacijski atribut ima lahko le dve vrednosti: vrednost 0, \u010de kategorija ne dr\u017ei za instanco ter vrednost 1, \u010de kategorija dr\u017ei za instanco. Poglejmo si tabelo podatkov obeh \u010devljev po transformaciji kategori\u010dnih atributov v indikacijske atribute. Zdaj je primerjava mnogo bolj smiselna. Prav tako je opazno, da obstaja razlika tako v vodoodpornosti \u010devljev, kakor v barvi. Vi\u0161ina Te\u017ea Vod. Vod. Barva Barva Barva (da) (ne) (modri) (rjavi) (\u010drni) \u010cevlji A 17 231 1 0 0 0 1 \u010cevlji B 9 119 0 1 0 1 0 Razlika 8 112 1 -1 0 -1 1 Primer ra\u010dunanja razdalje z indikacijskimi atributi.","title":"Indikacijski atributi"},{"location":"pages/knjiga/05_knn/#kreacija-indikacijskih-atributov-v-pythonu","text":"Najenostavnej\u0161i postopek kreacije indikacijskih atributov je s pomo\u010djo pandas knji\u017enice. Struktura podatkov DataFrame ima \u017ee zabele\u017een tip vsakega stolpca, kar poenostavi avtomatsko transformacijo kategori\u010dnih atributov v indikacijske. Slede\u010da koda prikazuje kreacijo podatkov, iz katerih se bodo kreirali indikacijski atributi. import pandas as pd cevljiA = [ 17 , 231 , 'da' , 'rjavi' ] cevljiB = [ 9 , 119 , 'ne' , 'modri' ] cevljiC = [ 12 , 143 , 'ne' , '\u010drni' ] cevljiD = [ 8 , 112 , 'ne' , 'rjavi' ] cevljiE = [ 11 , 198 , 'da' , 'modri' ] cevljiF = [ 15 , 245 , 'da' , '\u010drni' ] # Z zdru\u017eitvijo instanc ustvarimo DataFrame podatki = pd . DataFrame ([ cevljiA , cevljiB , cevljiC , cevljiD , cevljiE , cevljiF ], columns = [ 'Vi\u0161ina' , 'Te\u017ea' , 'Vodood' , 'Barva' ], index = [ 'cevlji A' , 'cevlji B' , 'cevlji C' , 'cevlji D' , 'cevlji E' , 'cevlji F' ]) print ( podatki ) Vi\u0161ina Te\u017ea Vodood Barva cevlji A 17 231 da rjavi cevlji B 9 119 ne modri cevlji C 12 143 ne \u010drni cevlji D 8 112 ne rjavi cevlji E 11 198 da modri cevlji F 15 245 da \u010drni S pregledom vrednosti dtypes podatkov lahko ugotovimo kak\u0161nega tipa je posamezen atribut (stolpec). podatki . dtypes Vi\u0161ina int64 Te\u017ea int64 Vodood object Barva object dtype: object Atributa Vodood in Barva sta tipa object , kar pomeni, da sta obravnavana kot kategori\u010dni spremenljivki. Pred uporabo teh podatkov v procesu strojnega u\u010denja s knji\u017enico scikit-learn je potrebno spremeniti vse atribute object v \u0161tevilske. Knji\u017enica pandas ponuja metodo get_dummies() , ki pregleda podane podatke tipa DataFrame in vse atribute tipa object spremeni v indikacijske atribute. Izvorna spremenljivka se ne spremeni, temve\u010d se podatki z indikacijskimi atributi vrnejo kot rezultat metode. Pregled tipa atributov poka\u017ee, da so zdaj vsi atributi \u0161tevilskega tipa, s \u010dimer zadostimo uporabi v kombinaciji s scikit-learn knji\u017enico. # Vse kategori\u010dne (object) atribute spremenimo v indikacijske podatki_z_indikacijskimi = pd . get_dummies ( podatki ) podatki_z_indikacijskimi . dtypes Vi\u0161ina int64 Te\u017ea int64 Vodood_da uint8 Vodood_ne uint8 Barva_modri uint8 Barva_rjavi uint8 Barva_\u010drni uint8 dtype: object Pregled vsebine novih podatkov prika\u017ee novonastale indikacijske atribute. print ( podatki_z_indikacijskimi ) Vi\u0161ina Te\u017ea Vodood_da Vodovod_ne Barva_modri Barva_rjavi Barva_\u010drni cevlji A 17 231 1 0 0 1 0 cevlji B 9 119 0 1 1 0 0 cevlji C 12 143 0 1 0 0 1 cevlji D 8 112 0 1 0 1 0 cevlji E 11 198 1 0 1 0 0 cevlji F 15 245 1 0 0 0 1","title":"Kreacija indikacijskih atributov v Pythonu"},{"location":"pages/knjiga/05_knn/#skupna-razdalja","text":"Pri vmesnih izra\u010dunih razdalj \u0161e vedno ne pridemo do kon\u010dne skupne razdalje med dvema instancama. Za agregacijo vmesnih razdalj v eno mero razdalje pa lahko izberemo ve\u010d razli\u010dnih pristopov. Slede\u010da slika prikazuje ve\u010d vrst razdalj, ki bodo opisane v slede\u010dih sekcijah. Razli\u010dne razdalje med dvema to\u010dkama.","title":"Skupna razdalja"},{"location":"pages/knjiga/05_knn/#evklidska-razdalja","text":"Evklidsko razdaljo med dvema to\u010dkama v ravnini je definiral matematik Evklid. Deluje po principu Pitagorovega izreka, kjer se razdalja med dvema to\u010dkama oz. instancama ( \\(x_1\\) in \\(x_2\\) ) izra\u010duna kot koren vsote kvadratov vseh dimenzij. \\[\\begin{align*} \\begin{split} d_E\\left(x_1,x_2\\right)&=\\sqrt{\\left( x_1^{(1)}-x_2^{(1)}\\right)^2 + \\left( x_1^{(2)}-x_2^{(2)}\\right)^2 + \\dots \\left( x_1^{(l)}-x_2^{(l)}\\right)^2} \\\\ &=\\sqrt{\\sum \\nolimits _{i=1}^{l} \\left( x_1^{(i)}-x_2^{(i)}\\right)^2 } \\end{split} \\end{align*}\\] kjer je \\(l\\) \u0161tevilo atributov posamezne instance.","title":"Evklidska razdalja"},{"location":"pages/knjiga/05_knn/#mahattanska-razdalja","text":"Zelo pogosto pa nas zanima manhattanska razdalja, ki si zgled za ra\u010dunanje razdalje med dvema to\u010dkama vzame po postavitvi cest na otoku Manhattan, kot ka\u017ee slika spodaj. V ve\u010djem delu otoka so ceste postavljene vzdol\u017e otoka (avenije) in pre\u010dno po otoku (ulice). Pri ra\u010dunanju razdalje od ene to\u010dke do druge je tako potrebno v obzir vzeti dol\u017eine vseh cest med dvema to\u010dkama, pri tem pa ni mo\u017eno kraj\u0161ati poti z diagonalami. Razdalje na Manhattnu. Pri izra\u010dunu manhattanske razdalje tako ni najkraj\u0161a pot predstavljena kot diagonalna in najkraj\u0161a daljica med dvema instancama, ampak kot vsota vseh daljic, ki poteka vzdol\u017e vseh osi. \\[\\begin{align*} \\begin{split} d_M\\left(x_1,x_2\\right)&=\\left| x_1^{(1)}-x_2^{(1)}\\right| + \\left| x_1^{(2)}-x_2^{(2)}\\right| + \\dots \\left| x_1^{(l)}-x_2^{(l)}\\right| \\\\ &=\\sum \\nolimits _{i=1}^{l} \\left| x_1^{(i)}-x_2^{(i)}\\right| \\end{split} \\end{align*}\\]","title":"Mahattanska razdalja"},{"location":"pages/knjiga/05_knn/#kosinusna-razdalja","text":"Z manhattansko in evklidsko razdaljo pa pridemo do te\u017eav, ko imamo meritve, ki lahko imajo tako negativne kot pozitivne vrednosti. Primer take meritve bi bil letni zaslu\u017eek podjetja, saj je ta lahko tudi negativen (podjetje je na letni ravni imelo izgubo). Za primer vzemimo tri podjetja in njihove letne zaslu\u017eke ter spremembe dele\u017ea pokritega trga od prej\u0161njega leta, kot je na sliki. Primerjava evklidske, manhattanske in kosinusne razdalje. Tako evklidska kakor tudi manhattanska razdalja pravita, da sta podjetji A in C bli\u017eje kot pa podjetji A in B. \\[\\begin{align*} \\begin{split} d_E\\left(A,B\\right)&>d_E\\left(A,C\\right)\\\\ d_M\\left(A,B\\right)&>d_M\\left(A,C\\right)\\\\ d_C\\left(A,B\\right)&<d_M\\left(A,C\\right) \\end{split} \\end{align*}\\] To je v nasprotju z na\u0161o intuicijo: podjetje C je namre\u010d imelo izgubo v letnem prihodku in negativno spremembo pokritosti trga. Podjetji A in B pa sta imeli tako dobi\u010dek, kakor tudi se je njun dele\u017e pokritega trga pove\u010dal v primerjavi z lanskim letom. V takih situacijah sta evklidska in manhattanska razdalja neprimerni ter se uporabi kosinusna razdalja. Pri kosinusni razdalji je pomembna (1) smer vektorja posamezne instance ter (2) njegova dol\u017eina. Razliko med smerjo dveh vektorjev merimo s kotom \\(\\alpha\\) med vektorjema (instancama), ta pa je proporcionalna kosinusu kotov. Za instanci \\(x_1\\) in \\(x_2\\) velja slede\u010d izra\u010dun kosinusne razdalje. \\[\\begin{align*} \\begin{split} d_C\\left(x_1,x_2\\right)&=1- \\frac { x_1 \\cdot x_2}{||x_1|| \\cdot ||x_2||} \\\\ &=1-\\frac { \\sum _{i=1}^{l} x_1^{(i)} * x_2^{(i)}}{\\sqrt{\\sum _{i=1}^{l} x_1^{i~2}} * \\sqrt{\\sum _{i=1}^{l} x_2^{i~2}}} \\end{split} \\end{align*}\\] Izbira na\u010dina izra\u010duna razdalje je odvisna od problema. Najbolj intuitivna je vsekakor evklidska razdalja. Ve\u010dji je nabor atributov \\(l\\) , bolj je primerna manhattanska razdalja v primerjavi z evklidsko 1 . Kosinusna razdalja pa je primerna, ko nas bolj kot razdalje zanimajo podobnosti med instancami.","title":"Kosinusna razdalja"},{"location":"pages/knjiga/05_knn/#klasifikator-k-najblizjih-sosedov","text":"Pri nekaterih algoritmih klasifikacije ne nastane u\u010dni model, ampak se klasifikator odlo\u010da vsakokrat ob pogledu v \u017ee klasificirane instance. Taka vrsta u\u010denja se imenuje leno u\u010denje in algoritem klasifikacije k najbli\u017ejih sosedov (angl. k nearest neighbors ) je tipi\u010dni primer lenega algoritma u\u010denja. Sledi pregled delovanja tega algoritma k najbli\u017ejih sosedov.","title":"Klasifikator k najbli\u017ejih sosedov"},{"location":"pages/knjiga/05_knn/#delovanje-k-najblizjih-sosedov","text":"Algoritem k najbli\u017ejih sosedov ste\u010de po slede\u010dem postopku 2 , 3 . Za podano instanco \\(x_N\\) , ki jo \u017eelimo klasificirati, algoritem izra\u010duna razdalje do vseh \u017ee klasificiranih instanc. \u017de klasificirane instance razvrsti nara\u0161\u010dajo\u010de glede na razdaljo do instance \\(x_N\\) . V nadaljnji obravnavi upo\u0161teva le \\(k\\) prvih instanc v nara\u0161\u010dajo\u010dem seznamu - \\(k\\) najbli\u017ejih sosedov. Izra\u010duna pogostosti razredov iz nabora \\(k\\) najbli\u017ejih instanc kot je na sliki spodaj. Najpogostej\u0161i razred (modus) vrne kot rezultat klasifikacije instance \\(x_N\\) . \u010ce se ve\u010d razredov pojavlja z najve\u010djo pogostostjo, se izbere naklju\u010den razred izmed vseh najpogostej\u0161ih. Klasifikacija instance (moder trikotnik) s pomo\u010djo k najbli\u017ejih sosedov ob razli\u010dnih nastavitvah parametra k. Pri ra\u010dunanju razdalj je uporabljena evklidska razdalja. Sist. krvni tlak Tel. temp. Bolan Evklidska Manhattanska (mmHg) (\u00b0C) d rang d rang 82 38,0 Da 28,0 17 28,7 17 85 36,4 Da 25,1 16 27,3 16 89 36,7 Da 21,1 14 23,0 14 94 38,2 Da 16,0 11 16,5 11 95 38,2 Da 15,0 9 15,5 9 95 37,6 Ne 15,0 10 16,1 10 100 36,6 Ne 10,2 7 12,1 7 104 35,5 Ne 6,8 5 9,2 6 108 35,7 Ne 3,6 3 5,0 3 108 38,4 Da 2,0 2 2,3 2 109 39,4 Da 1,2 1 1,7 1 113 36,4 Ne 3,8 4 5,3 4 119 38,7 Da 9,0 6 9,0 5 124 37,5 Ne 14,1 8 15,2 8 128 37,8 Ne 18,0 12 18,9 12 129 38,8 Da 19,0 13 19,1 13 135 39,4 Da 25,0 15 25,7 15 140 36,6 Ne 30,1 18 32,1 18 145 36,0 Da 35,1 19 37,7 19 147 36,5 Da 37,1 20 39,2 20 U\u010dne instance. Sist. krvni tlak Tel. temp. Bolan (mmHg) (\u00b0C) 110 38,7 ? Nova instanca.","title":"Delovanje k najbli\u017ejih sosedov"},{"location":"pages/knjiga/05_knn/#primer-klasifikacije-pacientov","text":"Sledi primer izra\u010dunov algoritma k najbli\u017ejih sosedov po korakih na primeru diagnoze bolezni pacientov. Imamo u\u010dno mno\u017eico, kjer merimo sistoli\u010dni krvni tlak pacientov in njihovo telesno temperaturo. Med kartotekami imamo \u017ee zabele\u017eene podatke 20 prej\u0161njih pacientov in njihove diagnoze, ki so jih postavili zdravniki. Instance in nov pacient so prikazani v zgornji tabeli. V tabeli je prav tako podan izra\u010dun razdalj, evklidske in manhattanske. Izra\u010dun obeh razdalj med prvo instanco mno\u017eice in novo instanco pacienta poteka po slede\u010dem postopku. \\[\\begin{align*} \\begin{split} d_E\\left(x_N,x_1\\right)&=\\sqrt{\\left( 110-82\\right)^2 + \\left( 38,7 - 38,0\\right)^2}\\\\ &=\\sqrt{\\left(28\\right)^2 + \\left(0,7\\right)^2}\\\\ &=\\sqrt{784 + 0,49} =\\sqrt{784,49} = 28,0 \\\\ \\\\ d_M\\left(x_N,x_1\\right)&=\\left| 110-82\\right| + \\left| 38,7 - 38,0\\right|\\\\ &=\\left| 28\\right| + \\left| 0,7\\right|\\\\ &=28 + 0,7 =28,7 \\end{split} \\end{align*}\\] Razvidno je, da razlika v krvnem tlaku prevladuje pri izra\u010dunu razdalje. Do tega pride zaradi razli\u010dnih enot, posledica pa je, da razdalje atributov z ve\u010djimi \u0161tevilskimi vrednostmi prevladujejo nad atributi manj\u0161ih \u0161tevilskih vrednosti. \u010ce \u017eelimo prispevek atributov pri ra\u010dunanju skupne razdalje poenotiti, se je potrebno lotiti standardizacije podatkov, s \u010dimer postavimo vse meritve na enako zalogo vrednosti.","title":"Primer klasifikacije pacientov"},{"location":"pages/knjiga/05_knn/#standardizacija-podatkov","text":"Standardizacija (angl. standardization ) je postopek transformacije podatkov na tak na\u010din, da bodo ti po transformaciji imeli povpre\u010dje enako 0 in standardni odklon enak 1. Za standardizacijo podatkov \\(x\\) rabimo njihovo povpre\u010dje \\(\\mu\\) in standardni odklon \\(\\rho\\) , ki je definiran slede\u010de za vseh \\(n\\) instanc. \\[\\begin{align*} \\begin{split} {\\rho} &= \\sqrt{\\frac{(x_1-\\mu)^2+(x_1-\\mu)^2+\\dots+(x_n-\\mu)^2}{n}} \\end{split} \\end{align*}\\] Standardizirane vrednosti \\(z\\) izra\u010dunamo iz izvornih podatkov \\(x\\) slede\u010de. \\[\\begin{align*} \\begin{split} z &= \\frac{x-\\mu}{\\rho} \\end{split} \\end{align*}\\] Vsak atribut \\(x^1\\) , \\(x^2\\) , \\(\\dots\\) , \\(x^l\\) standardiziramo lo\u010deno - transformirano z njegovim povpre\u010djem in standardnim odklonom. Po standardizaciji vseh atributov (tudi indikacijskih) imajo vsi atributi povpre\u010dje v vrednosti 0 in standardni odklon 1. Razdalje posameznih atributov med instancami bodo tako v podobnem intervalu. Za standardizacijo podatkov pacientov tako potrebujemo podatke povpre\u010dja in standardnega odklona instanc, kar prikazuje spodnja ena\u010dba. \\[\\begin{align*} \\begin{split} \\mu\\left(\\text{Sistoli\u010dni krvni pritisk}\\right)&= 112,45\\\\ \\rho\\left(\\text{Sistoli\u010dni krvni pritisk}\\right)&= 19,58\\\\ \\mu\\left(\\text{Telesna temperatura}\\right)&= 37,42\\\\ \\rho\\left(\\text{Telesna temperatura}\\right)&= 1,17\\\\ \\end{split} \\end{align*}\\] Tabela prikazuje vrednosti atributov po standardizaciji. Sist. krvni tlak Tel. temp. Bolan Evklidska Manhattanska (mmHg) (\u00b0C) d rang d rang -1,55 0,49 Da 1,55 11 2,03 11 -1,40 -0,87 Da 2,34 15 3,24 17 -1,20 -0,61 Da 2,01 14 2,78 15 -0,94 0,66 Da 0,92 5 1,24 6 -0,89 0,66 Da 0,88 4 1,19 5 -0,89 0,15 Ne 1,21 8 1,70 8 -0,64 -0,70 Ne 1,86 12 2,30 13 -0,43 -1,64 Ne 2,74 19 3,03 16 -0,23 -1,47 Ne 2,56 17 2,66 14 -0,23 0,84 Da 0,28 1 0,36 1 -0,18 1,69 Da 0,60 3 0,65 3 0,03 -0,87 Ne 1,97 13 2,11 12 0,33 1,09 Da 0,46 2 0,46 2 0,59 0,07 Ne 1,25 9 1,74 9 0,79 0,32 Ne 1,20 7 1,69 7 0,85 1,18 Da 0,97 6 1,06 4 1,15 1,69 Da 1,41 10 1,87 10 1,41 -0,70 Ne 2,36 16 3,32 18 1,66 -1,21 Da 2,91 20 4,09 20 1,76 -0,78 Da 2,66 18 3,76 19 Standardizirane u\u010dne instance. Sist. krvni tlak Tel. temp. Bolan (mmHg) (\u00b0C) -0,13 1,09 ? Standardizirana nova instanca. Podatki po standardizaciji ka\u017eejo druga\u010dno sliko. Vrstni red najbli\u017ejih instanc podani instanci se spremeni. Instanca, ki je v tabeli ozna\u010dena, je pred standardizacijo bila \u010detrta najbli\u017eja podani instanci. Pred standardizacijo je namre\u010d bila razlika v temperaturi majhna v primerjavi z razliko v krvnem tlaku. Po standardizaciji pa je razlika v temperaturi mnogo ve\u010dja, kot razlika pri krvnem tlaku. To je prav, saj sta bili pred standardizacijo zalogi vrednosti obeh meritev druga\u010dni. Realni obseg telesne temperature \u010dloveka je pribli\u017eno med 35 in 41 \u00b0C, kar predstavlja maksimalno razliko 6 enot. \u0160est enot razlike pri krvnem tlaku pa ni niti pribli\u017eno realnemu obsegu sistoli\u010dnega krvnega tlaka, ki se lahko giblje v intervalu med 80 in 180 mmHg z maksimalno razliko kar 100 enot. Po standardizaciji se krvni tlak giblje med \\(-1,55\\) ter \\(1,76\\) z maksimalno razliko \\(3,31\\) . To je v skladu z obsegom telesne temperature, ki se po standardizaciji giblje med \\(-1,64\\) ter \\(1,69\\) z maksimalno razliko \\(3,33\\) . Standardizacija je le en postopek tehnike, ki jo imenujemo normalizacija podatkov . Alternative standardizaciji so min-max skaliranje , centriranje , rangiranje in drugi. Vsak pristop strojnega u\u010denja ni ob\u010dutljiv na intervale oz. skalo atributov. Med take \u0161tejemo algoritme kreacije odlo\u010ditvenih dreves in naivnega Bayesa. Dobra praksa je, da pri postopku podatkovnega rudarjenja podatke pred obdelavo vedno normaliziramo, saj se s tem izognemo morebitnemu zavajanju algoritmov. Negativna plat normalizacije podatkov pa je, da ti niso najve\u010dkrat enostavno interpretabilni. Kakor ka\u017eejo standardizirane vrednosti v zgornji tabeli, so vrednosti telesne temperature nesmiselne. To postane \u0161e posebej problemati\u010dno v primeru, ko gradimo model znanja, ki temelji na pravilih (odlo\u010ditvena pravila ali odlo\u010ditvena drevesa), saj bodo vrednosti v pravilih standardizirane.","title":"Standardizacija podatkov"},{"location":"pages/knjiga/05_knn/#standardizacija-v-pythonu","text":"Sledi primer, kjer za za\u010detek najprej pregledamo povpre\u010dne vrednosti in standardne odklone atributov pred standardizacijo. from sklearn.datasets import load_iris # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) print ( 'Povpre\u010dja pred standardizacijo:' ) print ( podatki . data . mean ( axis = 0 )) print ( 'Standardni odkloni pred standardizacijo:' ) print ( podatki . data . std ( axis = 0 )) Povpre\u010dja pred standardizacijo: sepal length (cm) 5.843333 sepal width (cm) 3.057333 petal length (cm) 3.758000 petal width (cm) 1.199333 dtype: float64 Standardni odkloni pred standardizacijo: sepal length (cm) 0.828066 sepal width (cm) 0.435866 petal length (cm) 1.765298 petal width (cm) 0.762238 dtype: float64 Sedaj pa te podatke standardiziramo in izpi\u0161emo povpre\u010dja ter standardne odklone novih vrednosti. Knji\u017enica scikit-learn ponuja kar nekaj na\u010dinov transformacije podatkov. Za standardizacijo se uporablja razred StandardScaler . S klicem metode fit se izra\u010dunata povpre\u010dje in standardni odklon iz podanih podatkov. Za transformacijo podatkov, pa se uporabi klic metode transform , kateri podamo podatke, ki jih \u017eelimo transformirati. from sklearn.preprocessing import StandardScaler # Inicializacija standardizatorja std = StandardScaler () # Izra\u010dunamo povpre\u010dja in standardne odklone atributov std . fit ( podatki . data ) # Standardizacija podatkov stand_podatki = std . transform ( podatki . data ) print ( 'Povpre\u010dja po standardizaciji:' ) print ( stand_podatki . mean ( axis = 0 )) print ( 'Standardni odkloni po standardizaciji:' ) print ( stand_podatki . std ( axis = 0 )) ovpre\u010dja po standardizaciji: -4.73695157e-16 -7.81597009e-16 -4.26325641e-16 -4.73695157e-16 Standardni odkloni po standardizaciji: 1. 1. 1. 1. Pregled rezultatov povpre\u010dij ka\u017ee, da so te zelo blizu vrednosti 0 ( \\(10^{-15}\\) ), standardni odkloni pa so enaki 1. \u010ce \u017eelimo proces izra\u010duna povpre\u010dij in standardnih odklonov ter proces transformacije podatkov zdru\u017eiti, lahko uporabimo metodo fit_transform , ki na podanih podatkih izra\u010duna vmesne vrednosti in jih vrne transformirane. stand_podatki = std . fit_transform ( podatki . data )","title":"Standardizacija v Pythonu"},{"location":"pages/knjiga/05_knn/#dolocitev-razreda-podane-instance","text":"Po izra\u010dunu razdalj in dolo\u010ditvi rangov glede na bli\u017eino do nove instance sledi dolo\u010ditev razreda (klasifikacija) nove instance. Pri tem procesu igra pomembno vlogo dolo\u010ditev vrednosti parametra \\(k\\) , ki nam pove, koliko najbli\u017ejih instanc upo\u0161tevamo pri klasifikaciji. Vseh \\(k\\) najbli\u017ejih instanc bo namre\u010d glasovalo za razred nove instance - vsaka instanca bo dala glas za razred, kateremu le-ta pripada. \u010ce je parameter \\(k=1\\) , potem vzamemo le najbli\u017ejo instanco in bo novi instanci dodeljen razred te instance. \u010ce se nave\u017eemo na primer iz tabele zgoraj, je pri izra\u010dunu obeh razdalj najbli\u017eja ista instanca - ta je razreda Da kar pomeni, da tudi novo instanco klasificiramo v razred Da . Tabeli spodaj prikazujeta rezultate klasifikacije pri razli\u010dnih nastavitvah - pri standardiziranih in nestandardiziranih podatkih, pri razli\u010dnih vrednostih \\(k\\) in pri evklidski ter manhattanski razdalji. Evklidska Manhattanska k Da Ne Rezultat Da Ne Rezultat 2 2 0 Da 2 0 Da 3 2 1 Da 2 1 Da 4 2 2 Da/Ne 2 2 Da/Ne 5 2 3 Ne 3 2 Da Klasifikacija s k najbli\u017ejih sosedov pri razli\u010dnih nastavitvah na nestandardiziranih podatkih. Evklidska Manhattanska k Da Ne Rezultat Da Ne Rezultat 2 2 0 Da 2 0 Da 3 3 0 Da 3 0 Da 4 4 0 Da 4 0 Da 5 5 0 Da 5 0 Da Klasifikacija s k najbli\u017ejih sosedov pri razli\u010dnih nastavitvah na standardiziranih podatkih. Zgornja tabela ka\u017ee zanimive rezultate. Najprej poglejmo nestandardizirane podatke. Tako pri evklidski, kot tudi pri manhattanski razdalji, se z ve\u010danjem \u0161tevila najbli\u017ejih instanc, ki se upo\u0161tevajo pri klasifikaciji, ve\u010da tudi negotovost, saj iz razreda Da pri upo\u0161tevanju le ene najbli\u017eje instance ( \\(k=1\\) ) preidemo do negotovosti, ko upo\u0161tevamo \u0161tiri najbli\u017eje instance ( \\(k=4\\) ), pa vse do spremembe odlo\u010ditve v razred Ne , ko upo\u0161tevamo pet najbli\u017ejih instanc ( \\(k=5\\) ) pri evklidski razdalji. Ti rezultati nam ka\u017eejo tudi razliko med evklidsko in manhattansko razdaljo, saj se odlo\u010ditve kon\u010dnega razreda instance ne skladajo pri \\(k=5\\) . Hkrati pa se moramo soo\u010diti \u0161e z negotovostjo pri \\(k=4\\) . Ko pridemo do neodlo\u010denega izida, se algoritem odlo\u010di za en naklju\u010den razred v vodstvu (tisti, ki ima manj\u0161i indeks), kar pa ni vedno najbolj\u0161a odlo\u010ditev. \u010ce bi ro\u010dno \u017eeleli izni\u010diti mo\u017enosti za neodlo\u010dene izide in naklju\u010dne odlo\u010ditve med njimi, bi algoritem preprosto zagnali \u0161e na drugih nastavitvah vrednosti \\(k\\) in pogledali, kak\u0161en je kon\u010den razred tedaj. Ko imamo opravek z binarno klasifikacijo (delitev instanc v dva razreda), pa se neodlo\u010denih izidov lahko znebimo z liho vrednostjo \\(k\\) . Po drugi strani pa ob pregledu rezultatov po standardizaciji vidimo ve\u010djo stabilnost, saj obstaja konsenz pri vseh nastavitvah \\(k\\) in pri obeh tipih razdalje. Ta pristop se tako izka\u017ee kot bolj robusten na manj\u0161e spremembe, pa tudi konceptualno je primernej\u0161i, saj vsi atributi instanc enakovredno vplivajo na odlo\u010ditev klasifikacije.","title":"Dolo\u010ditev razreda podane instance"},{"location":"pages/knjiga/05_knn/#uporaba-k-najblizjih-sosedov-v-pythonu","text":"Algoritem k najbli\u017ejih sosedov je v knji\u017enici scikit-learn implementiran z razredom KNeighborsClassifier . \u017de pri inicializaciji primerka tega razreda dolo\u010dimo \\(k\\) \u0161tevilo najbli\u017ejih sosedov s parametrom n_neighbors in na\u010din ra\u010dunanja razdalje s parametrom metric . from sklearn.neighbors import KNeighborsClassifier klasif = KNeighborsClassifier ( n_neighbors = 3 , metric = 'manhattan' ) klasif . fit ( X_u , y_u ) napovedi = klasif . predict ( X_t ) Pri ra\u010dunanju razdalje lahko uporabimo evklidsko razdaljo z euclidean , mahattansko z manhattan in kosinusno razdaljo s cosine . S klicem metode fit in podajo podatkov instanc X_u in njihovih razredov y_u te shranimo in bodo slu\u017eili za izra\u010dun najbli\u017ejih sosedov. Metodo predict pa kli\u010demo s podajo mno\u017eice instanc X_t , ki jih \u017eelimo klasificirati. To je tudi standardni postopek uporabe drugih algoritmov klasifikacije, regresije in gru\u010denja v knji\u017enici scikit-learn : Nastavitve definiramo v konstruktorju. Model znanja zgradimo s fit(podatki_instanc, resitve_instanc) . Model znanja uporabimo s predict(podatki_novih_instanc) . Algoritmi pa lahko imajo tudi sebi specifi\u010dne metode. V primeru k najbli\u017ejih sosedov je njemu posebna metoda kneighbors , kateri podamo instance, za katere i\u0161\u010demo najbli\u017eje sosede, ter parameter n_neighbors , s katerim povemo, koliko najbli\u017ejih sosedov i\u0161\u010demo iz nabora vseh instanc podanih \u017ee prej v metodi fit . Rezultat sta dva seznama: seznam razdalj od izbranih instanc do najbli\u017ejih sosedov v nara\u0161\u010dajo\u010dem vrstnem redu glede na razdaljo, ter seznam indeksov najbli\u017ejih instanc, ponovno od najbli\u017ejega naprej. klasif . fit ( X_u , y_u ) razdalje , sosedi = klasif . kneighbors ( nove_instance , n_neighbors = 3 ) Sledi pregled uporabe klasifikacijskega algoritma k najbli\u017ejih sosedov za namen iskanja petih najbli\u017ejih sosedov eni instanci. Najprej s slede\u010do kodo nalo\u017eimo instance podatkovne mno\u017eice Iris , iz nje izberemo instanco v vrstici z indeksom 133 ter jo odstranimo iz podatkovne mno\u017eice. from sklearn.datasets import load_iris # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Izberemo eno instanco izbrana = 133 X_izbrana = podatki . data . iloc [ izbrana ,:] y_izbrana = podatki . target . iloc [ izbrana ] # Izbrano instanco izlo\u010dimo iz ostalih podatkov X_ostali = podatki . data . drop ( izbrana , axis = 0 ) y_ostali = podatki . target . drop ( izbrana ) Izbira vrednosti iz polja poteka s pomo\u010djo klica iloc[vrstica, stolpec] , kamor podamo indeks vrstice in indeks stolpca. \u010ce \u017eelimo izbrati celotno vrsto, podamo namesto indeksa kar dvopi\u010dje : . Tako s podatki.data.iloc[:, 12] izberemo stolpec z indeksom 12, s klicem podatki.data.iloc[8, :] pa izberemo vrstico z indeksom 8. \u010ce \u017eelimo izbrati ve\u010d vrednosti, pa namesto \u0161tevila na mestu vrstice in stolpca podamo polje indeksov. S klicem podatki.data.iloc[[2, 5, 8], :] izberemo vrstice s temi indeksi. In obratno, s klicem podatki.data.iloc[:, [3, 6, 9]] se vrnejo stolpci z indeksi 3, 6 in 9. Pri izbiri vrednosti iz vektorja podamo le eno vrednost, saj ima ta le eno dimenzijo. Tako nam klic podatki.target.iloc[[2, 5, 8]] vrne podatke z indeksi 2, 5 in 8. Z metodo podatki.data.drop(izbrana, axis=0) odstranimo instanco z indeksom izbrana iz podatkov podatki.data . Parameter axis dolo\u010da, po kateri osi izbri\u0161emo podatke - \u010de je podana 0, izbri\u0161emo vrstico, \u010de pa 1, pa izbri\u0161emo stolpec. Pri izbrisu iz podatki.target parametra axis ni potrebno podati, saj so podatki v obliki vektorja, ki ima le eno dimenzijo.","title":"Uporaba k najbli\u017ejih sosedov v Pythonu"},{"location":"pages/knjiga/05_knn/#vizualizacija-podatkov","text":"S pomo\u010djo knji\u017enice seaborn lahko enostavno vizualiziramo instance. Z dvema klicema metode scatterplot se en na drugega izri\u0161eta dva grafa raztrosa. S parametroma x in y podamo podatke, ki naj so izrisani na teh oseh. import seaborn as sns x_os , y_os = 0 , 1 # Izri\u0161emo ostale instance sns . scatterplot ( x = X_ostali . iloc [:, x_os ], y = X_ostali . iloc [:, y_os ], hue = podatki . target_names [ y_ostali ], palette = 'colorblind' ) # Izri\u0161emo eno izbrano instanco sns . scatterplot ( x = [ X_izbrana . iloc [ x_os ]], y = [ X_izbrana . iloc [ y_os ]], hue = [ 'Neznan' ], style = [ 'Neznan' ], markers = { 'Neznan' : '^' }) Instance podatkovne zbirke Iris. Horizontalna x os predstavlja prvi atribut, vertikalna y os pa drugi atribut podatkovne mno\u017eice. Barve lo\u010dijo razrede instanc, s trikotnikom pa je ozna\u010dena instanca, ki jo \u017eelimo klasificirati. Pomanjkljivost grafi\u010dne predstavitve podatkov je, da lahko izri\u0161emo instance glede na omejeno \u0161tevilo njihovih atributov. V na\u0161em primeru imamo dvodimenzionalen graf, kjer smo posamezne osi dolo\u010dili s spremenljivkama x_os in y_os . Parameter hue prejme podatke, ki bodo narisane ozna\u010dbe delili glede na barvo - v na\u0161em primeru so to podatki o razredu. Parameter style pa prejme podatke, ki dolo\u010dajo stil ozna\u010dbe, ki jih definiramo s parametrom markers . Barve ozna\u010db dolo\u010damo s podajanjem teme v parameter palette .","title":"Vizualizacija podatkov"},{"location":"pages/knjiga/05_knn/#ucenje-in-uporaba-modela-znanja","text":"U\u010denje modela in napoved razreda instance na indeksu 133 poteka na slede\u010d na\u010din. Najprej s konstruktorjem dolo\u010dimo vrednost \\(k\\) na pet najbli\u017ejih sosedov po izra\u010dunu manhattanske razdalje. Temu sledi klic metode fit , kateri podamo podatke instanc X_ostali in njihove razrede y_ostali . Ker metoda predict pri\u010dakuje ve\u010d instanc, dodamo instanco X_izbrana najprej v seznam in ta seznam podamo v klic metode predict([X_izbrana]) . \u010ce X_izbrana ne bi bil vektor, ampak polje, bi klic metode potekal brez ovijanja v seznam predict(X_izbrana) . from sklearn.neighbors import KNeighborsClassifier # Inicializiramo klasifikator knn = KNeighborsClassifier ( n_neighbors = 5 , metric = 'manhattan' ) # Shranimo instance za primerjavo knn . fit ( X_ostali , y_ostali ) # Napovemo razred izbrane instance napoved = knn . predict ([ X_izbrana ]) print ( f 'KNN je napovedal, da je instanca razreda { podatki . target_names [ napoved ] } .' ) print ( f 'Ta instanca je dejansko razreda { podatki . target_names [ y_izbrana ] } .' ) KNN je napovedal, da je instanca razreda ['versicolor']. Ta instanca je dejansko razreda virginica. Klasifikator je napovedal napa\u010den razred za to instanco. Preglejmo katerih pet instanc je po izra\u010dunu manhattanske razdalje najbli\u017eje na\u0161i izbrani instanci iz vrstice z indeksom 133. # Izbrani instanci najdemo pet najbli\u017ejih sosedov razdalje , sosedi = knn . kneighbors ([ X_izbrana ], n_neighbors = 5 ) print ( f 'Pet najbli\u017ejih: { sosedi } ' ) print ( f 'Razdalje od najbli\u017ejih do izbrane: { razdalje } ' ) Pet najbli\u017ejih: [[ 72 83 123 126 54]] Razdalje od najbli\u017ejih do izbrane: [[0.5 0.5 0.6 0.7 0.7]] Pet najbli\u017ejih instanc po manhattanski razdalji so instance z indeksi 72, 83, 123, 126 in 54. Rezultati razdalj pa so kar manhattanske razdalje teh sosedov do podane instance.","title":"U\u010denje in uporaba modela znanja"},{"location":"pages/knjiga/05_knn/#vpliv-nastavitev-na-rezultate","text":"Poglejmo, kako se spremeni seznam petih najbli\u017ejih instanc, ko spreminjamo na\u010din izra\u010duna razdalje med instancami. for razdalja in [ 'euclidean' , 'manhattan' , 'cosine' ]: knn = KNeighborsClassifier ( n_neighbors = 5 , metric = razdalja ) knn . fit ( X_ostali , y_ostali ) razdalje , najblizje = knn . kneighbors ([ X_izbrana ], n_neighbors = 5 ) print ( f 'Najbli\u017eje instance po { razdalja } so { najblizje } ' ) print ( f 'Razdalje so { razdalje } ' ) Najbli\u017eje instance po euclidean so [[ 83 72 123 126 127]] Razdalje so [[0.33166248 0.36055513 0.37416574 0.43588989 0.45825757]] Najbli\u017eje instance po manhattan so [[ 72 83 123 126 54]] Razdalje so [[0.5 0.5 0.6 0.7 0.7]] Najbli\u017eje instance po cosine so [[125 129 90 131 83]] Razdalje so [[0.0001158 0.00022532 0.00033682 0.00034546 0.00039085]] Izra\u010dun petih najbli\u017ejih sosedov po evklidski in manhattanski razdalji vrne podobne rezultate. Instanca z indeksom 83 je najbli\u017eja izbrani po izra\u010dunu evklidske razdalje in je druga najbli\u017eja po manhattanski razdalji - mesto si izmenja z instanco 72. Tretje in \u010detrto mesto sta v obeh razdaljah zasedli instanci 123 in 125. Peto mesto ima v primeru evklidske razdalje instanca 127, v primeru manhattanske pa instanca 54. \u010ce sta instanci 127 in 54 druga\u010dnega razreda, lahko ta razlika vpliva na razred izbrane instance. Seznam najbli\u017ejih instanc po izra\u010dunu kosinusne razdalje pa je skorajda popolnoma druga\u010den od ostalih dveh - le instanca 83 je v vseh treh seznamih. Vse ostale \u0161tiri instance so v seznamu kosinusne razdalje druge v primerjavi s seznamoma evklidske in manhattanske razdalje. Ti rezultati nam ka\u017eejo, kako pomembna je odlo\u010ditev glede na\u010dina izra\u010duna razdalje. Slede\u010da slika ka\u017ee delitev obmo\u010dja razredov glede na na\u010din ra\u010dunanja razdalje. Pri kreaciji slike sta bila upo\u0161tevana prva dva atributa in vrednost \u0161tevila sosedov k je bila nastavljena na 5. Slika ka\u017ee, da ve\u010djih razlik med evklidsko in manhattansko razdaljo pri tej podatkovni mno\u017eici in nastavitvi k ni. Delitev obmo\u010dij je relativno jasna, z nekoliko ve\u010djim prekrivanjem v sredini, kjer so si instance razredov versicolor in virginica zelo podobne. Po drugi strani je razvidno, da delitev po izra\u010dunu glede na kosinusno razdaljo ni primerna za podano podatkovno mno\u017eico, saj sta obmo\u010dji versicolor in virginica preve\u010d prepleteni in je klasifikacija instanc v tem obmo\u010dju nestabilna. Delitev na obmo\u010dja razredov glede na na\u010din ra\u010dunanja razdalj med instancami. Poglejmo si \u0161e, kako vrednost k vpliva na kon\u010dno klasifikacijo izbrane instance. for k in [ 1 , 3 , 5 , 8 , 10 ]: knn = KNeighborsClassifier ( n_neighbors = k , metric = 'euclidean' ) knn . fit ( X_ostali , y_ostali ) napoved = knn . predict ([ X_izbrana ]) print ( f 'KNN z k= { k } je napovedal, da je izbrana instanca razreda { podatki . target_names [ napoved ] } ' ) KNN z k=1 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=3 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=5 je napovedal, da je izbrana instanca razreda ['virginica'] KNN z k=8 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=10 je napovedal, da je izbrana instanca razreda ['virginica'] Pri spreminjanju \u0161tevila najbli\u017ejih sosedov ne vidimo konsenza. Razred izbrane instance z indeksom 133 se namre\u010d spreminja iz (napa\u010dnega) razreda versicolor ob glasovanju enega, treh in osmih najbli\u017ejih sosedov, v (pravilen) razred virginica ob glasovanju petih ali desetih najbli\u017ejih sosedov. Ponovno je razvidno, da nastavitev igra vlogo pri napovedih algoritma. Vrednost k se najve\u010dkrat dolo\u010di po preizku\u0161anju, vsekakor pa mora biti vrednost smiselna (\u010de je k prevelik, bo v resnici algoritem vrnil najpogostej\u0161i razred). Preve\u010d optimiziranja nastavitev za namen bolj\u0161e klasifikacije ene instance je nesmiselno. Instanca indeksa 133 je bila izbrana namenoma, saj se napovedi njenega razreda zelo spreminjajo ob druga\u010dnih nastavitvah. Ve\u010dji del instanc dobi enako napoved razreda, ne glede na tip razdalje in k . To nam pove ve\u010d o tej instanci kot pa o samem algoritmu. Mogo\u010de je ta nekoliko nenavadna, ali pa je bila \u017ee v osnovi (s strani ekspertov) klasificirana napa\u010dno. Iz tega sledi, da je smiselno gledati rezultate in kakovost klasifikacije na ve\u010d instancah, ne le na eni - kaj pa \u010de je ta izbrana nenavadno. V naslednjem poglavju bomo spoznali na\u010dine ovrednotenja kakovosti klasifikacije in proces pravilne izbire instanc, s katerimi testiramo izbrani algoritem klasifikacije in njegovih nastavitev. Spodnja slika prikazuje razli\u010dna obmo\u010dja razredov glede na razli\u010dne vrednosti \u0161tevila sosedov k . Pri kreaciji slike sta bila ponovno upo\u0161tevana le prva dva atributa podatkov in ra\u010dunanje evklidskih razdalj. Iz slike je razvidno, da majhne vrednosti k prinesejo kar nekaj majhnih podro\u010dij klasifikacije. Po drugi strani pa je razvidno, da nastavitev \u0161tevila sosedov na 10 nekoliko pokvari delitev obmo\u010dja. Delitev na obmo\u010dja razredov glede na \u0161tevilo najbli\u017ejih sosedov k: 1, 3, 5, 8 in 10.","title":"Vpliv nastavitev na rezultate"},{"location":"pages/knjiga/05_knn/#vpliv-standardizacije-podatkov-na-rezultate","text":"Poglejmo si, \u010de se klasifikacija kaj spremeni, \u010de uporabimo standardizirane podatke. from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import StandardScaler # Standardiziramo podatke standardizator = StandardScaler () standardizator . fit ( X_ostali ) X_ostali_s = standardizator . transform ( X_ostali ) X_izbrana_s = standardizator . transform ([ X_izbrana ]) for razdalja in [ 'euclidean' , 'manhattan' , 'cosine' ]: for k in [ 1 , 3 , 5 ]: knn = KNeighborsClassifier ( n_neighbors = k , metric = razdalja ) knn . fit ( X_ostali_s , y_ostali ) nap = knn . predict ( X_izbrana_s ) print ( f 'KNN metric= { razdalja } , k= { k } je napovedal razred { podatki . target_names [ nap ] } ' ) KNN metric=euclidean, k=1 je napovedal razred ['versicolor'] KNN metric=euclidean, k=3 je napovedal razred ['versicolor'] KNN metric=euclidean, k=5 je napovedal razred ['versicolor'] KNN metric=manhattan, k=1 je napovedal razred ['versicolor'] KNN metric=manhattan, k=3 je napovedal razred ['versicolor'] KNN metric=manhattan, k=5 je napovedal razred ['versicolor'] KNN metric=cosine, k=1 je napovedal razred ['versicolor'] KNN metric=cosine, k=3 je napovedal razred ['virginica'] KNN metric=cosine, k=5 je napovedal razred ['versicolor'] Objekt razreda StandardScaler najprej nau\u010dimo, kaj so povpre\u010dja in standardni odkloni podatkov s pomo\u010djo klica fit . Kasneje to uporabilo za transformacijo (standardizacijo) tako ostalih podatkov X_ostali , kakor tudi izbrane instance X_izbrana . Pri tem uporabimo klic metode transform . Napovedi so mnogo bolj robustne na spreminjanje nastavitev klasifikacijskega algoritma, ko imamo opravek s standardiziranimi podatki. Namre\u010d, najpogosteje napovedan razred je bil versicolor . \u010ceprav je napoved razreda napa\u010dna, imamo raje robustne napovedi, kot pa take, ki so preve\u010d odvisne od nastavitev algoritma.","title":"Vpliv standardizacije podatkov na rezultate"},{"location":"pages/knjiga/05_knn/#enovit-primer-klasifikacije-vec-instanc","text":"Sledi enovit primer klasifikacije ve\u010d instanc iz Iris podatkovne zbirke, kjer so podatki tudi standardizirani. from sklearn.neighbors import KNeighborsClassifier from sklearn.datasets import load_iris import numpy as np from sklearn.preprocessing import StandardScaler # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Izberemo ve\u010d instanc izbrane = [ 1 , 31 , 61 , 91 , 121 ] X_izbrane = podatki . data . iloc [ izbrane ,:] y_izbrane = podatki . target . iloc [ izbrane ] # Izbrano instanco izlo\u010dimo iz ostalih podatkov X_ostali = podatki . data . drop ( izbrane , axis = 0 ) y_ostali = podatki . target . drop ( izbrane ) # Standardiziramo podatke standardizator = StandardScaler () standardizator . fit ( X_ostali ) X_ostali_s = standardizator . transform ( X_ostali ) X_izbrane_s = standardizator . transform ( X_izbrane ) # Zgradimo klasifikator in napovedmo razrede knn = KNeighborsClassifier ( n_neighbors = 3 , metric = 'euclidean' ) knn . fit ( X_ostali_s , y_ostali ) napovedi = knn . predict ( X_izbrane_s ) for i , napoved , dejansko in zip ( izbrane , napovedi , y_izbrane ): print ( f 'Instanca { i } je klasificirana kot { podatki . target_names [ napoved ] } dejansko pa je { podatki . target_names [ dejansko ] } .' ) Instanca 1 je klasificirana kot setosa dejansko pa je setosa. Instanca 31 je klasificirana kot setosa dejansko pa je setosa. Instanca 61 je klasificirana kot versicolor dejansko pa je versicolor. Instanca 91 je klasificirana kot versicolor dejansko pa je versicolor. Instanca 121 je klasificirana kot virginica dejansko pa je virginica. Aggarwal, C.C., Hinneburg, A. and Keim, D.A., 2001, January. On the surprising behavior of distance metrics in high dimensional space. In International conference on database theory (pp. 420-434). Springer, Berlin, Heidelberg. \u21a9 Aha, D.W., Kibler, D. and Albert, M.K., 1991. Instance-based learning algorithms. Machine learning, 6(1), pp.37-66. \u21a9 Goldberger, J., Hinton, G.E., Roweis, S. and Salakhutdinov, R.R., 2004. Neighbourhood components analysis. Advances in neural information processing systems, 17. \u21a9","title":"Enovit primer klasifikacije ve\u010d instanc"},{"location":"pages/knjiga/09_algoritmi/","text":"Nadzorovano u\u010denje Regresija Klasifikacija k najbli\u017ejih sosedov","title":"09 algoritmi"}]}