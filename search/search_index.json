{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Domov"},{"location":"pages/avtorja/","text":"Avtorja Sa\u0161o Karakati\u010d ( doma\u010da stran , LinkedIn ) in Iztok Fister ml. ( doma\u010da stran ) sta docenta na Fakulteti za elektrotehniko, ra\u010dunalni\u0161tvo in informatiko Univerze v Mariboru, ter je \u010dlana Laboratorija za inteligentne sisteme v okviru In\u0161tituta za informatiko .","title":"Avtorja"},{"location":"pages/avtorja/#avtorja","text":"Sa\u0161o Karakati\u010d ( doma\u010da stran , LinkedIn ) in Iztok Fister ml. ( doma\u010da stran ) sta docenta na Fakulteti za elektrotehniko, ra\u010dunalni\u0161tvo in informatiko Univerze v Mariboru, ter je \u010dlana Laboratorija za inteligentne sisteme v okviru In\u0161tituta za informatiko .","title":"Avtorja"},{"location":"pages/sodelovanje/","text":"Sodelovanje fnkdslfasd","title":"Sodelovanje"},{"location":"pages/sodelovanje/#sodelovanje","text":"fnkdslfasd","title":"Sodelovanje"},{"location":"pages/knjiga/00_knjiga/","text":"Strojno u\u010denje Gradivo slu\u017ei kot uvod v podro\u010dje strojnega u\u010denja za vse, ki imajo vsaj osnovne izku\u0161nje s programiranjem. Pregledajo se pomembni pojmi strojnega u\u010denja (model znanja, u\u010dna in testna mno\u017eica, algoritem u\u010denja), natan\u010dneje pa se predstavi tehnika klasifikacije in na\u010din ovrednotenja kvalitete modelov znanja klasifikacije. Spozna se algoritem klasifikacije k najbli\u017ejih sosedov in predstavi se uporaba tega algoritma - tako konceptualno kakor v programski kodi. Knjiga poda \u0161tevilne primere v programskem jeziku Python in okolju Jupyter Notebooks. Za namen utrjevanja znanja pa so ponujene naloge (tako ra\u010dunske, kot programerske) s podanimi re\u0161itvami. Avtorja: Sa\u0161o Karakati\u010d & Iztok Fister ml. Klju\u010dne besede: strojno u\u010denje, umetna inteligenca, klasifikacija, k najbli\u017ejih sosedov, Python Struktura gradiva Uvod Za\u010detno poglavje predstavi namen gradiva. Vzpostavitev Phyton delovnega okolja Poglavje govori o vzpostavitvi okolja, ki bo primerno za uporabo knji\u017enic, ki jih to gradivo vklju\u010duje, za zagon primerov iz gradiva in za re\u0161evanje prakti\u010dnih nalog. \u010ce ima\u0161 Python okolje \u017ee vzpostavljeno na svojem ra\u010dunalniku, lahko to poglavje presko\u010di\u0161. O strojnem u\u010denju To poglavje za\u010dne s splo\u0161no razlago definicije strojnega u\u010denja in vzpostavi vzporednice tega z na\u0161im (\u010dlove\u0161kim) u\u010denjem novega znanja. O klasifikaciji Poglavje je namenjeno predstavitvi glavne tehnike strojnega u\u010denja v tem gradivu - klasifikaciji podatkov. Opisi in definicije gredo od najenostavnej\u0161e razlage na primerih pa do formalne matemati\u010dne definicije pojmov. To poglavje presko\u010di, \u010de si z osnovnimi tehnikami \u017ee seznanjen/-a in bi se \u017eelel/-a hitro posvetiti programiranju. K najbli\u017ejih sosedov Poglavje obravnava izbran algoritem klasifikacije v tem gradivu - k najbli\u017ejih sosedov. Algoritem je najprej predstavljen neodvisno od programiranja na na\u010din, ki ti bo omogo\u010dal, da ga zna\u0161 re\u0161iti tudi na roko. Predstavljene so nekatere nastavitve tega algoritma, s \u010dimer se predstavi pomembnost poznavanja podrobnosti algoritma, da je uporaba tega kar se da u\u010dinkovita. Opisu in definicijam pa sledijo primeri v programski kodi. Ti primeri so napisani tako, da se lahko neposredno uporabijo pri testiranju in spoznavanju. Za utrjevanje znanja iz tega poglavja so prilo\u017eene tudi \u0161tiri naloge - dve preverjata teoreti\u010dno razumevanje in se re\u0161ita na roko, dve pa sta programerskega tipa. Tega poglavja nikar ne presko\u010di, saj predstavlja osnovo, brez katere ne gre. Kakovost klasifikacije To poglavje pa se dotakne pomembnega podro\u010dja ovrednotenja kakovosti uporabljenih tehnik strojnega u\u010denja. Strojno u\u010denje nam prinese dodatno vrednost le, \u010de deluje pravilno. Kako pa izmerimo, \u010de deluje pravilno? V tem poglavju so predstavljene razli\u010dne metrike kakovosti klasifikacijskih modelov in pristopi, k pravilni vzpostavi testnega okolja (in podatkov) za vrednotenje modelov. Ker je ovrednotenje kakovosti klasifikacijskih modelov klju\u010dnega pomena za razvoj uporabnih modelov, je tudi to poglavje priporo\u010dljivo za za\u010detnike. O gradivu To poglavje predstavi bibliografske podatke gradiva in navede na\u010din navajanja tega gradiva.","title":"Povzetek gradiva"},{"location":"pages/knjiga/00_knjiga/#strojno-ucenje","text":"Gradivo slu\u017ei kot uvod v podro\u010dje strojnega u\u010denja za vse, ki imajo vsaj osnovne izku\u0161nje s programiranjem. Pregledajo se pomembni pojmi strojnega u\u010denja (model znanja, u\u010dna in testna mno\u017eica, algoritem u\u010denja), natan\u010dneje pa se predstavi tehnika klasifikacije in na\u010din ovrednotenja kvalitete modelov znanja klasifikacije. Spozna se algoritem klasifikacije k najbli\u017ejih sosedov in predstavi se uporaba tega algoritma - tako konceptualno kakor v programski kodi. Knjiga poda \u0161tevilne primere v programskem jeziku Python in okolju Jupyter Notebooks. Za namen utrjevanja znanja pa so ponujene naloge (tako ra\u010dunske, kot programerske) s podanimi re\u0161itvami. Avtorja: Sa\u0161o Karakati\u010d & Iztok Fister ml. Klju\u010dne besede: strojno u\u010denje, umetna inteligenca, klasifikacija, k najbli\u017ejih sosedov, Python","title":"Strojno u\u010denje"},{"location":"pages/knjiga/00_knjiga/#struktura-gradiva","text":"Uvod Za\u010detno poglavje predstavi namen gradiva. Vzpostavitev Phyton delovnega okolja Poglavje govori o vzpostavitvi okolja, ki bo primerno za uporabo knji\u017enic, ki jih to gradivo vklju\u010duje, za zagon primerov iz gradiva in za re\u0161evanje prakti\u010dnih nalog. \u010ce ima\u0161 Python okolje \u017ee vzpostavljeno na svojem ra\u010dunalniku, lahko to poglavje presko\u010di\u0161. O strojnem u\u010denju To poglavje za\u010dne s splo\u0161no razlago definicije strojnega u\u010denja in vzpostavi vzporednice tega z na\u0161im (\u010dlove\u0161kim) u\u010denjem novega znanja. O klasifikaciji Poglavje je namenjeno predstavitvi glavne tehnike strojnega u\u010denja v tem gradivu - klasifikaciji podatkov. Opisi in definicije gredo od najenostavnej\u0161e razlage na primerih pa do formalne matemati\u010dne definicije pojmov. To poglavje presko\u010di, \u010de si z osnovnimi tehnikami \u017ee seznanjen/-a in bi se \u017eelel/-a hitro posvetiti programiranju. K najbli\u017ejih sosedov Poglavje obravnava izbran algoritem klasifikacije v tem gradivu - k najbli\u017ejih sosedov. Algoritem je najprej predstavljen neodvisno od programiranja na na\u010din, ki ti bo omogo\u010dal, da ga zna\u0161 re\u0161iti tudi na roko. Predstavljene so nekatere nastavitve tega algoritma, s \u010dimer se predstavi pomembnost poznavanja podrobnosti algoritma, da je uporaba tega kar se da u\u010dinkovita. Opisu in definicijam pa sledijo primeri v programski kodi. Ti primeri so napisani tako, da se lahko neposredno uporabijo pri testiranju in spoznavanju. Za utrjevanje znanja iz tega poglavja so prilo\u017eene tudi \u0161tiri naloge - dve preverjata teoreti\u010dno razumevanje in se re\u0161ita na roko, dve pa sta programerskega tipa. Tega poglavja nikar ne presko\u010di, saj predstavlja osnovo, brez katere ne gre. Kakovost klasifikacije To poglavje pa se dotakne pomembnega podro\u010dja ovrednotenja kakovosti uporabljenih tehnik strojnega u\u010denja. Strojno u\u010denje nam prinese dodatno vrednost le, \u010de deluje pravilno. Kako pa izmerimo, \u010de deluje pravilno? V tem poglavju so predstavljene razli\u010dne metrike kakovosti klasifikacijskih modelov in pristopi, k pravilni vzpostavi testnega okolja (in podatkov) za vrednotenje modelov. Ker je ovrednotenje kakovosti klasifikacijskih modelov klju\u010dnega pomena za razvoj uporabnih modelov, je tudi to poglavje priporo\u010dljivo za za\u010detnike. O gradivu To poglavje predstavi bibliografske podatke gradiva in navede na\u010din navajanja tega gradiva.","title":"Struktura gradiva"},{"location":"pages/knjiga/01_uvod/","text":"Uvod Strojno u\u010denje, umetna inteligenca, podatkovna znanost, podatkovno rudarjenje in velepodatki (ali kar veliki podatki). V zadnjih letih so to precej uporabljeni izrazi, ki jih zasledimo v novicah o revolucionarnih premikih na podro\u010djih obdelave podatkov, razpoznave slik ter videoposnetkov in obdelave besedila. S temi izrazi podjetniki zvito opi\u0161ejo svoje storitve in produkte, da jim dvignejo vrednost ali jih la\u017eje prodajo. S temi izrazi razvijalci programske opreme opi\u0161ejo svoje izdelke, ko so se ti zmo\u017eni prilagajati na razli\u010dne situacije. Pa gre res za revolucionarne metode ali le za trenutno modo oziroma modne besede (angl. buzzwords )? So algoritmi strojnega u\u010denja res nekaj, kar izra\u017ea inteligentnost ra\u010dunalnika, ali pa gre le za kombinacijo if stavkov in statisti\u010dnih metod? \u017dal ni enovitega odgovora. Inteligentnega in samostojnega stroja, kot so prikazani v nekaterih filmskih uspe\u0161nicah, ne moremo pri\u010dakovati \u0161e nekaj \u010dasa, \u010de sploh kdaj. Vsekakor pa imajo pristopi strojnega u\u010denja vrednost, kar ka\u017ee tudi vztrajnost zanimanja in njihove uporabe tako v industriji kakor tudi raziskovalni sferi. Strojno u\u010denje je danes klju\u010den sestavni del \u0161tevilnih komercialnih informacijskih sistemov in raziskovalnih projektov na razli\u010dnih podro\u010djih - od medicinske diagnoze in zdravljenja 1 , avtomatskega trgovanja z vrednostnimi papirji 2 , samovoze\u010dih avtomobilov 3 , pa do priporo\u010dil na dru\u017ebenih omre\u017ejih in spletnih trgovinah 4 . Mnogi menijo, da je uporaba strojnega u\u010denja primerna za uporabo le v velikih podjetjih z obse\u017enimi raziskovalnimi skupinami in globokimi \u017eepi. S tem gradivom ti avtorja \u017eeliva pokazati, kako enostavna je uporaba strojnega u\u010denja, \u010de le \u017ee pozna\u0161 osnove programiranja. Kot na drugih podro\u010djih, tudi pri strojnem u\u010denju obstajajo tako kompleksni in napredni pristopi, kakor tudi enostavni. Za razumevanje najbolj naprednih pristopov (na primer globokega u\u010denja (angl. deep learning )) res potrebujemo \u017ee kar nekaj semestrov matematike. Po drugi strani pa razumevanje najenostavnej\u0161ih pristopov zahteva le izku\u0161nje iz programiranja. Seveda se najve\u010d govori o najbolj naprednih metodah, ki terjajo ekipo dvestotih podatkovnih znanstvenikov (kot se imenujemo) in za dve ko\u0161arkarski dvorani velik ra\u010dunalnik. V resnici pa ve\u010dji del informacijskih sistemov, ki so podprti s strojnim u\u010denjem, uporablja zelo elementarne tehnike. Vsakdo pa \u017ee ne potrebuje naprednega algoritma, ki identificira \u010dloveka iz treh pikslov. V ve\u010dji meri so za obogatitev obstoje\u010dih sistemov dovolj enostavni pristopi, kar je razvidno iz uporabe umetne inteligence v praksi. Nova storitev, katere razvijalci poudarjajo, da je podprta z naprednimi pristopi umetne inteligence, res nima le nekaj pogojnih if -ov in osnovnih opisno statisti\u010dnih izra\u010dunov, kljub temu pa navadno ni tako kompleksna, kot se mogo\u010de zdi. Komu je to gradivo namenjeno? To gradivo slu\u017ei kot uvod v podro\u010dje strojnega u\u010denja. Namenjena je tistim, ki \u017ee znajo programirati - idealno v programskem jeziku Python, saj so primeri prikazani z uporabo tega jezika. Zakaj Python? Ker je to eden izmed najbolj uporabljenih programskih jezikov za namen uporabe strojnega u\u010denja. Knjiga se osredoto\u010da na uporabo Pythona in knji\u017enice scikit-learn , ki je osnovna knji\u017enica za uporabo strojnega u\u010denja. Seveda se ne gre izogniti uporabi knji\u017enic NumPy in Matplotlib oz. pandas in seaborn - ampak poudarek vsekakor ni na teh. Zavestno je bila narejena odlo\u010ditev, da se gradivo preve\u010d ne osredoto\u010da na matemati\u010dni vidik strojnega u\u010denja. Dr\u017ei, matemati\u010dne formule so \u0161e vedno prisotne, ampak le do te mere, da pomagajo (ne pa ovirajo) pri razumevanju posameznih pristopov. Knjiga se osredoto\u010di le na eno tehniko strojnega u\u010denja - na klasifikacijo. Klasifikacija se predstavi na le enem, po mnenju avtorjev, najbolj intuitivnem algoritmu klasifikacije, imenovanemu k najbli\u017ejih sosedov. Ta algoritem je predstavljen nekoliko bolj podrobno, s \u010dimer se ponazori, da ima vsak pristop strojnega u\u010denja svoje posebnosti, ki ga naredijo bodisi primernega ali neprimernega za dan problem. Komu to gradivo ni namenjeno? V gradivu avtorja okvirno predstaviva razli\u010dne tehnike in podro\u010dja strojnega u\u010denja - podrobnosti pa izpustiva. \u010ce te zanima regresija, katera izmed tehnik nenadzorovanega u\u010denja ali delo z globokimi nevronskimi mre\u017eami, to gradivo ni zate. Vsak dober podatkovni znanstvenik je najprej spoznaval temelje strojnega u\u010denja ter se \u0161ele potem posvetil naprednej\u0161im tehnikam. \u010cemu nam bo napredna globoka nevronska mre\u017ea, \u010de pa je ne znamo pravilno ovrednotiti? Knjiga je napisana tako, da bo samostojno nadaljevanje u\u010denja kar se da enostavno. V gradivu niso omenjene vse mo\u017ene knji\u017enice strojnega u\u010denja v Pythonu ali mo\u017enosti uporabe strojnega u\u010denja v drugih programskih jezikih. \u010cetudi te delo na strojnem u\u010denju v Pythonu zanese k uporabi PyTorcha ali TensorFlowa , je scikit-learn osnova, brez katere ne gre. \u010cetudi te bosta delodajalec ali lastna radovednost v prihodnosti usmerila v uporabo strojnega u\u010denja v drugih jezikih, so tudi ostale knji\u017enice strojnega u\u010denja spisane po zgledu knji\u017enice scikit-learn . Po povr\u0161nem pregledu gradiva bi kdo morda pri\u0161el do zaklju\u010dka, da to gradivo pokrije snov, ki bi jo lahko predstavili v eni objavi na blogu. Dr\u017ei - \u010de bi snov, predstavljeno v tem gradivu, strnili v programsko kodo, to ne bi bil kompleksen informacijski sistem, temve\u010d le dalj\u0161a Python skripta. Namen gradiva ni, da predstavi \u010dim ve\u010d razli\u010dnih na\u010dinov uporabe knji\u017enice scikit-learn in predela vse mo\u017ene algoritme v tej - temu namre\u010d slu\u017ei dokumentacija te knji\u017enice. Skozi branje se od bralca tega gradiva pri\u010dakuje, da vsak korak (oziroma vsako vrstico kode) razume - kaj se zgodi in \u010demu je namenjena. Skozi poglobljeno razumevanje ti avtorja \u017eeliva predati sposobnost nadaljnjega samostojnega u\u010denja - kaj je pomembno pri dolo\u010denem pristopu, kako se uporabi ta pristop, kako nastavitve vplivajo na rezultate in tako naprej. Povedano druga\u010de, s tem gradivom te avtorja \u017eeliva nau\u010diti samostojnega u\u010denja snovi strojnega u\u010denja. Dobrodo\u0161el oziroma dobrodo\u0161la v svet strojnega u\u010denja. Holzinger, A., Langs, G., Denk, H., Zatloukal, K. and M\u00fcller, H., 2019. Causability and explainability of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(4), p.e1312. \u21a9 Fister, D., Mun, J.C., Jagri\u010d, V. and Jagri\u010d, T., 2019. Deep learning for stock market trading: a superior trading strategy?. Neural Network World, 29(3), pp.151-171. \u21a9 Del Ser, J., Osaba, E., Sanchez-Medina, J.J. and Fister, I., 2019. Bioinspired computational intelligence and transportation systems: a long road ahead. IEEE Transactions on Intelligent Transportation Systems, 21(2), pp.466-495. \u21a9 Bello-Orgaz, G., Jung, J.J. and Camacho, D., 2016. Social big data: Recent achievements and new challenges. Information Fusion, 28, pp.45-59. \u21a9","title":"Uvod"},{"location":"pages/knjiga/01_uvod/#uvod","text":"Strojno u\u010denje, umetna inteligenca, podatkovna znanost, podatkovno rudarjenje in velepodatki (ali kar veliki podatki). V zadnjih letih so to precej uporabljeni izrazi, ki jih zasledimo v novicah o revolucionarnih premikih na podro\u010djih obdelave podatkov, razpoznave slik ter videoposnetkov in obdelave besedila. S temi izrazi podjetniki zvito opi\u0161ejo svoje storitve in produkte, da jim dvignejo vrednost ali jih la\u017eje prodajo. S temi izrazi razvijalci programske opreme opi\u0161ejo svoje izdelke, ko so se ti zmo\u017eni prilagajati na razli\u010dne situacije. Pa gre res za revolucionarne metode ali le za trenutno modo oziroma modne besede (angl. buzzwords )? So algoritmi strojnega u\u010denja res nekaj, kar izra\u017ea inteligentnost ra\u010dunalnika, ali pa gre le za kombinacijo if stavkov in statisti\u010dnih metod? \u017dal ni enovitega odgovora. Inteligentnega in samostojnega stroja, kot so prikazani v nekaterih filmskih uspe\u0161nicah, ne moremo pri\u010dakovati \u0161e nekaj \u010dasa, \u010de sploh kdaj. Vsekakor pa imajo pristopi strojnega u\u010denja vrednost, kar ka\u017ee tudi vztrajnost zanimanja in njihove uporabe tako v industriji kakor tudi raziskovalni sferi. Strojno u\u010denje je danes klju\u010den sestavni del \u0161tevilnih komercialnih informacijskih sistemov in raziskovalnih projektov na razli\u010dnih podro\u010djih - od medicinske diagnoze in zdravljenja 1 , avtomatskega trgovanja z vrednostnimi papirji 2 , samovoze\u010dih avtomobilov 3 , pa do priporo\u010dil na dru\u017ebenih omre\u017ejih in spletnih trgovinah 4 . Mnogi menijo, da je uporaba strojnega u\u010denja primerna za uporabo le v velikih podjetjih z obse\u017enimi raziskovalnimi skupinami in globokimi \u017eepi. S tem gradivom ti avtorja \u017eeliva pokazati, kako enostavna je uporaba strojnega u\u010denja, \u010de le \u017ee pozna\u0161 osnove programiranja. Kot na drugih podro\u010djih, tudi pri strojnem u\u010denju obstajajo tako kompleksni in napredni pristopi, kakor tudi enostavni. Za razumevanje najbolj naprednih pristopov (na primer globokega u\u010denja (angl. deep learning )) res potrebujemo \u017ee kar nekaj semestrov matematike. Po drugi strani pa razumevanje najenostavnej\u0161ih pristopov zahteva le izku\u0161nje iz programiranja. Seveda se najve\u010d govori o najbolj naprednih metodah, ki terjajo ekipo dvestotih podatkovnih znanstvenikov (kot se imenujemo) in za dve ko\u0161arkarski dvorani velik ra\u010dunalnik. V resnici pa ve\u010dji del informacijskih sistemov, ki so podprti s strojnim u\u010denjem, uporablja zelo elementarne tehnike. Vsakdo pa \u017ee ne potrebuje naprednega algoritma, ki identificira \u010dloveka iz treh pikslov. V ve\u010dji meri so za obogatitev obstoje\u010dih sistemov dovolj enostavni pristopi, kar je razvidno iz uporabe umetne inteligence v praksi. Nova storitev, katere razvijalci poudarjajo, da je podprta z naprednimi pristopi umetne inteligence, res nima le nekaj pogojnih if -ov in osnovnih opisno statisti\u010dnih izra\u010dunov, kljub temu pa navadno ni tako kompleksna, kot se mogo\u010de zdi.","title":"Uvod"},{"location":"pages/knjiga/01_uvod/#komu-je-to-gradivo-namenjeno","text":"To gradivo slu\u017ei kot uvod v podro\u010dje strojnega u\u010denja. Namenjena je tistim, ki \u017ee znajo programirati - idealno v programskem jeziku Python, saj so primeri prikazani z uporabo tega jezika. Zakaj Python? Ker je to eden izmed najbolj uporabljenih programskih jezikov za namen uporabe strojnega u\u010denja. Knjiga se osredoto\u010da na uporabo Pythona in knji\u017enice scikit-learn , ki je osnovna knji\u017enica za uporabo strojnega u\u010denja. Seveda se ne gre izogniti uporabi knji\u017enic NumPy in Matplotlib oz. pandas in seaborn - ampak poudarek vsekakor ni na teh. Zavestno je bila narejena odlo\u010ditev, da se gradivo preve\u010d ne osredoto\u010da na matemati\u010dni vidik strojnega u\u010denja. Dr\u017ei, matemati\u010dne formule so \u0161e vedno prisotne, ampak le do te mere, da pomagajo (ne pa ovirajo) pri razumevanju posameznih pristopov. Knjiga se osredoto\u010di le na eno tehniko strojnega u\u010denja - na klasifikacijo. Klasifikacija se predstavi na le enem, po mnenju avtorjev, najbolj intuitivnem algoritmu klasifikacije, imenovanemu k najbli\u017ejih sosedov. Ta algoritem je predstavljen nekoliko bolj podrobno, s \u010dimer se ponazori, da ima vsak pristop strojnega u\u010denja svoje posebnosti, ki ga naredijo bodisi primernega ali neprimernega za dan problem.","title":"Komu je to gradivo namenjeno?"},{"location":"pages/knjiga/01_uvod/#komu-to-gradivo-ni-namenjeno","text":"V gradivu avtorja okvirno predstaviva razli\u010dne tehnike in podro\u010dja strojnega u\u010denja - podrobnosti pa izpustiva. \u010ce te zanima regresija, katera izmed tehnik nenadzorovanega u\u010denja ali delo z globokimi nevronskimi mre\u017eami, to gradivo ni zate. Vsak dober podatkovni znanstvenik je najprej spoznaval temelje strojnega u\u010denja ter se \u0161ele potem posvetil naprednej\u0161im tehnikam. \u010cemu nam bo napredna globoka nevronska mre\u017ea, \u010de pa je ne znamo pravilno ovrednotiti? Knjiga je napisana tako, da bo samostojno nadaljevanje u\u010denja kar se da enostavno. V gradivu niso omenjene vse mo\u017ene knji\u017enice strojnega u\u010denja v Pythonu ali mo\u017enosti uporabe strojnega u\u010denja v drugih programskih jezikih. \u010cetudi te delo na strojnem u\u010denju v Pythonu zanese k uporabi PyTorcha ali TensorFlowa , je scikit-learn osnova, brez katere ne gre. \u010cetudi te bosta delodajalec ali lastna radovednost v prihodnosti usmerila v uporabo strojnega u\u010denja v drugih jezikih, so tudi ostale knji\u017enice strojnega u\u010denja spisane po zgledu knji\u017enice scikit-learn . Po povr\u0161nem pregledu gradiva bi kdo morda pri\u0161el do zaklju\u010dka, da to gradivo pokrije snov, ki bi jo lahko predstavili v eni objavi na blogu. Dr\u017ei - \u010de bi snov, predstavljeno v tem gradivu, strnili v programsko kodo, to ne bi bil kompleksen informacijski sistem, temve\u010d le dalj\u0161a Python skripta. Namen gradiva ni, da predstavi \u010dim ve\u010d razli\u010dnih na\u010dinov uporabe knji\u017enice scikit-learn in predela vse mo\u017ene algoritme v tej - temu namre\u010d slu\u017ei dokumentacija te knji\u017enice. Skozi branje se od bralca tega gradiva pri\u010dakuje, da vsak korak (oziroma vsako vrstico kode) razume - kaj se zgodi in \u010demu je namenjena. Skozi poglobljeno razumevanje ti avtorja \u017eeliva predati sposobnost nadaljnjega samostojnega u\u010denja - kaj je pomembno pri dolo\u010denem pristopu, kako se uporabi ta pristop, kako nastavitve vplivajo na rezultate in tako naprej. Povedano druga\u010de, s tem gradivom te avtorja \u017eeliva nau\u010diti samostojnega u\u010denja snovi strojnega u\u010denja. Dobrodo\u0161el oziroma dobrodo\u0161la v svet strojnega u\u010denja. Holzinger, A., Langs, G., Denk, H., Zatloukal, K. and M\u00fcller, H., 2019. Causability and explainability of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(4), p.e1312. \u21a9 Fister, D., Mun, J.C., Jagri\u010d, V. and Jagri\u010d, T., 2019. Deep learning for stock market trading: a superior trading strategy?. Neural Network World, 29(3), pp.151-171. \u21a9 Del Ser, J., Osaba, E., Sanchez-Medina, J.J. and Fister, I., 2019. Bioinspired computational intelligence and transportation systems: a long road ahead. IEEE Transactions on Intelligent Transportation Systems, 21(2), pp.466-495. \u21a9 Bello-Orgaz, G., Jung, J.J. and Camacho, D., 2016. Social big data: Recent achievements and new challenges. Information Fusion, 28, pp.45-59. \u21a9","title":"Komu to gradivo ni namenjeno?"},{"location":"pages/knjiga/02_okolje/","text":"Vzpostavitev okolja Za namen prikaza prakti\u010dnih primerov bo v tej knjigi uporabljen programski jezik Python, saj je en izmed bolj priljubljenih programskih jezikov z odli\u010dnim naborom knji\u017enic strojnega u\u010denja. Za razumevanje primerov je vsekakor priporo\u010dljivo poznavanje tega programskega jezika ali vsaj splo\u0161no poznavanje enega izmed modernih programskih jezikov (Java, Perl, C#, R, C, C++). Sledijo navodila vzpostavitve okolja, primernega za izvedbo prakti\u010dnih primerov na svojem ra\u010dunalniku. Razvojno okolje Jupyter Notebook Vsi prakti\u010dni primeri v knjigi so prikazani tako, da se brez te\u017eave izvedejo v JupyterLab okolju. To razvojno okolje omogo\u010da pisanje Python programske kode v tako imenovanih Jupyter zvezkih (angl. Jupyter notebooks ). Ti zvezki se urejajo v poljubnem spletnem brskalniku in zato za urejanje ne potrebujejo namenskega orodja. Posebnost kode, zapisane v Jupyter zvezkih, je, da je ta v eni datoteki (zvezku) me\u0161ana s takoj\u0161njimi izpisi kode in navodili, ki jih razvijalci zapi\u0161emo sproti za razlago delovanja kode. Zvezki so razdeljeni na tako imenovane celice, kjer je celica bodisi navodilo ali pa programska koda z izpisom rezultata, kot je prikazano na spodnji sliki. Jupyter zvezek na spletni storitvi Google Colab. Namen takega razvoja je, da je programska koda z navodili enostavno deljiva in razumljiva. Prav tako razvoj v zvezku omogo\u010da inkrementalno zaganjanje kode, celico za celico - tako se za\u017eenejo le \u017eeleni deli kode in ne celotna datoteka. To je idealen na\u010din razvoja za u\u010denje in za raziskovanje. Jupyter zvezki so postali privzeti na\u010din razvoja na podro\u010dju podatkovne znanosti, saj zadostijo primarnemu namenu podatkovne znanosti - odkrivanju vzorcev in pregledu podatkov skozi zgodbo. Tako je vsaka celica s kodo in prilo\u017eeno celico navodil kar en del zgodbe, kjer celica z navodili opi\u0161e, kaj bo celica s kodo naredila ter povzame njene rezultate. Alternativnih razvojnih okolij je mnogo. Od popolnoma namenskih za programiranje v Pythonu, kot sta PyCharm in Spyder , pa do splo\u0161nih razvojnih okolij, kot sta Visual Studio Code in Atom . Uporaba obla\u010dnega okolja Prvi na\u010din uporabe Jupyter zvezkov za namen strojnega u\u010denja je uporaba ene izmed ponujenih storitev. V ta namen so na voljo \u0161tevilni ponudniki z bodisi zastonjskimi ali pla\u010dljivimi storitvami izvajanja Python kode v oblaku. Dober primer take storitve je Google Colab , ki vsem svojim uporabnikom ponuja kreacijo Jupyter zvezkov in zagon teh na Googlovih stre\u017enikih, do dolo\u010dene mere zastonj - v \u010dasu pisanja knjige je uporaba Google Colab zvezkov za namen zagona primerov in nalog bila brezpla\u010dna. Google Colab je prikazan prav na zgornji sliki. Konkurence na podro\u010dju obla\u010dnih zvezkov je ogromno in uporabnost teh se mese\u010dno spreminja ter je vezana na ceno storitve, dostopnost slovenskim razvijalcem in nabor funkcionalnosti. Preprosto spletno iskanje \"Jupyter Notebook service\"~ali \"Google Colab alternative\"~vrne \u0161tevilne rezultate. V \u010dasu pisanja knjige so med omembe vrednimi bile storitve drugih gigantov strojnega u\u010denja Microsoft Azure Machine Learning in Amazon SageMaker ter s strojno opremo radodarna Paperspace in Deepnote . Namestitev in uporaba lokalnega okolja Anaconda V tej sekciji bo predstavljeno, kako vzpostavimo primerno okolje na svojem ra\u010dunalniku. Da namestimo Python in vse potrebne knji\u017enice, se bomo zatekli k okolju Anaconda, ki vsebuje tako Python kot skupek \u0161tevilnih knji\u017enic, ki se uporabljajo za namen analize podatkov. Na uradni spletni strani okolja Anaconda poi\u0161\u010demo predel za prenos razli\u010dice, namenjene za posameznike. V \u010dasu pisanja knjige se ta razli\u010dica imenuje Individual Edition in je dosegljiva na uradni spletni strani ter prikazana spodaj. Spletna stran Python okolja Anaconda. Okolje Anaconda poleg programskega jezika Python namesti tudi \u0161tevilne knji\u017enice, namenjene za podatkovno analizo v tem programskem jeziku. Skozi knjigo bomo uporabili \u0161tevilne izmed teh: NumPy , ki se uporablja za upravljanje s podatki v matri\u010dnih in ve\u010d-dimenzionalnih poljih. Podrobnej\u0161ega dela s to knji\u017enico ne bo, je pa vseeno dobro, da se zavedamo, da je to ena izmed klju\u010dnih knji\u017enic, ko imamo opravek s podatki in analizo teh. Ta knji\u017enica je tudi sestavni del slede\u010de knji\u017enice. pandas je knji\u017enica, ki je namenjena za manipulacijo in analizo podatkov v tabelari\u010dni obliki. \u010ceprav v zaledju uporablja knji\u017enico NumPy , so dolo\u010dene operacije nad podatki poenostavljene. scikit-learn je knji\u017enica, ki vsebuje implementacije razli\u010dnih algoritmov, metrik in pristopov pred-procesiranja za strojno u\u010denje. Algoritem klasifikacije, ki bo predstavljen v tej knjigi, je povzet po implementaciji iz te knji\u017enice. Matplotlib , ki se uporablja za prikaz grafov razli\u010dnih vrst. Je zelo prilagodljiva, saj omogo\u010da velik nabor razli\u010dnih tipov vizualizacij podatkov in prilagoditve teh (tako stilno, kot vsebinsko). Je pa zaradi svoje prilagodljivost delo s to knji\u017enico nekoliko te\u017eje in zaradi tega uporabljamo ... seaborn , ki poenostavi prikaz grafov. V zaledju uporablja knji\u017enico Matplotlib , ampak preko svojih vmesnikov dolo\u010dene pogoste operacije pri risanju grafov poenostavi. Uporaba Jupyter zvezkov Za pisanje Jupyter zvezkov na lastnem ra\u010dunalniku je najprej potreben zagon okolja JupyterLab, za kar lahko uporabimo enega izmed dveh na\u010dinov. Prvi na\u010din je zagon JupyterLaba ro\u010dno iz komandne vrstice: 1. Za\u017eenemo komandno vrstico. V Windowsih sta to bodisi Command Prompt ali PowerShell. V Linux in MacOS operacijskih sistemih pa je komanda vrstica najve\u010dkrat pod imenom Terminal. 2. Za\u017eenemo ukaz jupyter lab . 3. Odpre se okno brskalnika in poka\u017ee se doma\u010da stran JupyterLab, kot je prikazano na spodnji sliki. Drugi na\u010din je preko uporabni\u0161kega vmesnika okolja Anaconda. Za\u017eenemo Anaconda Navigator. V seznamu ponujenih aplikacij najdimo JupyterLab in ga s pritiskom mo\u017enosti Launch za\u017eenemo. Odpre se okno brskalnika in poka\u017ee se doma\u010da stran JupyterLab, kot je prikazano na slede\u010di sliki. Doma\u010da stran JupyterLab okolja. Po \u017eelji naredimo novo mapo, kamor shranimo na\u0161e zvezke in druge podporne datoteke. V tej mapi ustvarimo novi zvezek, kot je prikazano slede\u010de. Kreacija novega zvezka v JupyterLab okolju. Odpre se novo okno s praznim zvezkom, kot je prikazano spodaj. Jupyter zvezke razvijamo preko spletnega vmesnika, kar je tudi razlog za prikaz spletnega brskalnika. Nov prazen Jupyter zvezek. Zgornja slika ima ozna\u010dene pomembne dele uporabni\u0161kega vmesnika. Oznaka 1 prikazuje naslov zvezka, kar s klikom na napis lahko spremenimo. Oznaka 2 prikazuje tip celice, ki je trenutno izbrana, oznaka 3 pa prikazuje trenutno izbrano in hkrati zaenkrat \u0161e edino celico. Markdown v Jupyter zvezkih Za za\u010detek spremenimo tip prve celice na Markdown , ki je stil zapisa navodil in druge vsebine v celico. V celico zapi\u0161emo slede\u010d Markdown zapis. # Prvi primer V __prvem__ primeru bomo: - Naredili prvi izpis `Python` kode. - Naredili prvi graf s pomo\u010djo _seaborn_ knji\u017enice. S pritiskom na gumb zagona, ki je na prej\u0161nji sliki 4 (ali s pritiskom Shift + Enter ), se izbrana celica izvede in izbere se (ter \u010de ne obstaja se tudi ustvari) naslednja celica. Ker je izbrana celica bila tipa Markdown, se je zapisana koda oblikovala po stilu Markdown zapisa. Osnovni ukazi za zapis Markdowna so slede\u010di. Z znakom #, \u010demur sledi presledek definiramo naslov. # za glavne naslove, ## za podnaslove, ### za tretji nivo naslovov, pa vse do \u0161estega nivoja naslovov z ###### . Besedilo lahko tudi poudarimo. Po\u0161evno zapisano besedilo: _besedilo_ ali *besedilo* ter krepko _ zapisano besedilo: __besedilo__ ali **besedilo** . Dodamo lahko tudi neurejen seznam, tako da vsak element seznama zapi\u0161emo v svoji vrstici, za\u010dnemo pa ga bodisi z znakom * ali z - . Sezname lahko gnezdimo - ugnezdene elemente zamaknemo s tabulatorjem. - Prvi element. * Drugi element. - Prvi ugnezden element. - Pa \u0161e drugi. Urejene sezname, kjer si elementi sledijo v vrstnem redu, pa ustvarimo tako, da pred elemente dodamo \u0161tevilko ali oznako vrstnega reda. Tudi take sezname lahko gnezdimo. \u0160tevilko vrstnega reda lahko podamo poljubno, saj ni potrebno, da so te v pravem vrstnem redu. 1. Prvi element. 2. Drugi element. 11. Prvi ugnezden element. 12. Pa \u0161e drugi. Besedilo v stilu kode pa v besedilo zapi\u0161emo med znakoma ` , ki ga na slovenski tipkovnici izberemo z AltGr + 7 . Spremenljivka `vrednost` in razred `podatki` . Tabelo vstavimo vrstico po vrstico, vsaka celica v vrstici se za\u010dne in kon\u010da z znakom | (na slovenski tipkovnici AltGr + W ). Tako je ena vrstica s tremi celicami zapisana na slede\u010d na\u010din. |Prva celica|Druga celica|Tretja celica| \u010crte med vrstice pa dodamo kar z znakom - namesto vsebine celice. Primer tabele s tremi stolpci in tremi vrsticami ter \u010drto med prvo in drugo vrstico izgleda slede\u010de. |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |-|-|-| |Druga vrstica|12|-42| |Tretja vrstica|-7|65| Za la\u017ejo razumevanje kode lahko to nekoliko oblikujemo s presledki. |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |----------------------------|-------------|--------------| |Druga vrstica |12 |-42 | |Tretja vrstica |-7 |65 | Hiperpovezave vstavljamo s slede\u010do kodo [Napis povezave](URL) , slike pa z zapisom ![Naziv slike](URL do datoteke slike) - razlika je v klicaju. [ Povezava do iskalnika Google ]( http://www.google.com/ ) ![ Primer slike ]( /imgs/slika.png ) \u010ce ne gre druga\u010de, pa lahko vsebino celic navodil oblikujemo tudi s preprosto HTML kodo. HTML in Markdown zapis lahko po \u017eelji me\u0161amo. Kon\u010den primer vsega skupaj bi zapisali s slede\u010do kodo, rezultat pa je prikazan na sliki pod kodo. ## Preizkus Markdown zapisa 1. Preizkusili smo oblikovanje pisav 1. __krepko__ in 2. _po\u0161evno_ pisavo 2. Pisanje seznamov - urejenih in neurejenih, - ter ugnezdenih 3. Dodajanje [ povezava ]( http://www.um.si/ ) 4. Dodajanje slik ![ FERI ]( https://feri.um.si/site/images/logo-feri.png ) 5. Pisanje v stilu `programske kode` 6. Dodajanje tabel |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |----------------------------|-------------|--------------| |Druga vrstica |12 |-42 | |Tretja vrstica |-7 |65 | 7. Tudi <a href=\"https://feri.um.si/\">HTML koda</a> deluje. Rezultat Markdown zapisa. Python koda v Jupyter zvezkih \u010ce ima celica vsebino tipa Code , pa se vsebina smatra kot Python koda - primerno je tudi obarvana, deluje avtomatsko dopolnjevanje kode (angl. code auto-completion ) in rezultati kode se izpi\u0161ejo kar takoj pod celico. Sledi primer zapisa programske kode v celico. V prvi vrstici se uvozi knji\u017enica NumPy ter se poimenuje kot np za nadaljnjo uporabo. Sledi izpis niza znakov z ukazom print() - izpis sledi kar pod celico, kot je prikazano na slede\u010di sliki. Tretja vrstica vsebuje komentar v kodi, ki opisuje zadnjo vrstico. Zadnja vrstica pa vsebuje klic metode rand() knji\u017enice NumPy , ki vrne polje naklju\u010dnih \u0161tevil po podanih velikostih (v primeru je podana velikost polja s tremi vrsticami in dvema stolpcema). Ker ta vrstica vsebuje tudi zadnji ukaz, ki vra\u010da rezultat, se rezultat tega izpi\u0161e tudi pod celico. import numpy as np print ( 'Sledi izpis polja naklju\u010dnih \u0161tevil s tremi vrsticami in dvema stolpcema.' ) # Rezultat zadnjega ukaza v celici se izpi\u0161e kot rezultat celice. np . random . rand ( 3 , 2 ) Sledi izpis polja naklju\u010dnih \u0161tevil s tremi vrsticami in dvema stolpcema. array(0.27687911, 0.19824952, 0.06910974, 0.7548379, 0.64441325, 0.11951306) Prvi preizkus zapisa Python kode v Jupyter zvezek. Preizkusimo \u0161e uporabo knji\u017enice pandas , ki se uvozi v prvi vrstici kode. Kot \u017ee omenjeno pri prvi predstavitvi knji\u017enice, ta v zaledju uporablja podatke v obliki NumPy polj. To dejstvo lahko izkoristimo pri kreaciji nove strukture podatkov - v pandas -u je to razpredelnica, poimenovana DataFrame . Slede\u010da koda ustvari polje naklju\u010dnih \u0161tevil s 100 vrsticami in dvema stolpcema, ki pa slu\u017ei pri kreaciji razpredelnice DataFrame . V DataFrame razpredelnici lahko stolpce in vrstice poimenujemo, kot je prikazano s podanim parametrom columns . Zadnja vrstica kli\u010de metodo head() na\u0161e instance data_df razreda DataFrame , ki vrne prvih pet vrstic te razpredelnice, kot tudi ka\u017ee slika pod kodo. import numpy as np import pandas as pd data_np = np . random . rand ( 100 , 2 ) data_df = pd . DataFrame ( data_np , columns = [ 'Prvi stolpec' , 'Drugi stolpec' ]) data_df . head () Prvi stolpec Drugi stolpec 0 0.753358 0.029272 1 0.901894 0.846576 2 0.492876 0.533067 3 0.943245 0.538149 4 0.567161 0.537613 Preizkus delovanja knji\u017enice pandas . Sledi preizkus izrisa grafov s knji\u017enicama seaborn in Matplotlib . Slede\u010da koda prikazuje izris grafa raztrosa (angl. scatter plot ) za prej ustvarjeno razpredelnico data_df v obeh knj\u017enicah. seaborn Matplotlib import seaborn as sns sns . scatterplot ( x = data_df [ 'Prvi stolpec' ], y = data_df [ 'Drugi stolpec' ]) Preizkus izrisa grafa raztrosa s knji\u017enico seaborn . import matplotlib.pyplot as plt plt . scatter ( x = data_df [ 'Prvi stolpec' ], y = data_df [ 'Drugi stolpec' ]) Preizkus izrisa grafa raztrosa s knji\u017enico Matplotlib . Na prvi pogled se zdi uporaba seaborn in Matplotlib knji\u017enic podobna, kar tudi v resnici je. Nekatere tehnike grafi\u010dne predstavitve podatkov je s knji\u017enico seaborn la\u017eje narediti, medtem, ko je pri izrisu grafov Matplotlib mnogo bolj prilagodljiv. Sedaj bomo preizkusili \u0161e knji\u017enico scikit-learn , ki vsebuje razli\u010dne pristope, metode ter tehnike strojnega u\u010denja in predprocesiranja podatkov. Preden se lotimo uporabe metod strojnega u\u010denja, bomo tokrat preizkusili nalaganje prilo\u017eenih podatkov v knji\u017enici scikit-learn . Ena izmed prilo\u017eenih je podatkovna zbirka Iris , ki vsebuje podatke o cveto\u010dih rastlinah perunikah. V prvi vrstici se najprej nalo\u017ei modul knji\u017enice scikit-learn , v drugi vrstici pa se nalo\u017ei podatkovna zbirka Iris ter se shrani v spremenljivko iris_vse . Ker smo pri klicu metode podali as_frame=True , bodo rezultati v obliki pandas razpredelnice DataFrame . \u010ce tega ne bi podali, pa bi rezultat podatkov bil v obliki NumPy polja. Shranjeni podatki v spremenljivki iris_vse imajo ve\u010d vrednosti: vrednost data nam vrne podatke o ro\u017eah (vi\u0161ine in \u0161irine \u010da\u0161nih in cvetnih listov). Vrednost target pa nam vrne vrednosti razredov te podatkovne zbirke - za kateri tip perunike gre. Zadnja vrstica kode izpi\u0161e prvih pet vrstic podatkov. from sklearn.datasets import load_iris iris_vse = load_iris ( as_frame = True ) iris_podatki = iris_vse . data iris_razredi = iris_vse . target iris_podatki . head () sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 Za konec preizkusimo z izrisom grafa raztrosa, kjer bodo pike vsake izmed instanc pobarvane glede na podan razred te instance. sns . scatterplot ( x = 'sepal length (cm)' , y = 'sepal width (cm)' , data = iris_podatki , hue = iris_razredi ) Graf raztrosa podatkovne zbirke Iris . Tokrat smo za parameter data podali kar razpredelnico iris_podatki , ki je pandas DataFrame . Parametra x in y smo tokrat zapisali kot niz znakov - imena stolpcev iz podane razpredelnice. S parametrom hue dolo\u010dimo, kako se oznake na grafu pobarvajo. Lahko bi podali tudi ime stolpca (ki pa ga tokrat v razpredelnici data nimamo), ali pa kot lo\u010deno spremenljivko razpredelnice s temi podatki (kot smo naredili tokrat z iris_razredi ). Nalaganje podatkov Da lahko izvedemo analize in uporabimo algoritme strojnega u\u010denja, pa potrebujemo podatke. Slede\u010da sekcija pregleda, kako nalo\u017eimo podatke v na\u0161 Python program oziroma Jupyter zvezek. Nalaganje prostodostopnih podatkov \u017de v prej\u0161nji sekciji smo pokazali, kako lahko dostopamo do standardnih podatkovnih zbirk, ki se mnogokrat uporabljajo v procesu u\u010denja. To smo storili s pomo\u010djo knji\u017enice scikit-learn , saj so preko te knji\u017enice \u017ee name\u0161\u010dene \u0161tevilne podatkovne zbirke. Skozi celotno knjigo bomo najve\u010dkrat uporabili podatkovno zbirko Iris , v nalogah pa bodo uporabljene tudi Wine in Breast cancer . V vseh primerih gre za podatkovne zbirke, namenjene klasifikaciji. Nabor podatkovnih zbirk, ki so prilo\u017eene tej knji\u017enici, je dostopen na slede\u010dem naslovu za enostavne in majhne zbirke ter na tem naslovu za zbirke iz realnega sveta. Podatkovne zbirke iz scikit-learn vrnejo objekt s slede\u010dimi vrednostmi: data je razpredelnica s podatki vseh instanc, target je polje z re\u0161itvami vseh instanc, feature_names je seznam imen vseh atributov, ki so v data ter target_names je seznam imen re\u0161itev (\u010de gre za razrede). \u010ce nam prilo\u017eene prostodostopne podatkovne zbirke niso dovolj, lahko uporabimo poljubno podatkovno zbirko iz portala OpenML.org . Ta portal igra vlogo repozitorija za podatkovne zbirke. Te lahko uporabniki portala prosto nalagajo na portal in tudi do njih dostopajo. Knji\u017enica scikit-learn ponuja vmesnik za neposredno nalaganje teh datotek s pomo\u010djo metode fetch_openml , kateri podamo ime zbirke, kot ka\u017ee spodnja koda. from sklearn.datasets import fetch_openml podatki = fetch_openml ( name = 'ecoli' ) print ( podatki . feature_names ) ['mcg', 'gvh', 'lip', 'chg', 'aac', 'alm1', 'alm2'] Tudi pri nalaganju podatkovnih zbirk iz OpenML.org lahko fetch_openml metodi povemo, da \u017eelimo razpredelnico DataFrame . To storimo enako kot pri nalaganju prilo\u017eenih podatkovnih zbirk, s podajo as_frame=True . from sklearn.datasets import fetch_openml podatki = fetch_openml ( name = 'nursery' , as_frame = True ) print ( podatki . target . head ()) 0 recommend 1 priority 2 not_recom 3 recommend4 priority Nalaganje lastnih podatkov Podatke lahko v program oziroma zvezek nalo\u017eimo s pomo\u010djo knji\u017enice pandas , ki prepozna podatke v \u0161tevilnih razli\u010dnih formatih : tekstovne datoteke, kjer so podatki lo\u010deni z znaki (npr. z vejico v CSV, tabulatorjem ali drugim znakom), JSON format tekstovne datoteke, HTML spletne strani, iz katerih se podatki preberejo iz elementa <table> , Excel ali OpenDocument razpredelnice, Lastni\u0161ke SAS ali SPSS datoteke podatkov, podatkovne baze s SQL klicem ter lokalne odlo\u017ei\u0161\u010da (angl. clipboard ). Primer nalaganja iz CSV datoteke, kjer so vrednosti lo\u010dene s podpi\u010djem ; prikazuje spodnja koda. Vsak format ima svojo metodo za prebiranje datotek, saj so dolo\u010dene nastavitve formatu specifi\u010dne. Pri prebiranju iz tekstovnih datotek tako lahko podamo znak, ki lo\u010duje vrednosti s sep , znak lo\u010devanja decimalnih mest s dec , vrstico, ki predstavlja imena atributov, s header ali pa podamo ta imena kar v names . import pandas as pd podatki = pd . read_csv ( 'datoteka.txt' , sep = ';' , dec = ',' ) S pomo\u010djo tega je po prebiranju knjige mogo\u010de osvojeno znanje aplicirati na lastne podatke.","title":"Vzpostavitev okolja"},{"location":"pages/knjiga/02_okolje/#vzpostavitev-okolja","text":"Za namen prikaza prakti\u010dnih primerov bo v tej knjigi uporabljen programski jezik Python, saj je en izmed bolj priljubljenih programskih jezikov z odli\u010dnim naborom knji\u017enic strojnega u\u010denja. Za razumevanje primerov je vsekakor priporo\u010dljivo poznavanje tega programskega jezika ali vsaj splo\u0161no poznavanje enega izmed modernih programskih jezikov (Java, Perl, C#, R, C, C++). Sledijo navodila vzpostavitve okolja, primernega za izvedbo prakti\u010dnih primerov na svojem ra\u010dunalniku.","title":"Vzpostavitev okolja"},{"location":"pages/knjiga/02_okolje/#razvojno-okolje-jupyter-notebook","text":"Vsi prakti\u010dni primeri v knjigi so prikazani tako, da se brez te\u017eave izvedejo v JupyterLab okolju. To razvojno okolje omogo\u010da pisanje Python programske kode v tako imenovanih Jupyter zvezkih (angl. Jupyter notebooks ). Ti zvezki se urejajo v poljubnem spletnem brskalniku in zato za urejanje ne potrebujejo namenskega orodja. Posebnost kode, zapisane v Jupyter zvezkih, je, da je ta v eni datoteki (zvezku) me\u0161ana s takoj\u0161njimi izpisi kode in navodili, ki jih razvijalci zapi\u0161emo sproti za razlago delovanja kode. Zvezki so razdeljeni na tako imenovane celice, kjer je celica bodisi navodilo ali pa programska koda z izpisom rezultata, kot je prikazano na spodnji sliki. Jupyter zvezek na spletni storitvi Google Colab. Namen takega razvoja je, da je programska koda z navodili enostavno deljiva in razumljiva. Prav tako razvoj v zvezku omogo\u010da inkrementalno zaganjanje kode, celico za celico - tako se za\u017eenejo le \u017eeleni deli kode in ne celotna datoteka. To je idealen na\u010din razvoja za u\u010denje in za raziskovanje. Jupyter zvezki so postali privzeti na\u010din razvoja na podro\u010dju podatkovne znanosti, saj zadostijo primarnemu namenu podatkovne znanosti - odkrivanju vzorcev in pregledu podatkov skozi zgodbo. Tako je vsaka celica s kodo in prilo\u017eeno celico navodil kar en del zgodbe, kjer celica z navodili opi\u0161e, kaj bo celica s kodo naredila ter povzame njene rezultate. Alternativnih razvojnih okolij je mnogo. Od popolnoma namenskih za programiranje v Pythonu, kot sta PyCharm in Spyder , pa do splo\u0161nih razvojnih okolij, kot sta Visual Studio Code in Atom .","title":"Razvojno okolje Jupyter Notebook"},{"location":"pages/knjiga/02_okolje/#uporaba-oblacnega-okolja","text":"Prvi na\u010din uporabe Jupyter zvezkov za namen strojnega u\u010denja je uporaba ene izmed ponujenih storitev. V ta namen so na voljo \u0161tevilni ponudniki z bodisi zastonjskimi ali pla\u010dljivimi storitvami izvajanja Python kode v oblaku. Dober primer take storitve je Google Colab , ki vsem svojim uporabnikom ponuja kreacijo Jupyter zvezkov in zagon teh na Googlovih stre\u017enikih, do dolo\u010dene mere zastonj - v \u010dasu pisanja knjige je uporaba Google Colab zvezkov za namen zagona primerov in nalog bila brezpla\u010dna. Google Colab je prikazan prav na zgornji sliki. Konkurence na podro\u010dju obla\u010dnih zvezkov je ogromno in uporabnost teh se mese\u010dno spreminja ter je vezana na ceno storitve, dostopnost slovenskim razvijalcem in nabor funkcionalnosti. Preprosto spletno iskanje \"Jupyter Notebook service\"~ali \"Google Colab alternative\"~vrne \u0161tevilne rezultate. V \u010dasu pisanja knjige so med omembe vrednimi bile storitve drugih gigantov strojnega u\u010denja Microsoft Azure Machine Learning in Amazon SageMaker ter s strojno opremo radodarna Paperspace in Deepnote .","title":"Uporaba obla\u010dnega okolja"},{"location":"pages/knjiga/02_okolje/#namestitev-in-uporaba-lokalnega-okolja-anaconda","text":"V tej sekciji bo predstavljeno, kako vzpostavimo primerno okolje na svojem ra\u010dunalniku. Da namestimo Python in vse potrebne knji\u017enice, se bomo zatekli k okolju Anaconda, ki vsebuje tako Python kot skupek \u0161tevilnih knji\u017enic, ki se uporabljajo za namen analize podatkov. Na uradni spletni strani okolja Anaconda poi\u0161\u010demo predel za prenos razli\u010dice, namenjene za posameznike. V \u010dasu pisanja knjige se ta razli\u010dica imenuje Individual Edition in je dosegljiva na uradni spletni strani ter prikazana spodaj. Spletna stran Python okolja Anaconda. Okolje Anaconda poleg programskega jezika Python namesti tudi \u0161tevilne knji\u017enice, namenjene za podatkovno analizo v tem programskem jeziku. Skozi knjigo bomo uporabili \u0161tevilne izmed teh: NumPy , ki se uporablja za upravljanje s podatki v matri\u010dnih in ve\u010d-dimenzionalnih poljih. Podrobnej\u0161ega dela s to knji\u017enico ne bo, je pa vseeno dobro, da se zavedamo, da je to ena izmed klju\u010dnih knji\u017enic, ko imamo opravek s podatki in analizo teh. Ta knji\u017enica je tudi sestavni del slede\u010de knji\u017enice. pandas je knji\u017enica, ki je namenjena za manipulacijo in analizo podatkov v tabelari\u010dni obliki. \u010ceprav v zaledju uporablja knji\u017enico NumPy , so dolo\u010dene operacije nad podatki poenostavljene. scikit-learn je knji\u017enica, ki vsebuje implementacije razli\u010dnih algoritmov, metrik in pristopov pred-procesiranja za strojno u\u010denje. Algoritem klasifikacije, ki bo predstavljen v tej knjigi, je povzet po implementaciji iz te knji\u017enice. Matplotlib , ki se uporablja za prikaz grafov razli\u010dnih vrst. Je zelo prilagodljiva, saj omogo\u010da velik nabor razli\u010dnih tipov vizualizacij podatkov in prilagoditve teh (tako stilno, kot vsebinsko). Je pa zaradi svoje prilagodljivost delo s to knji\u017enico nekoliko te\u017eje in zaradi tega uporabljamo ... seaborn , ki poenostavi prikaz grafov. V zaledju uporablja knji\u017enico Matplotlib , ampak preko svojih vmesnikov dolo\u010dene pogoste operacije pri risanju grafov poenostavi.","title":"Namestitev in uporaba lokalnega okolja Anaconda"},{"location":"pages/knjiga/02_okolje/#uporaba-jupyter-zvezkov","text":"Za pisanje Jupyter zvezkov na lastnem ra\u010dunalniku je najprej potreben zagon okolja JupyterLab, za kar lahko uporabimo enega izmed dveh na\u010dinov. Prvi na\u010din je zagon JupyterLaba ro\u010dno iz komandne vrstice: 1. Za\u017eenemo komandno vrstico. V Windowsih sta to bodisi Command Prompt ali PowerShell. V Linux in MacOS operacijskih sistemih pa je komanda vrstica najve\u010dkrat pod imenom Terminal. 2. Za\u017eenemo ukaz jupyter lab . 3. Odpre se okno brskalnika in poka\u017ee se doma\u010da stran JupyterLab, kot je prikazano na spodnji sliki. Drugi na\u010din je preko uporabni\u0161kega vmesnika okolja Anaconda. Za\u017eenemo Anaconda Navigator. V seznamu ponujenih aplikacij najdimo JupyterLab in ga s pritiskom mo\u017enosti Launch za\u017eenemo. Odpre se okno brskalnika in poka\u017ee se doma\u010da stran JupyterLab, kot je prikazano na slede\u010di sliki. Doma\u010da stran JupyterLab okolja. Po \u017eelji naredimo novo mapo, kamor shranimo na\u0161e zvezke in druge podporne datoteke. V tej mapi ustvarimo novi zvezek, kot je prikazano slede\u010de. Kreacija novega zvezka v JupyterLab okolju. Odpre se novo okno s praznim zvezkom, kot je prikazano spodaj. Jupyter zvezke razvijamo preko spletnega vmesnika, kar je tudi razlog za prikaz spletnega brskalnika. Nov prazen Jupyter zvezek. Zgornja slika ima ozna\u010dene pomembne dele uporabni\u0161kega vmesnika. Oznaka 1 prikazuje naslov zvezka, kar s klikom na napis lahko spremenimo. Oznaka 2 prikazuje tip celice, ki je trenutno izbrana, oznaka 3 pa prikazuje trenutno izbrano in hkrati zaenkrat \u0161e edino celico.","title":"Uporaba Jupyter zvezkov"},{"location":"pages/knjiga/02_okolje/#markdown-v-jupyter-zvezkih","text":"Za za\u010detek spremenimo tip prve celice na Markdown , ki je stil zapisa navodil in druge vsebine v celico. V celico zapi\u0161emo slede\u010d Markdown zapis. # Prvi primer V __prvem__ primeru bomo: - Naredili prvi izpis `Python` kode. - Naredili prvi graf s pomo\u010djo _seaborn_ knji\u017enice. S pritiskom na gumb zagona, ki je na prej\u0161nji sliki 4 (ali s pritiskom Shift + Enter ), se izbrana celica izvede in izbere se (ter \u010de ne obstaja se tudi ustvari) naslednja celica. Ker je izbrana celica bila tipa Markdown, se je zapisana koda oblikovala po stilu Markdown zapisa. Osnovni ukazi za zapis Markdowna so slede\u010di. Z znakom #, \u010demur sledi presledek definiramo naslov. # za glavne naslove, ## za podnaslove, ### za tretji nivo naslovov, pa vse do \u0161estega nivoja naslovov z ###### . Besedilo lahko tudi poudarimo. Po\u0161evno zapisano besedilo: _besedilo_ ali *besedilo* ter krepko _ zapisano besedilo: __besedilo__ ali **besedilo** . Dodamo lahko tudi neurejen seznam, tako da vsak element seznama zapi\u0161emo v svoji vrstici, za\u010dnemo pa ga bodisi z znakom * ali z - . Sezname lahko gnezdimo - ugnezdene elemente zamaknemo s tabulatorjem. - Prvi element. * Drugi element. - Prvi ugnezden element. - Pa \u0161e drugi. Urejene sezname, kjer si elementi sledijo v vrstnem redu, pa ustvarimo tako, da pred elemente dodamo \u0161tevilko ali oznako vrstnega reda. Tudi take sezname lahko gnezdimo. \u0160tevilko vrstnega reda lahko podamo poljubno, saj ni potrebno, da so te v pravem vrstnem redu. 1. Prvi element. 2. Drugi element. 11. Prvi ugnezden element. 12. Pa \u0161e drugi. Besedilo v stilu kode pa v besedilo zapi\u0161emo med znakoma ` , ki ga na slovenski tipkovnici izberemo z AltGr + 7 . Spremenljivka `vrednost` in razred `podatki` . Tabelo vstavimo vrstico po vrstico, vsaka celica v vrstici se za\u010dne in kon\u010da z znakom | (na slovenski tipkovnici AltGr + W ). Tako je ena vrstica s tremi celicami zapisana na slede\u010d na\u010din. |Prva celica|Druga celica|Tretja celica| \u010crte med vrstice pa dodamo kar z znakom - namesto vsebine celice. Primer tabele s tremi stolpci in tremi vrsticami ter \u010drto med prvo in drugo vrstico izgleda slede\u010de. |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |-|-|-| |Druga vrstica|12|-42| |Tretja vrstica|-7|65| Za la\u017ejo razumevanje kode lahko to nekoliko oblikujemo s presledki. |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |----------------------------|-------------|--------------| |Druga vrstica |12 |-42 | |Tretja vrstica |-7 |65 | Hiperpovezave vstavljamo s slede\u010do kodo [Napis povezave](URL) , slike pa z zapisom ![Naziv slike](URL do datoteke slike) - razlika je v klicaju. [ Povezava do iskalnika Google ]( http://www.google.com/ ) ![ Primer slike ]( /imgs/slika.png ) \u010ce ne gre druga\u010de, pa lahko vsebino celic navodil oblikujemo tudi s preprosto HTML kodo. HTML in Markdown zapis lahko po \u017eelji me\u0161amo. Kon\u010den primer vsega skupaj bi zapisali s slede\u010do kodo, rezultat pa je prikazan na sliki pod kodo. ## Preizkus Markdown zapisa 1. Preizkusili smo oblikovanje pisav 1. __krepko__ in 2. _po\u0161evno_ pisavo 2. Pisanje seznamov - urejenih in neurejenih, - ter ugnezdenih 3. Dodajanje [ povezava ]( http://www.um.si/ ) 4. Dodajanje slik ![ FERI ]( https://feri.um.si/site/images/logo-feri.png ) 5. Pisanje v stilu `programske kode` 6. Dodajanje tabel |Prva vrstica in prvi stolpec|Drugi stolpec|Tretji stolpec| |----------------------------|-------------|--------------| |Druga vrstica |12 |-42 | |Tretja vrstica |-7 |65 | 7. Tudi <a href=\"https://feri.um.si/\">HTML koda</a> deluje. Rezultat Markdown zapisa.","title":"Markdown v Jupyter zvezkih"},{"location":"pages/knjiga/02_okolje/#python-koda-v-jupyter-zvezkih","text":"\u010ce ima celica vsebino tipa Code , pa se vsebina smatra kot Python koda - primerno je tudi obarvana, deluje avtomatsko dopolnjevanje kode (angl. code auto-completion ) in rezultati kode se izpi\u0161ejo kar takoj pod celico. Sledi primer zapisa programske kode v celico. V prvi vrstici se uvozi knji\u017enica NumPy ter se poimenuje kot np za nadaljnjo uporabo. Sledi izpis niza znakov z ukazom print() - izpis sledi kar pod celico, kot je prikazano na slede\u010di sliki. Tretja vrstica vsebuje komentar v kodi, ki opisuje zadnjo vrstico. Zadnja vrstica pa vsebuje klic metode rand() knji\u017enice NumPy , ki vrne polje naklju\u010dnih \u0161tevil po podanih velikostih (v primeru je podana velikost polja s tremi vrsticami in dvema stolpcema). Ker ta vrstica vsebuje tudi zadnji ukaz, ki vra\u010da rezultat, se rezultat tega izpi\u0161e tudi pod celico. import numpy as np print ( 'Sledi izpis polja naklju\u010dnih \u0161tevil s tremi vrsticami in dvema stolpcema.' ) # Rezultat zadnjega ukaza v celici se izpi\u0161e kot rezultat celice. np . random . rand ( 3 , 2 ) Sledi izpis polja naklju\u010dnih \u0161tevil s tremi vrsticami in dvema stolpcema. array(0.27687911, 0.19824952, 0.06910974, 0.7548379, 0.64441325, 0.11951306) Prvi preizkus zapisa Python kode v Jupyter zvezek. Preizkusimo \u0161e uporabo knji\u017enice pandas , ki se uvozi v prvi vrstici kode. Kot \u017ee omenjeno pri prvi predstavitvi knji\u017enice, ta v zaledju uporablja podatke v obliki NumPy polj. To dejstvo lahko izkoristimo pri kreaciji nove strukture podatkov - v pandas -u je to razpredelnica, poimenovana DataFrame . Slede\u010da koda ustvari polje naklju\u010dnih \u0161tevil s 100 vrsticami in dvema stolpcema, ki pa slu\u017ei pri kreaciji razpredelnice DataFrame . V DataFrame razpredelnici lahko stolpce in vrstice poimenujemo, kot je prikazano s podanim parametrom columns . Zadnja vrstica kli\u010de metodo head() na\u0161e instance data_df razreda DataFrame , ki vrne prvih pet vrstic te razpredelnice, kot tudi ka\u017ee slika pod kodo. import numpy as np import pandas as pd data_np = np . random . rand ( 100 , 2 ) data_df = pd . DataFrame ( data_np , columns = [ 'Prvi stolpec' , 'Drugi stolpec' ]) data_df . head () Prvi stolpec Drugi stolpec 0 0.753358 0.029272 1 0.901894 0.846576 2 0.492876 0.533067 3 0.943245 0.538149 4 0.567161 0.537613 Preizkus delovanja knji\u017enice pandas . Sledi preizkus izrisa grafov s knji\u017enicama seaborn in Matplotlib . Slede\u010da koda prikazuje izris grafa raztrosa (angl. scatter plot ) za prej ustvarjeno razpredelnico data_df v obeh knj\u017enicah. seaborn Matplotlib import seaborn as sns sns . scatterplot ( x = data_df [ 'Prvi stolpec' ], y = data_df [ 'Drugi stolpec' ]) Preizkus izrisa grafa raztrosa s knji\u017enico seaborn . import matplotlib.pyplot as plt plt . scatter ( x = data_df [ 'Prvi stolpec' ], y = data_df [ 'Drugi stolpec' ]) Preizkus izrisa grafa raztrosa s knji\u017enico Matplotlib . Na prvi pogled se zdi uporaba seaborn in Matplotlib knji\u017enic podobna, kar tudi v resnici je. Nekatere tehnike grafi\u010dne predstavitve podatkov je s knji\u017enico seaborn la\u017eje narediti, medtem, ko je pri izrisu grafov Matplotlib mnogo bolj prilagodljiv. Sedaj bomo preizkusili \u0161e knji\u017enico scikit-learn , ki vsebuje razli\u010dne pristope, metode ter tehnike strojnega u\u010denja in predprocesiranja podatkov. Preden se lotimo uporabe metod strojnega u\u010denja, bomo tokrat preizkusili nalaganje prilo\u017eenih podatkov v knji\u017enici scikit-learn . Ena izmed prilo\u017eenih je podatkovna zbirka Iris , ki vsebuje podatke o cveto\u010dih rastlinah perunikah. V prvi vrstici se najprej nalo\u017ei modul knji\u017enice scikit-learn , v drugi vrstici pa se nalo\u017ei podatkovna zbirka Iris ter se shrani v spremenljivko iris_vse . Ker smo pri klicu metode podali as_frame=True , bodo rezultati v obliki pandas razpredelnice DataFrame . \u010ce tega ne bi podali, pa bi rezultat podatkov bil v obliki NumPy polja. Shranjeni podatki v spremenljivki iris_vse imajo ve\u010d vrednosti: vrednost data nam vrne podatke o ro\u017eah (vi\u0161ine in \u0161irine \u010da\u0161nih in cvetnih listov). Vrednost target pa nam vrne vrednosti razredov te podatkovne zbirke - za kateri tip perunike gre. Zadnja vrstica kode izpi\u0161e prvih pet vrstic podatkov. from sklearn.datasets import load_iris iris_vse = load_iris ( as_frame = True ) iris_podatki = iris_vse . data iris_razredi = iris_vse . target iris_podatki . head () sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 Za konec preizkusimo z izrisom grafa raztrosa, kjer bodo pike vsake izmed instanc pobarvane glede na podan razred te instance. sns . scatterplot ( x = 'sepal length (cm)' , y = 'sepal width (cm)' , data = iris_podatki , hue = iris_razredi ) Graf raztrosa podatkovne zbirke Iris . Tokrat smo za parameter data podali kar razpredelnico iris_podatki , ki je pandas DataFrame . Parametra x in y smo tokrat zapisali kot niz znakov - imena stolpcev iz podane razpredelnice. S parametrom hue dolo\u010dimo, kako se oznake na grafu pobarvajo. Lahko bi podali tudi ime stolpca (ki pa ga tokrat v razpredelnici data nimamo), ali pa kot lo\u010deno spremenljivko razpredelnice s temi podatki (kot smo naredili tokrat z iris_razredi ).","title":"Python koda v Jupyter zvezkih"},{"location":"pages/knjiga/02_okolje/#nalaganje-podatkov","text":"Da lahko izvedemo analize in uporabimo algoritme strojnega u\u010denja, pa potrebujemo podatke. Slede\u010da sekcija pregleda, kako nalo\u017eimo podatke v na\u0161 Python program oziroma Jupyter zvezek.","title":"Nalaganje podatkov"},{"location":"pages/knjiga/02_okolje/#nalaganje-prostodostopnih-podatkov","text":"\u017de v prej\u0161nji sekciji smo pokazali, kako lahko dostopamo do standardnih podatkovnih zbirk, ki se mnogokrat uporabljajo v procesu u\u010denja. To smo storili s pomo\u010djo knji\u017enice scikit-learn , saj so preko te knji\u017enice \u017ee name\u0161\u010dene \u0161tevilne podatkovne zbirke. Skozi celotno knjigo bomo najve\u010dkrat uporabili podatkovno zbirko Iris , v nalogah pa bodo uporabljene tudi Wine in Breast cancer . V vseh primerih gre za podatkovne zbirke, namenjene klasifikaciji. Nabor podatkovnih zbirk, ki so prilo\u017eene tej knji\u017enici, je dostopen na slede\u010dem naslovu za enostavne in majhne zbirke ter na tem naslovu za zbirke iz realnega sveta. Podatkovne zbirke iz scikit-learn vrnejo objekt s slede\u010dimi vrednostmi: data je razpredelnica s podatki vseh instanc, target je polje z re\u0161itvami vseh instanc, feature_names je seznam imen vseh atributov, ki so v data ter target_names je seznam imen re\u0161itev (\u010de gre za razrede). \u010ce nam prilo\u017eene prostodostopne podatkovne zbirke niso dovolj, lahko uporabimo poljubno podatkovno zbirko iz portala OpenML.org . Ta portal igra vlogo repozitorija za podatkovne zbirke. Te lahko uporabniki portala prosto nalagajo na portal in tudi do njih dostopajo. Knji\u017enica scikit-learn ponuja vmesnik za neposredno nalaganje teh datotek s pomo\u010djo metode fetch_openml , kateri podamo ime zbirke, kot ka\u017ee spodnja koda. from sklearn.datasets import fetch_openml podatki = fetch_openml ( name = 'ecoli' ) print ( podatki . feature_names ) ['mcg', 'gvh', 'lip', 'chg', 'aac', 'alm1', 'alm2'] Tudi pri nalaganju podatkovnih zbirk iz OpenML.org lahko fetch_openml metodi povemo, da \u017eelimo razpredelnico DataFrame . To storimo enako kot pri nalaganju prilo\u017eenih podatkovnih zbirk, s podajo as_frame=True . from sklearn.datasets import fetch_openml podatki = fetch_openml ( name = 'nursery' , as_frame = True ) print ( podatki . target . head ()) 0 recommend 1 priority 2 not_recom 3 recommend4 priority","title":"Nalaganje prostodostopnih podatkov"},{"location":"pages/knjiga/02_okolje/#nalaganje-lastnih-podatkov","text":"Podatke lahko v program oziroma zvezek nalo\u017eimo s pomo\u010djo knji\u017enice pandas , ki prepozna podatke v \u0161tevilnih razli\u010dnih formatih : tekstovne datoteke, kjer so podatki lo\u010deni z znaki (npr. z vejico v CSV, tabulatorjem ali drugim znakom), JSON format tekstovne datoteke, HTML spletne strani, iz katerih se podatki preberejo iz elementa <table> , Excel ali OpenDocument razpredelnice, Lastni\u0161ke SAS ali SPSS datoteke podatkov, podatkovne baze s SQL klicem ter lokalne odlo\u017ei\u0161\u010da (angl. clipboard ). Primer nalaganja iz CSV datoteke, kjer so vrednosti lo\u010dene s podpi\u010djem ; prikazuje spodnja koda. Vsak format ima svojo metodo za prebiranje datotek, saj so dolo\u010dene nastavitve formatu specifi\u010dne. Pri prebiranju iz tekstovnih datotek tako lahko podamo znak, ki lo\u010duje vrednosti s sep , znak lo\u010devanja decimalnih mest s dec , vrstico, ki predstavlja imena atributov, s header ali pa podamo ta imena kar v names . import pandas as pd podatki = pd . read_csv ( 'datoteka.txt' , sep = ';' , dec = ',' ) S pomo\u010djo tega je po prebiranju knjige mogo\u010de osvojeno znanje aplicirati na lastne podatke.","title":"Nalaganje lastnih podatkov"},{"location":"pages/knjiga/03_strojno_ucenje/","text":"Strojno u\u010denje Kam gremo po nasvet, ko se po\u010dutimo bolni? Se zate\u010demo k sosedu, sodelavki, prijatelju ali pa raje obi\u0161\u010demo zdravnika? Vsekakor je najbolje, da obi\u0161\u010demo zdravnika. Premislimo, zakaj. Kaj je prednost zdravnika v primerjavi z ostalimi na\u0161tetimi v danem problemu? Zdravnik ima najverjetneje mnogo ve\u010d znanja in izku\u0161enj pri diagnozi bolezni, saj se je kot \u0161tudent ve\u010d let u\u010dil prav o tej tematiki in v svoji delovni karieri \u017ee pridobil mnogotero izku\u0161enj. Tekom tega procesa pridobivanja znanja in izku\u0161enj tako predpostavimo, da se je zdravnik \u017ee usposobil za kakovostno spopadanje z danim problemom diagnoze bolezni. Model znanja Naslednji\u010d, ko sre\u010date zdravnika, ga kar povpra\u0161ajte o tem, kako se spopade z izzivom diagnoze pacienta na podlagi simptomov obolenj. Verjetno bo za\u010del s pregledom trenutnega in preteklega stanja pacienta. Naredil bo razli\u010dne meritve pacientovega telesa (telesna temperatura, krvni pritisk, hormonska slika, krvna slika, rentgen ...) in dodatno pregledal pacientovo kartoteko. Ob preu\u010devanju novih in preteklih meritev pacienta bo zdravnik najverjetneje \u017ee imel ustaljen postopek razmi\u0161ljanja pri postavitvi diagnoze. Recimo, da \u017eelimo analizirati delo na\u0161ega zdravnika in ga prosimo, da na kar se da razumljiv na\u010din opi\u0161e svoj proces odlo\u010danja. Najverjetneje zdravnik (ali strokovnjak drugih podro\u010dij) nima formalno dolo\u010denega procesa odlo\u010danja, ampak se pri tem zateka tudi k svoji intuiciji, ki jo je razvil po vseh letih \u0161olanja in izku\u0161enj. Pa vendar, \u010de bi se zdravnik odlo\u010dil zapisati svoj proces odlo\u010danja na papir, bi ta verjetno izgledal nekako tako: \u010ce je telesna temperature pacienta nad 40 \u00b0C, je pacient bolan . \u010ce je telesna temperature nad 38 \u00b0C in je sistoli\u010dni krvni pritisk nad 140 mHg, je pacient bolan . \u010ce je telesna temperature pod 38 \u00b0C, je pacient zdrav . Svoj proces diagnoze bi zdravnik zapisal v obliki pravil, katerim bi sledil bodisi po vrsti ali pa bi jih upo\u0161teval vse hkrati. Tem pravilom pravimo model znanja (angl. knowledge model ). Kateri drug zdravnik pa bi svoj proces odlo\u010danja mogo\u010de la\u017eje zapisal v obliki odlo\u010ditvenega drevesa. Model znanja v obliki odlo\u010ditvenega drevesa. Pri interpretaciji takega drevesa bi zdravnik za\u010del na vrhu (korenu) drevesa in se z odgovarjanjem na vpra\u0161anja v posameznem vozli\u0161\u010du drevesa pri pregledu posameznega pacienta premikal vse do listov drevesa. Listi pa mu povedo odgovor glede diagnoze pacienta. Nek tretji zdravnik pa bolj\u0161e razmi\u0161lja, ko svoj model znanja predstavi v obliki matemati\u010dnih formul. Slede\u010da slika prikazuje graf, kjer je na horizontalni X osi telesna temperatura pacienta, na vertikalni Y osi pa je krvni pritisk pacienta. Matemati\u010dna formula deli prostor na dva dela - na prostor, kjer se nahajajo zdravi in na prostor, kjer se nahajajo bolni pacienti. Model znanja v obliki matemati\u010dne funkcije. Zadnji zdravnik pa bi deloval popolnoma druga\u010de kot prej\u0161nji trije. Ta ne bi znal na papir zapisati svojega modela znanja, saj se odlo\u010da po druga\u010dnem postopku - pregleda svoje prej\u0161nje paciente iz bogate dvajsetletne kariere in novega pacienta primerja z njimi. Ko najde primere \u017ee diagnosticiranih pacientov, ki so novemu pacientu najbolj podobni, preprosto pogleda, kak\u0161na je bila njihova diagnoza in temu novemu poda mnenje o njegovem stanju bolezni, ki je skladna s prej\u0161njimi (podobnimi) pacienti. Ta proces je prikazan slede\u010de. Model znanja v obliki sprotne primerjave z \u017ee re\u0161enimi primeri. Vsi \u0161tirje opisani na\u010dini odlo\u010danja zdravnikov v resnici prikazujejo razli\u010dne na\u010dine predstavitve modela znanja. Teh je v realnosti \u0161e mnogo ve\u010d, tekom te knjige pa bomo podrobneje spoznali zadnji na\u010din zapisa modela znanja - primerjavo novih primerov s starimi, \u017ee re\u0161enimi. Ekspertni sistem Gradnjo modela znanja zdravnika vizualno prikazuje spodnja slika. Najverjetneje preberejo zdravniki tekom \u0161tudija medicine in delovne kariere mnogo knjig, znanstvenih in strokovnih \u010dlankov ter pridobijo mnogotere izku\u0161nje. To ka\u017ee zgornji del slike. Rezultat takega procesa je model znanja (seveda ne formalno zapisan). Ob pregledu novega pacienta uporabijo ta model znanja, da pridejo do kon\u010dne diagnoze pacienta, kar je ponazorjeno s spodnjim delom slike. Poenostavljen prikaz procesa gradnje in uporabe modela znanja. Seveda lahko zdravnikov model znanja prepi\u0161emo v programsko kodo in tako uporabimo njegov model znanja v vsakdanjih informacijskih sistemih, kot ka\u017ee slede\u010da slika. To je standardni postopek pri gradnji ekspertnih sistemov (angl. expert systems ) tj. informacijskih sistemov, ki uporabijo model znanja, ki so jih zgradili strokovnjaki (na primer zdravniki). Z digitalizacijo modela znanja pridobimo zmo\u017enost hitrej\u0161e uporabe modela znanja in posledi\u010dnega odlo\u010danja, saj ra\u010dunalnik lahko tak model znanja uporabi tudi milijonkrat v sekundi. Proces uporabe modela znanja v ekspertnem sistemu. Pri ekspertnem sistemu \u0161e vedno ne govorimo o umetni inteligenci ali strojnem u\u010denju, saj ra\u010dunalnik ni sodeloval pri procesu u\u010denja (gradnje modela znanja), ampak je le neumen stroj, ki sledi modelu znanja, zapisanem v obliki if in for stavkov. Inteligentni sistem Premislimo, kako bi lahko v prej\u0161njem procesu vklju\u010dili ra\u010dunalnik v proces gradnje modela znanja. Namesto da \u010dlovek postane strokovnjak s pomo\u010djo preu\u010devanja literature in nabiranja izku\u0161enj, bomo tokrat uporabili kar ra\u010dunalnik za gradnjo modela. Ampak kako? Ra\u010dunalnik ne razume prebranih knjig in ne more pridobivati izku\u0161enj. Tukaj pa uporabimo enak pristop u\u010denja, kot ga uporabijo ljudje v situacijah, kjer se u\u010dijo s pomo\u010djo opazovanja - pri delu opazujejo strokovnjake in sku\u0161ajo ugotoviti, kako strokovnjaki delajo, da lahko to kasneje posnemajo. Podobno pot uberemo, ko \u017eelimo pri gradnji modela uporabiti ra\u010dunalnik kar ka\u017ee slika spodaj. Namesto iz knjig in izku\u0161enj, ra\u010dunalnik razbere vzorce iz dela strokovnjakov, ki pa je v tem primeru sestavljeno iz \u017ee re\u0161enih problemov. \u010ce ra\u010dunalniku podamo kartoteke \u017ee diagnosticiranih pacientov, bo ta lahko iz teh kartotek razbral vzorce, ki so zna\u010dilni za bolne paciente in vzorce, ki so zna\u010dilni za zdrave paciente. Seveda, enako kot vajenec ne bo postal mojster pri enournem opazovanju strokovnjaka pri delu, tako tudi ra\u010dunalnik ne uspe najti vzorcev iz le nekaj primerov \u017ee diagnosticiranih pacientov. Koliko podatkov pa bo dovolj? \u010ce je delo mojstra zelo zapleteno, bo vajenec potreboval nekaj let opazovanja, da bo ugotovil pravi na\u010din dela tega strokovnjaka. \u010ce pa vajencu ka\u017eemo, kako nalepiti obli\u017e na rano, pa bo vajenec vzorec za reprodukcijo videnega odkril \u017ee po nekaj minutah. Podobno je tudi pri ra\u010dunalniku. Koli\u010dina potrebnih re\u0161enih primerov je odvisna od koli\u010dine in kompleksnosti vzorcev - ve\u010d kot je vzorcev in bolj so ti kompleksni, ve\u010d podatkov bo ra\u010dunalnik potreboval, da bo vzorce prepoznal. Ra\u010dunalni\u0161ki algoritem, ki iz podatkov razbira vzorce, imenujemo algoritem strojnega u\u010denja (angl. machine learning algorithm ). Ta algoritem je zadol\u017een, da namesto \u010dloveka ustvari model znanja, ki ga kasneje lahko pri svojem delu uporabita tako \u010dlovek, kakor tudi ra\u010dunalnik. Sistem, ki vklju\u010duje algoritem strojnega u\u010denja za gradnjo modela znanja, pa imenujemo inteligentni sistem (angl. _intelligent system). Proces u\u010denja in uporabe modela znanja v inteligentnem sistemu. Podatki Podatki, uporabljeni v algoritmu strojnega u\u010denja za namen kreacije modela znanja, so zdru\u017eeni v podatkovne mno\u017eice (angl. datasets ) in so lahko v obliki preproste strukturirane tekstovne datoteke ali podatkovne baze poljubne strukture (relacijske, objektne ali dokumentne podatkovne baze). Najenostavnej\u0161a struktura podatkov, namenjenih za strojno u\u010denje ima obliko, ki je prikazana v spodnji tabeli. Vsako vrstico v tekstovnem dokumentu ali vsak primerek podatkovne baze imenujemo u\u010dna instanca ali u\u010dni primerek (angl. learning example ali learning instance ). Vsaka instanca je opisana z mno\u017eico karakteristik imenovanih atributi (angl. features } ali tudi neodvisne spremenljivke oziroma zna\u010dilnice . Prostor atributov (angl. feature space } \\(F\\) je vektor vseh atributov. Vi\u0161ina Te\u017ea Starost \u2026 181 92 45 178 71 27 168 73 65 \u2026 Strojno u\u010denje Strojno u\u010denje (angl. machine learning } zajema tehnike, kjer se ra\u010dunalnik nau\u010di re\u0161evanja specifi\u010dnih in ozko usmerjenih nalog iz podatkov - pravimo, da odlo\u010ditve strojnega u\u010denja temeljijo na podatkih. Tehnike strojnega u\u010denja uvr\u0161\u010damo v krovno podro\u010dje umetne inteligence (angl. artificial intelligence ), ki pa pokriva mnogo \u0161ir\u0161e raziskovalno podro\u010dje. Podro\u010dje, povezano s strojnim u\u010denjem, je tako imenovano podatkovno rudarjenje (angl. data mining ), kjer s pomo\u010djo razli\u010dnih tehnik, med drugimi tudi s strojnim u\u010denjem, obdelujemo in preu\u010dujemo podatke ter posku\u0161amo iz njih razbrati vzorce in posledi\u010dno novo znanje. Izraz podatkovno rudarjenje uporabljamo, ko se sre\u010damo s problemom uporabe samih metod strojnega u\u010denja kot orodij za re\u0161evanje drugih problemov in ne s samo implementacijo teh. Strojno u\u010denje delimo na \u0161tiri podro\u010dja glede na stopnjo nadzora nad u\u010denjem 1 : Nadzorovano u\u010denje (angl. supervised learning ) se uporablja, ko \u017eelimo, da se ra\u010dunalnik nau\u010di klasificirati (razvr\u0161\u010dati) podatke v vnaprej dolo\u010dene razrede ali jim pripisovati \u0161tevilske vrednosti. Temu re\u010demo nadzorovano u\u010denje, ker se stroj u\u010di na re\u0161enih podatkih (z znanimi razredi ali vrednostmi) in ker lahko nadzorujemo kakovost dobljenih modelov znanja. Pri nadzorovanem u\u010denju imamo dve nalogi, ki ju mora stroj opravljati: regresijo in klasifikacijo. Regresija (angl. regression ) se uporablja, ko se ra\u010dunalnik na podlagi podanih karakteristik (neodvisnih spremenljivk) nau\u010di napovedovanja numeri\u010dnih vrednosti (odvisne spremenljivke). Primer takega problema bi bil napovedovanje cene delnice ali koli\u010dine de\u017eja glede na znane podatke. Z regresijo se v tej knjigi ne bomo ukvarjali, zato se v podrobnej\u0161e razlage ne bomo spu\u0161\u010dali. Klasifikacija (angl. classification ) pa se uporablja, ko se ra\u010dunalnik iz re\u0161enih podatkov nau\u010di te razvr\u0161\u010dati v vnaprej dolo\u010dene razrede. Primer smo \u017ee omenjali, ko smo govorili o diagnozi pacientov in ga bomo podrobneje opisali v nadaljevanju. Nenadzorovano u\u010denje (angl. unsupervised learning ) uporabimo, ko \u017eelimo odkriti \u0161e neznane povezave med podatki in strukturo teh podatkov. V tem primeru na\u0161i podatki ne vsebujejo re\u0161itve, saj re\u0161itev \u0161e ne poznamo, in posledi\u010dno kakovosti takih modelov ne moremo nadzirati. Nenadzorovano u\u010denje ima ve\u010d nalog, ki se jih stroj nau\u010di: gru\u010denje in sprememba strukture podatkov. Gru\u010denje (angl. clustering ) je tehnika, kjer ra\u010dunalnik najde vzorce, ki povezujejo podatke v gru\u010de, in tako najde doslej neznane povezave med podatki. Definicija teh gru\u010d ni vnaprej znana, a v vsakem primeru ra\u010dunalnik uporabi eno izmed tehnik, da gru\u010de zdru\u017eujejo podobne in povezane podatke skupaj. \u0160tevilo gru\u010d je lahko vnaprej dolo\u010deno ali pa se odlo\u010ditev o \u0161tevilu gru\u010d prepusti ra\u010dunalniku. Primer gru\u010denja je iskanje profilov strank v trgovini, kjer imajo stranke v isti gru\u010di podobne nakupovalne navade. Spreminjanje in preu\u010devanje strukture podatkov pa zdru\u017euje tehnike, ki se ukvarjajo s transformacijo, preslikavo, zdru\u017eevanjem in selekcijo posameznih karakteristik iz podatkov. Podrobneje teh tehnik ne bomo obravnavali v okviru te knjige. Delno nadzorovano u\u010denje (angl. semi-supervised learning ) je srednja pot med nadzorovanim in nenadzorovanim u\u010denjem. Pri tej tehniki \u0161e vedno klasificiramo podatke v vnaprej podane razrede, pri \u010demer si pomagamo z novimi karakteristikami podatkov, ki pa so rezultat nenadzorovanega u\u010denja, ali pa uporabljamo le delno ozna\u010dene podatke (na primer, \u010de imamo v u\u010dni mno\u017eici le bolne paciente). Tipi\u010dna uporaba delno nadzorovanega u\u010denja je iskanje anomalij, kjer poznamo samo lastnosti normalnih podatkov in iz tega sklepamo, kaj je normalno - vse, kar je druga\u010dno, pa je anomalija. Okrepitveno u\u010denje (angl. reinforcement learning ) je raz\u0161irjeno nadzorovano ali nenadzorovano u\u010denje, kjer se ra\u010dunalnik u\u010di na podlagi nagrad ali kazni glede na izide u\u010denja. Pri nagrajevanju in kaznovanju lahko sodeluje \u010dlovek ali pa je nagrada podeljena ra\u010dunsko. V primeru sodelovanja \u010dloveka, ta poda dodatne informacije o samih podatkih, ali pa v iterativnem postopku poda mnenje o kakovosti modela. Tega pristopa strojnega u\u010denja v okviru te knjige ne bomo obravnavali. Jiawei Han, M.K. and Pei, J., 2011. Data mining: concepts and techniques: concepts and techniques. \u21a9","title":"Strojno u\u010denje"},{"location":"pages/knjiga/03_strojno_ucenje/#strojno-ucenje","text":"Kam gremo po nasvet, ko se po\u010dutimo bolni? Se zate\u010demo k sosedu, sodelavki, prijatelju ali pa raje obi\u0161\u010demo zdravnika? Vsekakor je najbolje, da obi\u0161\u010demo zdravnika. Premislimo, zakaj. Kaj je prednost zdravnika v primerjavi z ostalimi na\u0161tetimi v danem problemu? Zdravnik ima najverjetneje mnogo ve\u010d znanja in izku\u0161enj pri diagnozi bolezni, saj se je kot \u0161tudent ve\u010d let u\u010dil prav o tej tematiki in v svoji delovni karieri \u017ee pridobil mnogotero izku\u0161enj. Tekom tega procesa pridobivanja znanja in izku\u0161enj tako predpostavimo, da se je zdravnik \u017ee usposobil za kakovostno spopadanje z danim problemom diagnoze bolezni.","title":"Strojno u\u010denje"},{"location":"pages/knjiga/03_strojno_ucenje/#model-znanja","text":"Naslednji\u010d, ko sre\u010date zdravnika, ga kar povpra\u0161ajte o tem, kako se spopade z izzivom diagnoze pacienta na podlagi simptomov obolenj. Verjetno bo za\u010del s pregledom trenutnega in preteklega stanja pacienta. Naredil bo razli\u010dne meritve pacientovega telesa (telesna temperatura, krvni pritisk, hormonska slika, krvna slika, rentgen ...) in dodatno pregledal pacientovo kartoteko. Ob preu\u010devanju novih in preteklih meritev pacienta bo zdravnik najverjetneje \u017ee imel ustaljen postopek razmi\u0161ljanja pri postavitvi diagnoze. Recimo, da \u017eelimo analizirati delo na\u0161ega zdravnika in ga prosimo, da na kar se da razumljiv na\u010din opi\u0161e svoj proces odlo\u010danja. Najverjetneje zdravnik (ali strokovnjak drugih podro\u010dij) nima formalno dolo\u010denega procesa odlo\u010danja, ampak se pri tem zateka tudi k svoji intuiciji, ki jo je razvil po vseh letih \u0161olanja in izku\u0161enj. Pa vendar, \u010de bi se zdravnik odlo\u010dil zapisati svoj proces odlo\u010danja na papir, bi ta verjetno izgledal nekako tako: \u010ce je telesna temperature pacienta nad 40 \u00b0C, je pacient bolan . \u010ce je telesna temperature nad 38 \u00b0C in je sistoli\u010dni krvni pritisk nad 140 mHg, je pacient bolan . \u010ce je telesna temperature pod 38 \u00b0C, je pacient zdrav . Svoj proces diagnoze bi zdravnik zapisal v obliki pravil, katerim bi sledil bodisi po vrsti ali pa bi jih upo\u0161teval vse hkrati. Tem pravilom pravimo model znanja (angl. knowledge model ). Kateri drug zdravnik pa bi svoj proces odlo\u010danja mogo\u010de la\u017eje zapisal v obliki odlo\u010ditvenega drevesa. Model znanja v obliki odlo\u010ditvenega drevesa. Pri interpretaciji takega drevesa bi zdravnik za\u010del na vrhu (korenu) drevesa in se z odgovarjanjem na vpra\u0161anja v posameznem vozli\u0161\u010du drevesa pri pregledu posameznega pacienta premikal vse do listov drevesa. Listi pa mu povedo odgovor glede diagnoze pacienta. Nek tretji zdravnik pa bolj\u0161e razmi\u0161lja, ko svoj model znanja predstavi v obliki matemati\u010dnih formul. Slede\u010da slika prikazuje graf, kjer je na horizontalni X osi telesna temperatura pacienta, na vertikalni Y osi pa je krvni pritisk pacienta. Matemati\u010dna formula deli prostor na dva dela - na prostor, kjer se nahajajo zdravi in na prostor, kjer se nahajajo bolni pacienti. Model znanja v obliki matemati\u010dne funkcije. Zadnji zdravnik pa bi deloval popolnoma druga\u010de kot prej\u0161nji trije. Ta ne bi znal na papir zapisati svojega modela znanja, saj se odlo\u010da po druga\u010dnem postopku - pregleda svoje prej\u0161nje paciente iz bogate dvajsetletne kariere in novega pacienta primerja z njimi. Ko najde primere \u017ee diagnosticiranih pacientov, ki so novemu pacientu najbolj podobni, preprosto pogleda, kak\u0161na je bila njihova diagnoza in temu novemu poda mnenje o njegovem stanju bolezni, ki je skladna s prej\u0161njimi (podobnimi) pacienti. Ta proces je prikazan slede\u010de. Model znanja v obliki sprotne primerjave z \u017ee re\u0161enimi primeri. Vsi \u0161tirje opisani na\u010dini odlo\u010danja zdravnikov v resnici prikazujejo razli\u010dne na\u010dine predstavitve modela znanja. Teh je v realnosti \u0161e mnogo ve\u010d, tekom te knjige pa bomo podrobneje spoznali zadnji na\u010din zapisa modela znanja - primerjavo novih primerov s starimi, \u017ee re\u0161enimi.","title":"Model znanja"},{"location":"pages/knjiga/03_strojno_ucenje/#ekspertni-sistem","text":"Gradnjo modela znanja zdravnika vizualno prikazuje spodnja slika. Najverjetneje preberejo zdravniki tekom \u0161tudija medicine in delovne kariere mnogo knjig, znanstvenih in strokovnih \u010dlankov ter pridobijo mnogotere izku\u0161nje. To ka\u017ee zgornji del slike. Rezultat takega procesa je model znanja (seveda ne formalno zapisan). Ob pregledu novega pacienta uporabijo ta model znanja, da pridejo do kon\u010dne diagnoze pacienta, kar je ponazorjeno s spodnjim delom slike. Poenostavljen prikaz procesa gradnje in uporabe modela znanja. Seveda lahko zdravnikov model znanja prepi\u0161emo v programsko kodo in tako uporabimo njegov model znanja v vsakdanjih informacijskih sistemih, kot ka\u017ee slede\u010da slika. To je standardni postopek pri gradnji ekspertnih sistemov (angl. expert systems ) tj. informacijskih sistemov, ki uporabijo model znanja, ki so jih zgradili strokovnjaki (na primer zdravniki). Z digitalizacijo modela znanja pridobimo zmo\u017enost hitrej\u0161e uporabe modela znanja in posledi\u010dnega odlo\u010danja, saj ra\u010dunalnik lahko tak model znanja uporabi tudi milijonkrat v sekundi. Proces uporabe modela znanja v ekspertnem sistemu. Pri ekspertnem sistemu \u0161e vedno ne govorimo o umetni inteligenci ali strojnem u\u010denju, saj ra\u010dunalnik ni sodeloval pri procesu u\u010denja (gradnje modela znanja), ampak je le neumen stroj, ki sledi modelu znanja, zapisanem v obliki if in for stavkov.","title":"Ekspertni sistem"},{"location":"pages/knjiga/03_strojno_ucenje/#inteligentni-sistem","text":"Premislimo, kako bi lahko v prej\u0161njem procesu vklju\u010dili ra\u010dunalnik v proces gradnje modela znanja. Namesto da \u010dlovek postane strokovnjak s pomo\u010djo preu\u010devanja literature in nabiranja izku\u0161enj, bomo tokrat uporabili kar ra\u010dunalnik za gradnjo modela. Ampak kako? Ra\u010dunalnik ne razume prebranih knjig in ne more pridobivati izku\u0161enj. Tukaj pa uporabimo enak pristop u\u010denja, kot ga uporabijo ljudje v situacijah, kjer se u\u010dijo s pomo\u010djo opazovanja - pri delu opazujejo strokovnjake in sku\u0161ajo ugotoviti, kako strokovnjaki delajo, da lahko to kasneje posnemajo. Podobno pot uberemo, ko \u017eelimo pri gradnji modela uporabiti ra\u010dunalnik kar ka\u017ee slika spodaj. Namesto iz knjig in izku\u0161enj, ra\u010dunalnik razbere vzorce iz dela strokovnjakov, ki pa je v tem primeru sestavljeno iz \u017ee re\u0161enih problemov. \u010ce ra\u010dunalniku podamo kartoteke \u017ee diagnosticiranih pacientov, bo ta lahko iz teh kartotek razbral vzorce, ki so zna\u010dilni za bolne paciente in vzorce, ki so zna\u010dilni za zdrave paciente. Seveda, enako kot vajenec ne bo postal mojster pri enournem opazovanju strokovnjaka pri delu, tako tudi ra\u010dunalnik ne uspe najti vzorcev iz le nekaj primerov \u017ee diagnosticiranih pacientov. Koliko podatkov pa bo dovolj? \u010ce je delo mojstra zelo zapleteno, bo vajenec potreboval nekaj let opazovanja, da bo ugotovil pravi na\u010din dela tega strokovnjaka. \u010ce pa vajencu ka\u017eemo, kako nalepiti obli\u017e na rano, pa bo vajenec vzorec za reprodukcijo videnega odkril \u017ee po nekaj minutah. Podobno je tudi pri ra\u010dunalniku. Koli\u010dina potrebnih re\u0161enih primerov je odvisna od koli\u010dine in kompleksnosti vzorcev - ve\u010d kot je vzorcev in bolj so ti kompleksni, ve\u010d podatkov bo ra\u010dunalnik potreboval, da bo vzorce prepoznal. Ra\u010dunalni\u0161ki algoritem, ki iz podatkov razbira vzorce, imenujemo algoritem strojnega u\u010denja (angl. machine learning algorithm ). Ta algoritem je zadol\u017een, da namesto \u010dloveka ustvari model znanja, ki ga kasneje lahko pri svojem delu uporabita tako \u010dlovek, kakor tudi ra\u010dunalnik. Sistem, ki vklju\u010duje algoritem strojnega u\u010denja za gradnjo modela znanja, pa imenujemo inteligentni sistem (angl. _intelligent system). Proces u\u010denja in uporabe modela znanja v inteligentnem sistemu.","title":"Inteligentni sistem"},{"location":"pages/knjiga/03_strojno_ucenje/#podatki","text":"Podatki, uporabljeni v algoritmu strojnega u\u010denja za namen kreacije modela znanja, so zdru\u017eeni v podatkovne mno\u017eice (angl. datasets ) in so lahko v obliki preproste strukturirane tekstovne datoteke ali podatkovne baze poljubne strukture (relacijske, objektne ali dokumentne podatkovne baze). Najenostavnej\u0161a struktura podatkov, namenjenih za strojno u\u010denje ima obliko, ki je prikazana v spodnji tabeli. Vsako vrstico v tekstovnem dokumentu ali vsak primerek podatkovne baze imenujemo u\u010dna instanca ali u\u010dni primerek (angl. learning example ali learning instance ). Vsaka instanca je opisana z mno\u017eico karakteristik imenovanih atributi (angl. features } ali tudi neodvisne spremenljivke oziroma zna\u010dilnice . Prostor atributov (angl. feature space } \\(F\\) je vektor vseh atributov. Vi\u0161ina Te\u017ea Starost \u2026 181 92 45 178 71 27 168 73 65 \u2026","title":"Podatki"},{"location":"pages/knjiga/03_strojno_ucenje/#strojno-ucenje_1","text":"Strojno u\u010denje (angl. machine learning } zajema tehnike, kjer se ra\u010dunalnik nau\u010di re\u0161evanja specifi\u010dnih in ozko usmerjenih nalog iz podatkov - pravimo, da odlo\u010ditve strojnega u\u010denja temeljijo na podatkih. Tehnike strojnega u\u010denja uvr\u0161\u010damo v krovno podro\u010dje umetne inteligence (angl. artificial intelligence ), ki pa pokriva mnogo \u0161ir\u0161e raziskovalno podro\u010dje. Podro\u010dje, povezano s strojnim u\u010denjem, je tako imenovano podatkovno rudarjenje (angl. data mining ), kjer s pomo\u010djo razli\u010dnih tehnik, med drugimi tudi s strojnim u\u010denjem, obdelujemo in preu\u010dujemo podatke ter posku\u0161amo iz njih razbrati vzorce in posledi\u010dno novo znanje. Izraz podatkovno rudarjenje uporabljamo, ko se sre\u010damo s problemom uporabe samih metod strojnega u\u010denja kot orodij za re\u0161evanje drugih problemov in ne s samo implementacijo teh. Strojno u\u010denje delimo na \u0161tiri podro\u010dja glede na stopnjo nadzora nad u\u010denjem 1 : Nadzorovano u\u010denje (angl. supervised learning ) se uporablja, ko \u017eelimo, da se ra\u010dunalnik nau\u010di klasificirati (razvr\u0161\u010dati) podatke v vnaprej dolo\u010dene razrede ali jim pripisovati \u0161tevilske vrednosti. Temu re\u010demo nadzorovano u\u010denje, ker se stroj u\u010di na re\u0161enih podatkih (z znanimi razredi ali vrednostmi) in ker lahko nadzorujemo kakovost dobljenih modelov znanja. Pri nadzorovanem u\u010denju imamo dve nalogi, ki ju mora stroj opravljati: regresijo in klasifikacijo. Regresija (angl. regression ) se uporablja, ko se ra\u010dunalnik na podlagi podanih karakteristik (neodvisnih spremenljivk) nau\u010di napovedovanja numeri\u010dnih vrednosti (odvisne spremenljivke). Primer takega problema bi bil napovedovanje cene delnice ali koli\u010dine de\u017eja glede na znane podatke. Z regresijo se v tej knjigi ne bomo ukvarjali, zato se v podrobnej\u0161e razlage ne bomo spu\u0161\u010dali. Klasifikacija (angl. classification ) pa se uporablja, ko se ra\u010dunalnik iz re\u0161enih podatkov nau\u010di te razvr\u0161\u010dati v vnaprej dolo\u010dene razrede. Primer smo \u017ee omenjali, ko smo govorili o diagnozi pacientov in ga bomo podrobneje opisali v nadaljevanju. Nenadzorovano u\u010denje (angl. unsupervised learning ) uporabimo, ko \u017eelimo odkriti \u0161e neznane povezave med podatki in strukturo teh podatkov. V tem primeru na\u0161i podatki ne vsebujejo re\u0161itve, saj re\u0161itev \u0161e ne poznamo, in posledi\u010dno kakovosti takih modelov ne moremo nadzirati. Nenadzorovano u\u010denje ima ve\u010d nalog, ki se jih stroj nau\u010di: gru\u010denje in sprememba strukture podatkov. Gru\u010denje (angl. clustering ) je tehnika, kjer ra\u010dunalnik najde vzorce, ki povezujejo podatke v gru\u010de, in tako najde doslej neznane povezave med podatki. Definicija teh gru\u010d ni vnaprej znana, a v vsakem primeru ra\u010dunalnik uporabi eno izmed tehnik, da gru\u010de zdru\u017eujejo podobne in povezane podatke skupaj. \u0160tevilo gru\u010d je lahko vnaprej dolo\u010deno ali pa se odlo\u010ditev o \u0161tevilu gru\u010d prepusti ra\u010dunalniku. Primer gru\u010denja je iskanje profilov strank v trgovini, kjer imajo stranke v isti gru\u010di podobne nakupovalne navade. Spreminjanje in preu\u010devanje strukture podatkov pa zdru\u017euje tehnike, ki se ukvarjajo s transformacijo, preslikavo, zdru\u017eevanjem in selekcijo posameznih karakteristik iz podatkov. Podrobneje teh tehnik ne bomo obravnavali v okviru te knjige. Delno nadzorovano u\u010denje (angl. semi-supervised learning ) je srednja pot med nadzorovanim in nenadzorovanim u\u010denjem. Pri tej tehniki \u0161e vedno klasificiramo podatke v vnaprej podane razrede, pri \u010demer si pomagamo z novimi karakteristikami podatkov, ki pa so rezultat nenadzorovanega u\u010denja, ali pa uporabljamo le delno ozna\u010dene podatke (na primer, \u010de imamo v u\u010dni mno\u017eici le bolne paciente). Tipi\u010dna uporaba delno nadzorovanega u\u010denja je iskanje anomalij, kjer poznamo samo lastnosti normalnih podatkov in iz tega sklepamo, kaj je normalno - vse, kar je druga\u010dno, pa je anomalija. Okrepitveno u\u010denje (angl. reinforcement learning ) je raz\u0161irjeno nadzorovano ali nenadzorovano u\u010denje, kjer se ra\u010dunalnik u\u010di na podlagi nagrad ali kazni glede na izide u\u010denja. Pri nagrajevanju in kaznovanju lahko sodeluje \u010dlovek ali pa je nagrada podeljena ra\u010dunsko. V primeru sodelovanja \u010dloveka, ta poda dodatne informacije o samih podatkih, ali pa v iterativnem postopku poda mnenje o kakovosti modela. Tega pristopa strojnega u\u010denja v okviru te knjige ne bomo obravnavali. Jiawei Han, M.K. and Pei, J., 2011. Data mining: concepts and techniques: concepts and techniques. \u21a9","title":"Strojno u\u010denje"},{"location":"pages/knjiga/04_klasifikacija/","text":"Klasifikacija Osrednja metoda strojnega u\u010denja te knjige je metoda klasifikacije , kjer se ra\u010dunalnik nau\u010di klasificirati instance v vnaprej dolo\u010dene razrede. Z regresijo napovemo \u0161tevilske vrednosti in so odlo\u010ditve zvezne, pri klasifikaciji pa napovedujemo nominalne vrednosti - diskretne razrede. Klasifikacija se uporablja v primerih, kot so razpoznava vzorcev na slikah, prepre\u010devanje prevar, zaznavanje neza\u017eelene po\u0161te in diagnosticiranje bolezni. \u010ce ra\u010dunalnik podatke deli v dva razreda, govorimo o binarni klasifikaciji , \u010de pa ra\u010dunalnik klasificira v ve\u010d razredov, pa imamo opravka z ve\u010drazredno klasifikacijo . 1 Obstaja na tiso\u010de razli\u010dnih algoritmov klasifikacije, ki jih v grobem delimo glede na to, v kak\u0161ni obliki shranijo model znanja 1 2 : matemati\u010dne formule in porazdelitve (logisti\u010dna regresija, naivni Bayesov klasifikator, metoda podpornih vektorjev), odlo\u010ditvena drevesa (CART, C4.5, ID3, evolucijska drevesa), odlo\u010ditvena pravila (RIPPER, PART, evolucijska pravila), umetne nevronske mre\u017ee (konvolucijske, rekurzivne) in klasifikatorji na podlagi podobnosti ( k najbli\u017ejih sosedov ). Nekatere metode zgradijo model znanja, ki se uporablja pri klasifikaciji novih instanc, ne da bi bil potreben vpogled v prej podane podatke. Takim metodam pravimo metode takoj\u0161njega u\u010denja (angl. eager learning ), saj zgradijo model znanja takoj in ga kasneje ne prilagajajo. Nasprotno, pa nekatere metode, kot na primer k najbli\u017ejih sosedov, ne ustvarijo u\u010dnega modela, ampak za vsako klasifikacijo nove instance ponovno naredijo pregled \u017ee prej podanih podatkov. Te metode uporabljajo leno u\u010denje (angl. lazy learning ), saj se u\u010dijo sproti po potrebi. Prav ta pristop bomo kasneje pregledali pri preizkusu na\u0161ega klasifikatorja k najbli\u017ejih sosedov. Matemati\u010dna definicija pojmov V nadaljevanju bomo za metodo klasifikacije uporabljali slede\u010do definicijo instanc. Ena instanca je par \\((x_i,y_i)\\) , kjer je \\(x_i\\) vektor vrednosti (atributov) te instance, \\(y_i\\) pa je dejanski razred instance (skalarna vrednost). U\u010dna mno\u017eica \\(X\\) je definirana kot mno\u017eica vseh instanc, na katerih se algoritem u\u010di, sestavljena je iz \\(n\\) instanc in je definirana, kot prikazuje spodnja ena\u010dba. \\[\\begin{align*} \\begin{split} X &= \\left\\lbrace \\left( x_1,y_1 \\right),\\left( x_2,y_2 \\right), \\dots ,\\left( x_n,y_n \\right) \\right\\rbrace \\\\ x_i &= (x_i^1,x_i^2, \\dots, x_i^l) \\\\ y_i &\\in \\left\\lbrace razred_1, razred_2, \\dots , razred_k \\right\\rbrace \\\\ n &= \\text{\u0161tevilo instanc} \\\\ l &= \\text{\u0161tevilo atributov} \\\\ k &= \\text{\u0161tevilo razredov} \\end{split} \\end{align*}\\] Vrednost \\(x_i\\) je vektor atributov \\((x_i^1,x_i^2, \\dots, x_i^l)\\) velikosti \\(l\\) , ki je definiran v prostoru atributov \\(F\\) , razred instance \\(y_i\\) pa je vrednost iz nabora vseh mo\u017enih razredov v velikosti \\(k\\) . Cilj algoritmov klasifikacije je, da ob dani u\u010dni mno\u017eici \\(X\\) najdejo slede\u010do funkcijo: \\[\\begin{align*} h: x_i \\rightarrow y_i \\end{align*}\\] za vsak \\(i \\in \\left[ 1,n\\right]\\) , tako da bo model znanja klasifikacije \\(h(x_i)\\) dovolj dober napovedovalec razreda \\(y_i\\) . Proces klasifikacije prikazuje slede\u010da slika, kjer iz podatkovne mno\u017eice \\(X\\) algoritem klasifikacije ustvari model znanja klasifikacije \\(h\\) . Ta model preslika vhode instance \\(x^1, x^2, \\dots, x^l\\) v razred \\(y\\) .. Splo\u0161ni proces klasifikacije. Ob pregledu splo\u0161nega podro\u010dja strojnega u\u010denja in klasifikacije pa zdaj sledi preizkus prvega klasifikatorja. Naredili bomo teoreti\u010dni pregled klasifikatorja k najbli\u017ejih sosedov in prikazali prakti\u010dni primer njegove uporabe. Friedman, J., Hastie, T. and Tibshirani, R., 2001. The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics. \u21a9 \u21a9 Kotsiantis, S.B., Zaharakis, I. and Pintelas, P., 2007. Supervised machine learning: A review of classification techniques. Emerging artificial intelligence applications in computer engineering, 160(1), pp.3-24. \u21a9","title":"Klasifikacija"},{"location":"pages/knjiga/04_klasifikacija/#klasifikacija","text":"Osrednja metoda strojnega u\u010denja te knjige je metoda klasifikacije , kjer se ra\u010dunalnik nau\u010di klasificirati instance v vnaprej dolo\u010dene razrede. Z regresijo napovemo \u0161tevilske vrednosti in so odlo\u010ditve zvezne, pri klasifikaciji pa napovedujemo nominalne vrednosti - diskretne razrede. Klasifikacija se uporablja v primerih, kot so razpoznava vzorcev na slikah, prepre\u010devanje prevar, zaznavanje neza\u017eelene po\u0161te in diagnosticiranje bolezni. \u010ce ra\u010dunalnik podatke deli v dva razreda, govorimo o binarni klasifikaciji , \u010de pa ra\u010dunalnik klasificira v ve\u010d razredov, pa imamo opravka z ve\u010drazredno klasifikacijo . 1 Obstaja na tiso\u010de razli\u010dnih algoritmov klasifikacije, ki jih v grobem delimo glede na to, v kak\u0161ni obliki shranijo model znanja 1 2 : matemati\u010dne formule in porazdelitve (logisti\u010dna regresija, naivni Bayesov klasifikator, metoda podpornih vektorjev), odlo\u010ditvena drevesa (CART, C4.5, ID3, evolucijska drevesa), odlo\u010ditvena pravila (RIPPER, PART, evolucijska pravila), umetne nevronske mre\u017ee (konvolucijske, rekurzivne) in klasifikatorji na podlagi podobnosti ( k najbli\u017ejih sosedov ). Nekatere metode zgradijo model znanja, ki se uporablja pri klasifikaciji novih instanc, ne da bi bil potreben vpogled v prej podane podatke. Takim metodam pravimo metode takoj\u0161njega u\u010denja (angl. eager learning ), saj zgradijo model znanja takoj in ga kasneje ne prilagajajo. Nasprotno, pa nekatere metode, kot na primer k najbli\u017ejih sosedov, ne ustvarijo u\u010dnega modela, ampak za vsako klasifikacijo nove instance ponovno naredijo pregled \u017ee prej podanih podatkov. Te metode uporabljajo leno u\u010denje (angl. lazy learning ), saj se u\u010dijo sproti po potrebi. Prav ta pristop bomo kasneje pregledali pri preizkusu na\u0161ega klasifikatorja k najbli\u017ejih sosedov.","title":"Klasifikacija"},{"location":"pages/knjiga/04_klasifikacija/#matematicna-definicija-pojmov","text":"V nadaljevanju bomo za metodo klasifikacije uporabljali slede\u010do definicijo instanc. Ena instanca je par \\((x_i,y_i)\\) , kjer je \\(x_i\\) vektor vrednosti (atributov) te instance, \\(y_i\\) pa je dejanski razred instance (skalarna vrednost). U\u010dna mno\u017eica \\(X\\) je definirana kot mno\u017eica vseh instanc, na katerih se algoritem u\u010di, sestavljena je iz \\(n\\) instanc in je definirana, kot prikazuje spodnja ena\u010dba. \\[\\begin{align*} \\begin{split} X &= \\left\\lbrace \\left( x_1,y_1 \\right),\\left( x_2,y_2 \\right), \\dots ,\\left( x_n,y_n \\right) \\right\\rbrace \\\\ x_i &= (x_i^1,x_i^2, \\dots, x_i^l) \\\\ y_i &\\in \\left\\lbrace razred_1, razred_2, \\dots , razred_k \\right\\rbrace \\\\ n &= \\text{\u0161tevilo instanc} \\\\ l &= \\text{\u0161tevilo atributov} \\\\ k &= \\text{\u0161tevilo razredov} \\end{split} \\end{align*}\\] Vrednost \\(x_i\\) je vektor atributov \\((x_i^1,x_i^2, \\dots, x_i^l)\\) velikosti \\(l\\) , ki je definiran v prostoru atributov \\(F\\) , razred instance \\(y_i\\) pa je vrednost iz nabora vseh mo\u017enih razredov v velikosti \\(k\\) . Cilj algoritmov klasifikacije je, da ob dani u\u010dni mno\u017eici \\(X\\) najdejo slede\u010do funkcijo: \\[\\begin{align*} h: x_i \\rightarrow y_i \\end{align*}\\] za vsak \\(i \\in \\left[ 1,n\\right]\\) , tako da bo model znanja klasifikacije \\(h(x_i)\\) dovolj dober napovedovalec razreda \\(y_i\\) . Proces klasifikacije prikazuje slede\u010da slika, kjer iz podatkovne mno\u017eice \\(X\\) algoritem klasifikacije ustvari model znanja klasifikacije \\(h\\) . Ta model preslika vhode instance \\(x^1, x^2, \\dots, x^l\\) v razred \\(y\\) .. Splo\u0161ni proces klasifikacije. Ob pregledu splo\u0161nega podro\u010dja strojnega u\u010denja in klasifikacije pa zdaj sledi preizkus prvega klasifikatorja. Naredili bomo teoreti\u010dni pregled klasifikatorja k najbli\u017ejih sosedov in prikazali prakti\u010dni primer njegove uporabe. Friedman, J., Hastie, T. and Tibshirani, R., 2001. The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics. \u21a9 \u21a9 Kotsiantis, S.B., Zaharakis, I. and Pintelas, P., 2007. Supervised machine learning: A review of classification techniques. Emerging artificial intelligence applications in computer engineering, 160(1), pp.3-24. \u21a9","title":"Matemati\u010dna definicija pojmov"},{"location":"pages/knjiga/05_knn/","text":"K najbli\u017ejih sosedov Za\u010deli bomo s pregledom delovanja klasifikacije na podlagi podobnosti med instancami. Prvo vpra\u0161anje, ki se nam poraja, je, kako sploh dolo\u010dimo podobnost med instancami? Oziroma povedano druga\u010de, kdaj sta si dve instanci podobni in kdaj ne? Intuitivno si ljudje naredimo prvi vtis glede na podobnost z \u017ee poznanimi koncepti. Vidimo nov avto? Ta je podoben na\u0161emu avtu doma - torej je najverjetneje hiter, porabi veliko goriva in ni primeren za vo\u017enjo po slabi cesti. Izbira novih \u010devljev s primerjavo. Kaj pa, \u010de izbiramo nove zimske \u010devlje izmed nabora \u010devljev, ki jih trgovina ponuja? Vse \u010devlje v ponudbi primerjamo na podlagi izku\u0161enj z zimskimi \u010devlji, ki smo si jih lastili v preteklosti. \u017delimo si nove \u010devlje, ki so \u010dim bolj podobni prej\u0161njim, na \u017ealost obrabljenim. V\u0161e\u010d nam je bila njihova barva, prav tako nas niso \u017eulili pri dalj\u0161i hoji, na ledu nam nikoli ni drselo pa tudi za na fakulteto so bili primerni. Izmed vseh ponujenih \u010devljev najdemo najbli\u017eje na\u0161im prej\u0161njim - to ljudje naredimo intuitivno, hitro in brez nepotrebnih kalkulacij. Kako pa pripravimo ra\u010dunalnik, da bo videl podobnosti med predstavljenimi koncepti? Z izra\u010dunom razdalje med dvema konceptoma. Bolj sta si dve stvari podobni, manj\u0161a je razdalja med njima, ter obratno - manj sta si dve stvari podobni, ve\u010dja je razdalja med njima. Ra\u010dunanje razdalj Poglejmo si primer primerjave dveh \u010devljev. Primerjava dveh \u010devljev. Ta primer preslikajmo v tabelo, kjer je prva vrstica namenjena atributom prvih \u010devljev, druga vrstica je namenjena atributom drugih \u010devljev, v tretji vrstici pa so razlike med njima. Vi\u0161ina Te\u017ea Vodoodporni Barva \u010cevlji A 17 231 da rjavi \u010cevlji B 9 119 ne modri Razlika 8 112 ? ? Primer ra\u010dunanja razdalje med dvema \u010devljema. Razlike med \u0161tevilskimi atributi je enostavno izra\u010dunati; preprosto vzamemo vrednost \u0161tevilskih atributov prve instance in od\u0161tejemo vrednost atributov druge instance. V primeru kategori\u010dnih atributov pa ni mo\u017eno dolo\u010diti, kateri kategoriji sta si bli\u017eji in kateri sta si dlje. Ko moramo izra\u010dunati razdaljo med dvema kategorijama, preprosto uporabimo naslednje pravilo: \u010ce sta kategoriji enaki, je razdalja med njima 0. \u010ce sta kategoriji razli\u010dni, je razdalja med njima 1. Knji\u017enica scikit-learn pa ne razlikuje med tipi spremenljivk - vse spremenljivke obravnava kot \u0161tevilske oziroma razmernostne. Za na\u0161i dve kategori\u010dni spremenljivki vodoodpornosti in barve \u010devljev to predstavlja te\u017eavo, saj v trenutni obliki nista zapisani v obliki \u0161tevil. Indikacijski atributi Za uporabo podatkov s knji\u017enico scikit-learn je podatke potrebno prilagoditi tako, da kategori\u010dne atribute spremenimo v \u0161tevilske. Napa\u010den pristop bi bil dolo\u010ditev \u0161tevila vsaki kategoriji. Pri barvi \u010devljev bi tako barva \u010drna postala \u0161tevilo 0, barva modra \u0161tevilo 1, barva rjava \u0161tevilo 2, in tako naprej. Ta pristop \u0161e vedno ni pravilen, saj algoritmi strojnega u\u010denja najve\u010dkrat operirajo s takimi vrednostmi - naklju\u010dna dolo\u010ditev \u0161tevilk kategorijam pa bi izra\u010dune pokvarila. Pri ra\u010dunanju razdalj bi tako razdalja med \u010drnimi in rjavimi \u010devlji bila 2, med \u010drnimi in modrimi pa le 1. To je nesmiselno, saj barv ne moremo postaviti v vrstni red. Posledi\u010dno se transformacije kategori\u010dnih atributov v \u0161tevilske atribute lotimo s pomo\u010djo kreacije indikacijskih atributov (angl. dummy attribute ). Iz enega kategori\u010dnega atributa tako nastane ve\u010d novih \u0161tevilskih atributov. Za vsako kategorijo enega kategori\u010dnega atributa tako nastane svoj \u0161tevilski atribut. \u010ce so \u010devlji lahko treh razli\u010dnih barv (\u010drni, modri, rjavi), potem nastanejo trije novi atributi: Barva (\u010drni) , Barva (modri) in Barva (rjavi) . Indikacijski atribut ima lahko le dve vrednosti: vrednost 0, \u010de kategorija ne dr\u017ei za instanco ter vrednost 1, \u010de kategorija dr\u017ei za instanco. Poglejmo si tabelo podatkov obeh \u010devljev po transformaciji kategori\u010dnih atributov v indikacijske atribute. Zdaj je primerjava mnogo bolj smiselna. Prav tako je opazno, da obstaja razlika tako v vodoodpornosti \u010devljev, kakor v barvi. Vi\u0161ina Te\u017ea Vod. Vod. Barva Barva Barva (da) (ne) (modri) (rjavi) (\u010drni) \u010cevlji A 17 231 1 0 0 0 1 \u010cevlji B 9 119 0 1 0 1 0 Razlika 8 112 1 -1 0 -1 1 Primer ra\u010dunanja razdalje z indikacijskimi atributi. Kreacija indikacijskih atributov v Pythonu Najenostavnej\u0161i postopek kreacije indikacijskih atributov je s pomo\u010djo pandas knji\u017enice. Struktura podatkov DataFrame ima \u017ee zabele\u017een tip vsakega stolpca, kar poenostavi avtomatsko transformacijo kategori\u010dnih atributov v indikacijske. Slede\u010da koda prikazuje kreacijo podatkov, iz katerih se bodo kreirali indikacijski atributi. import pandas as pd cevljiA = [ 17 , 231 , 'da' , 'rjavi' ] cevljiB = [ 9 , 119 , 'ne' , 'modri' ] cevljiC = [ 12 , 143 , 'ne' , '\u010drni' ] cevljiD = [ 8 , 112 , 'ne' , 'rjavi' ] cevljiE = [ 11 , 198 , 'da' , 'modri' ] cevljiF = [ 15 , 245 , 'da' , '\u010drni' ] # Z zdru\u017eitvijo instanc ustvarimo DataFrame podatki = pd . DataFrame ([ cevljiA , cevljiB , cevljiC , cevljiD , cevljiE , cevljiF ], columns = [ 'Vi\u0161ina' , 'Te\u017ea' , 'Vodood' , 'Barva' ], index = [ 'cevlji A' , 'cevlji B' , 'cevlji C' , 'cevlji D' , 'cevlji E' , 'cevlji F' ]) print ( podatki ) Vi\u0161ina Te\u017ea Vodood Barva cevlji A 17 231 da rjavi cevlji B 9 119 ne modri cevlji C 12 143 ne \u010drni cevlji D 8 112 ne rjavi cevlji E 11 198 da modri cevlji F 15 245 da \u010drni S pregledom vrednosti dtypes podatkov lahko ugotovimo kak\u0161nega tipa je posamezen atribut (stolpec). podatki . dtypes Vi\u0161ina int64 Te\u017ea int64 Vodood object Barva object dtype: object Atributa Vodood in Barva sta tipa object , kar pomeni, da sta obravnavana kot kategori\u010dni spremenljivki. Pred uporabo teh podatkov v procesu strojnega u\u010denja s knji\u017enico scikit-learn je potrebno spremeniti vse atribute object v \u0161tevilske. Knji\u017enica pandas ponuja metodo get_dummies() , ki pregleda podane podatke tipa DataFrame in vse atribute tipa object spremeni v indikacijske atribute. Izvorna spremenljivka se ne spremeni, temve\u010d se podatki z indikacijskimi atributi vrnejo kot rezultat metode. Pregled tipa atributov poka\u017ee, da so zdaj vsi atributi \u0161tevilskega tipa, s \u010dimer zadostimo uporabi v kombinaciji s scikit-learn knji\u017enico. # Vse kategori\u010dne (object) atribute spremenimo v indikacijske podatki_z_indikacijskimi = pd . get_dummies ( podatki ) podatki_z_indikacijskimi . dtypes Vi\u0161ina int64 Te\u017ea int64 Vodood_da uint8 Vodood_ne uint8 Barva_modri uint8 Barva_rjavi uint8 Barva_\u010drni uint8 dtype: object Pregled vsebine novih podatkov prika\u017ee novonastale indikacijske atribute. print ( podatki_z_indikacijskimi ) Vi\u0161ina Te\u017ea Vodood_da Vodovod_ne Barva_modri Barva_rjavi Barva_\u010drni cevlji A 17 231 1 0 0 1 0 cevlji B 9 119 0 1 1 0 0 cevlji C 12 143 0 1 0 0 1 cevlji D 8 112 0 1 0 1 0 cevlji E 11 198 1 0 1 0 0 cevlji F 15 245 1 0 0 0 1 Skupna razdalja Pri vmesnih izra\u010dunih razdalj \u0161e vedno ne pridemo do kon\u010dne skupne razdalje med dvema instancama. Za agregacijo vmesnih razdalj v eno mero razdalje pa lahko izberemo ve\u010d razli\u010dnih pristopov. Slede\u010da slika prikazuje ve\u010d vrst razdalj, ki bodo opisane v slede\u010dih sekcijah. Razli\u010dne razdalje med dvema to\u010dkama. Evklidska razdalja Evklidsko razdaljo med dvema to\u010dkama v ravnini je definiral matematik Evklid. Deluje po principu Pitagorovega izreka, kjer se razdalja med dvema to\u010dkama oz. instancama ( \\(x_1\\) in \\(x_2\\) ) izra\u010duna kot koren vsote kvadratov vseh dimenzij. \\[\\begin{align*} \\begin{split} d_E\\left(x_1,x_2\\right)&=\\sqrt{\\left( x_1^{(1)}-x_2^{(1)}\\right)^2 + \\left( x_1^{(2)}-x_2^{(2)}\\right)^2 + \\dots \\left( x_1^{(l)}-x_2^{(l)}\\right)^2} \\\\ &=\\sqrt{\\sum \\nolimits _{i=1}^{l} \\left( x_1^{(i)}-x_2^{(i)}\\right)^2 } \\end{split} \\end{align*}\\] kjer je \\(l\\) \u0161tevilo atributov posamezne instance. Mahattanska razdalja Zelo pogosto pa nas zanima manhattanska razdalja, ki si zgled za ra\u010dunanje razdalje med dvema to\u010dkama vzame po postavitvi cest na otoku Manhattan, kot ka\u017ee slika spodaj. V ve\u010djem delu otoka so ceste postavljene vzdol\u017e otoka (avenije) in pre\u010dno po otoku (ulice). Pri ra\u010dunanju razdalje od ene to\u010dke do druge je tako potrebno v obzir vzeti dol\u017eine vseh cest med dvema to\u010dkama, pri tem pa ni mo\u017eno kraj\u0161ati poti z diagonalami. Razdalje na Manhattnu. Pri izra\u010dunu manhattanske razdalje tako ni najkraj\u0161a pot predstavljena kot diagonalna in najkraj\u0161a daljica med dvema instancama, ampak kot vsota vseh daljic, ki poteka vzdol\u017e vseh osi. \\[\\begin{align*} \\begin{split} d_M\\left(x_1,x_2\\right)&=\\left| x_1^{(1)}-x_2^{(1)}\\right| + \\left| x_1^{(2)}-x_2^{(2)}\\right| + \\dots \\left| x_1^{(l)}-x_2^{(l)}\\right| \\\\ &=\\sum \\nolimits _{i=1}^{l} \\left| x_1^{(i)}-x_2^{(i)}\\right| \\end{split} \\end{align*}\\] Kosinusna razdalja Z manhattansko in evklidsko razdaljo pa pridemo do te\u017eav, ko imamo meritve, ki lahko imajo tako negativne kot pozitivne vrednosti. Primer take meritve bi bil letni zaslu\u017eek podjetja, saj je ta lahko tudi negativen (podjetje je na letni ravni imelo izgubo). Za primer vzemimo tri podjetja in njihove letne zaslu\u017eke ter spremembe dele\u017ea pokritega trga od prej\u0161njega leta, kot je na sliki. Primerjava evklidske, manhattanske in kosinusne razdalje. Tako evklidska kakor tudi manhattanska razdalja pravita, da sta podjetji A in C bli\u017eje kot pa podjetji A in B. \\[\\begin{align*} \\begin{split} d_E\\left(A,B\\right)&>d_E\\left(A,C\\right)\\\\ d_M\\left(A,B\\right)&>d_M\\left(A,C\\right)\\\\ d_C\\left(A,B\\right)&<d_M\\left(A,C\\right) \\end{split} \\end{align*}\\] To je v nasprotju z na\u0161o intuicijo: podjetje C je namre\u010d imelo izgubo v letnem prihodku in negativno spremembo pokritosti trga. Podjetji A in B pa sta imeli tako dobi\u010dek, kakor tudi se je njun dele\u017e pokritega trga pove\u010dal v primerjavi z lanskim letom. V takih situacijah sta evklidska in manhattanska razdalja neprimerni ter se uporabi kosinusna razdalja. Pri kosinusni razdalji je pomembna (1) smer vektorja posamezne instance ter (2) njegova dol\u017eina. Razliko med smerjo dveh vektorjev merimo s kotom \\(\\alpha\\) med vektorjema (instancama), ta pa je proporcionalna kosinusu kotov. Za instanci \\(x_1\\) in \\(x_2\\) velja slede\u010d izra\u010dun kosinusne razdalje. \\[\\begin{align*} \\begin{split} d_C\\left(x_1,x_2\\right)&=1- \\frac { x_1 \\cdot x_2}{||x_1|| \\cdot ||x_2||} \\\\ &=1-\\frac { \\sum _{i=1}^{l} x_1^{(i)} * x_2^{(i)}}{\\sqrt{\\sum _{i=1}^{l} x_1^{i~2}} * \\sqrt{\\sum _{i=1}^{l} x_2^{i~2}}} \\end{split} \\end{align*}\\] Izbira na\u010dina izra\u010duna razdalje je odvisna od problema. Najbolj intuitivna je vsekakor evklidska razdalja. Ve\u010dji je nabor atributov \\(l\\) , bolj je primerna manhattanska razdalja v primerjavi z evklidsko 1 . Kosinusna razdalja pa je primerna, ko nas bolj kot razdalje zanimajo podobnosti med instancami. Klasifikator k najbli\u017ejih sosedov Pri nekaterih algoritmih klasifikacije ne nastane u\u010dni model, ampak se klasifikator odlo\u010da vsakokrat ob pogledu v \u017ee klasificirane instance. Taka vrsta u\u010denja se imenuje leno u\u010denje in algoritem klasifikacije k najbli\u017ejih sosedov (angl. k nearest neighbors ) je tipi\u010dni primer lenega algoritma u\u010denja. Sledi pregled delovanja tega algoritma k najbli\u017ejih sosedov. Delovanje k najbli\u017ejih sosedov Algoritem k najbli\u017ejih sosedov ste\u010de po slede\u010dem postopku 2 , 3 . Za podano instanco \\(x_N\\) , ki jo \u017eelimo klasificirati, algoritem izra\u010duna razdalje do vseh \u017ee klasificiranih instanc. \u017de klasificirane instance razvrsti nara\u0161\u010dajo\u010de glede na razdaljo do instance \\(x_N\\) . V nadaljnji obravnavi upo\u0161teva le \\(k\\) prvih instanc v nara\u0161\u010dajo\u010dem seznamu - \\(k\\) najbli\u017ejih sosedov. Izra\u010duna pogostosti razredov iz nabora \\(k\\) najbli\u017ejih instanc kot je na sliki spodaj. Najpogostej\u0161i razred (modus) vrne kot rezultat klasifikacije instance \\(x_N\\) . \u010ce se ve\u010d razredov pojavlja z najve\u010djo pogostostjo, se izbere naklju\u010den razred izmed vseh najpogostej\u0161ih. Klasifikacija instance (moder trikotnik) s pomo\u010djo k najbli\u017ejih sosedov ob razli\u010dnih nastavitvah parametra k. Pri ra\u010dunanju razdalj je uporabljena evklidska razdalja. Sist. krvni tlak Tel. temp. Bolan Evklidska Manhattanska (mmHg) (\u00b0C) d rang d rang 82 38,0 Da 28,0 17 28,7 17 85 36,4 Da 25,1 16 27,3 16 89 36,7 Da 21,1 14 23,0 14 94 38,2 Da 16,0 11 16,5 11 95 38,2 Da 15,0 9 15,5 9 95 37,6 Ne 15,0 10 16,1 10 100 36,6 Ne 10,2 7 12,1 7 104 35,5 Ne 6,8 5 9,2 6 108 35,7 Ne 3,6 3 5,0 3 108 38,4 Da 2,0 2 2,3 2 109 39,4 Da 1,2 1 1,7 1 113 36,4 Ne 3,8 4 5,3 4 119 38,7 Da 9,0 6 9,0 5 124 37,5 Ne 14,1 8 15,2 8 128 37,8 Ne 18,0 12 18,9 12 129 38,8 Da 19,0 13 19,1 13 135 39,4 Da 25,0 15 25,7 15 140 36,6 Ne 30,1 18 32,1 18 145 36,0 Da 35,1 19 37,7 19 147 36,5 Da 37,1 20 39,2 20 U\u010dne instance. Sist. krvni tlak Tel. temp. Bolan (mmHg) (\u00b0C) 110 38,7 ? Nova instanca. Primer klasifikacije pacientov Sledi primer izra\u010dunov algoritma k najbli\u017ejih sosedov po korakih na primeru diagnoze bolezni pacientov. Imamo u\u010dno mno\u017eico, kjer merimo sistoli\u010dni krvni tlak pacientov in njihovo telesno temperaturo. Med kartotekami imamo \u017ee zabele\u017eene podatke 20 prej\u0161njih pacientov in njihove diagnoze, ki so jih postavili zdravniki. Instance in nov pacient so prikazani v zgornji tabeli. V tabeli je prav tako podan izra\u010dun razdalj, evklidske in manhattanske. Izra\u010dun obeh razdalj med prvo instanco mno\u017eice in novo instanco pacienta poteka po slede\u010dem postopku. \\[\\begin{align*} \\begin{split} d_E\\left(x_N,x_1\\right)&=\\sqrt{\\left( 110-82\\right)^2 + \\left( 38,7 - 38,0\\right)^2}\\\\ &=\\sqrt{\\left(28\\right)^2 + \\left(0,7\\right)^2}\\\\ &=\\sqrt{784 + 0,49} =\\sqrt{784,49} = 28,0 \\\\ \\\\ d_M\\left(x_N,x_1\\right)&=\\left| 110-82\\right| + \\left| 38,7 - 38,0\\right|\\\\ &=\\left| 28\\right| + \\left| 0,7\\right|\\\\ &=28 + 0,7 =28,7 \\end{split} \\end{align*}\\] Razvidno je, da razlika v krvnem tlaku prevladuje pri izra\u010dunu razdalje. Do tega pride zaradi razli\u010dnih enot, posledica pa je, da razdalje atributov z ve\u010djimi \u0161tevilskimi vrednostmi prevladujejo nad atributi manj\u0161ih \u0161tevilskih vrednosti. \u010ce \u017eelimo prispevek atributov pri ra\u010dunanju skupne razdalje poenotiti, se je potrebno lotiti standardizacije podatkov, s \u010dimer postavimo vse meritve na enako zalogo vrednosti. Standardizacija podatkov Standardizacija (angl. standardization ) je postopek transformacije podatkov na tak na\u010din, da bodo ti po transformaciji imeli povpre\u010dje enako 0 in standardni odklon enak 1. Za standardizacijo podatkov \\(x\\) rabimo njihovo povpre\u010dje \\(\\mu\\) in standardni odklon \\(\\rho\\) , ki je definiran slede\u010de za vseh \\(n\\) instanc. \\[\\begin{align*} \\begin{split} {\\rho} &= \\sqrt{\\frac{(x_1-\\mu)^2+(x_1-\\mu)^2+\\dots+(x_n-\\mu)^2}{n}} \\end{split} \\end{align*}\\] Standardizirane vrednosti \\(z\\) izra\u010dunamo iz izvornih podatkov \\(x\\) slede\u010de. \\[\\begin{align*} \\begin{split} z &= \\frac{x-\\mu}{\\rho} \\end{split} \\end{align*}\\] Vsak atribut \\(x^1\\) , \\(x^2\\) , \\(\\dots\\) , \\(x^l\\) standardiziramo lo\u010deno - transformirano z njegovim povpre\u010djem in standardnim odklonom. Po standardizaciji vseh atributov (tudi indikacijskih) imajo vsi atributi povpre\u010dje v vrednosti 0 in standardni odklon 1. Razdalje posameznih atributov med instancami bodo tako v podobnem intervalu. Za standardizacijo podatkov pacientov tako potrebujemo podatke povpre\u010dja in standardnega odklona instanc, kar prikazuje spodnja ena\u010dba. \\[\\begin{align*} \\begin{split} \\mu\\left(\\text{Sistoli\u010dni krvni pritisk}\\right)&= 112,45\\\\ \\rho\\left(\\text{Sistoli\u010dni krvni pritisk}\\right)&= 19,58\\\\ \\mu\\left(\\text{Telesna temperatura}\\right)&= 37,42\\\\ \\rho\\left(\\text{Telesna temperatura}\\right)&= 1,17\\\\ \\end{split} \\end{align*}\\] Tabela prikazuje vrednosti atributov po standardizaciji. Sist. krvni tlak Tel. temp. Bolan Evklidska Manhattanska (mmHg) (\u00b0C) d rang d rang -1,55 0,49 Da 1,55 11 2,03 11 -1,40 -0,87 Da 2,34 15 3,24 17 -1,20 -0,61 Da 2,01 14 2,78 15 -0,94 0,66 Da 0,92 5 1,24 6 -0,89 0,66 Da 0,88 4 1,19 5 -0,89 0,15 Ne 1,21 8 1,70 8 -0,64 -0,70 Ne 1,86 12 2,30 13 -0,43 -1,64 Ne 2,74 19 3,03 16 -0,23 -1,47 Ne 2,56 17 2,66 14 -0,23 0,84 Da 0,28 1 0,36 1 -0,18 1,69 Da 0,60 3 0,65 3 0,03 -0,87 Ne 1,97 13 2,11 12 0,33 1,09 Da 0,46 2 0,46 2 0,59 0,07 Ne 1,25 9 1,74 9 0,79 0,32 Ne 1,20 7 1,69 7 0,85 1,18 Da 0,97 6 1,06 4 1,15 1,69 Da 1,41 10 1,87 10 1,41 -0,70 Ne 2,36 16 3,32 18 1,66 -1,21 Da 2,91 20 4,09 20 1,76 -0,78 Da 2,66 18 3,76 19 Standardizirane u\u010dne instance. Sist. krvni tlak Tel. temp. Bolan (mmHg) (\u00b0C) -0,13 1,09 ? Standardizirana nova instanca. Podatki po standardizaciji ka\u017eejo druga\u010dno sliko. Vrstni red najbli\u017ejih instanc podani instanci se spremeni. Instanca, ki je v tabeli ozna\u010dena, je pred standardizacijo bila \u010detrta najbli\u017eja podani instanci. Pred standardizacijo je namre\u010d bila razlika v temperaturi majhna v primerjavi z razliko v krvnem tlaku. Po standardizaciji pa je razlika v temperaturi mnogo ve\u010dja, kot razlika pri krvnem tlaku. To je prav, saj sta bili pred standardizacijo zalogi vrednosti obeh meritev druga\u010dni. Realni obseg telesne temperature \u010dloveka je pribli\u017eno med 35 in 41 \u00b0C, kar predstavlja maksimalno razliko 6 enot. \u0160est enot razlike pri krvnem tlaku pa ni niti pribli\u017eno realnemu obsegu sistoli\u010dnega krvnega tlaka, ki se lahko giblje v intervalu med 80 in 180 mmHg z maksimalno razliko kar 100 enot. Po standardizaciji se krvni tlak giblje med \\(-1,55\\) ter \\(1,76\\) z maksimalno razliko \\(3,31\\) . To je v skladu z obsegom telesne temperature, ki se po standardizaciji giblje med \\(-1,64\\) ter \\(1,69\\) z maksimalno razliko \\(3,33\\) . Standardizacija je le en postopek tehnike, ki jo imenujemo normalizacija podatkov . Alternative standardizaciji so min-max skaliranje , centriranje , rangiranje in drugi. Vsak pristop strojnega u\u010denja ni ob\u010dutljiv na intervale oz. skalo atributov. Med take \u0161tejemo algoritme kreacije odlo\u010ditvenih dreves in naivnega Bayesa. Dobra praksa je, da pri postopku podatkovnega rudarjenja podatke pred obdelavo vedno normaliziramo, saj se s tem izognemo morebitnemu zavajanju algoritmov. Negativna plat normalizacije podatkov pa je, da ti niso najve\u010dkrat enostavno interpretabilni. Kakor ka\u017eejo standardizirane vrednosti v zgornji tabeli, so vrednosti telesne temperature nesmiselne. To postane \u0161e posebej problemati\u010dno v primeru, ko gradimo model znanja, ki temelji na pravilih (odlo\u010ditvena pravila ali odlo\u010ditvena drevesa), saj bodo vrednosti v pravilih standardizirane. Standardizacija v Pythonu Sledi primer, kjer za za\u010detek najprej pregledamo povpre\u010dne vrednosti in standardne odklone atributov pred standardizacijo. from sklearn.datasets import load_iris # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) print ( 'Povpre\u010dja pred standardizacijo:' ) print ( podatki . data . mean ( axis = 0 )) print ( 'Standardni odkloni pred standardizacijo:' ) print ( podatki . data . std ( axis = 0 )) Povpre\u010dja pred standardizacijo: sepal length (cm) 5.843333 sepal width (cm) 3.057333 petal length (cm) 3.758000 petal width (cm) 1.199333 dtype: float64 Standardni odkloni pred standardizacijo: sepal length (cm) 0.828066 sepal width (cm) 0.435866 petal length (cm) 1.765298 petal width (cm) 0.762238 dtype: float64 Sedaj pa te podatke standardiziramo in izpi\u0161emo povpre\u010dja ter standardne odklone novih vrednosti. Knji\u017enica scikit-learn ponuja kar nekaj na\u010dinov transformacije podatkov. Za standardizacijo se uporablja razred StandardScaler . S klicem metode fit se izra\u010dunata povpre\u010dje in standardni odklon iz podanih podatkov. Za transformacijo podatkov, pa se uporabi klic metode transform , kateri podamo podatke, ki jih \u017eelimo transformirati. from sklearn.preprocessing import StandardScaler # Inicializacija standardizatorja std = StandardScaler () # Izra\u010dunamo povpre\u010dja in standardne odklone atributov std . fit ( podatki . data ) # Standardizacija podatkov stand_podatki = std . transform ( podatki . data ) print ( 'Povpre\u010dja po standardizaciji:' ) print ( stand_podatki . mean ( axis = 0 )) print ( 'Standardni odkloni po standardizaciji:' ) print ( stand_podatki . std ( axis = 0 )) ovpre\u010dja po standardizaciji: -4.73695157e-16 -7.81597009e-16 -4.26325641e-16 -4.73695157e-16 Standardni odkloni po standardizaciji: 1. 1. 1. 1. Pregled rezultatov povpre\u010dij ka\u017ee, da so te zelo blizu vrednosti 0 ( \\(10^{-15}\\) ), standardni odkloni pa so enaki 1. \u010ce \u017eelimo proces izra\u010duna povpre\u010dij in standardnih odklonov ter proces transformacije podatkov zdru\u017eiti, lahko uporabimo metodo fit_transform , ki na podanih podatkih izra\u010duna vmesne vrednosti in jih vrne transformirane. stand_podatki = std . fit_transform ( podatki . data ) Dolo\u010ditev razreda podane instance Po izra\u010dunu razdalj in dolo\u010ditvi rangov glede na bli\u017eino do nove instance sledi dolo\u010ditev razreda (klasifikacija) nove instance. Pri tem procesu igra pomembno vlogo dolo\u010ditev vrednosti parametra \\(k\\) , ki nam pove, koliko najbli\u017ejih instanc upo\u0161tevamo pri klasifikaciji. Vseh \\(k\\) najbli\u017ejih instanc bo namre\u010d glasovalo za razred nove instance - vsaka instanca bo dala glas za razred, kateremu le-ta pripada. \u010ce je parameter \\(k=1\\) , potem vzamemo le najbli\u017ejo instanco in bo novi instanci dodeljen razred te instance. \u010ce se nave\u017eemo na primer iz tabele zgoraj, je pri izra\u010dunu obeh razdalj najbli\u017eja ista instanca - ta je razreda Da kar pomeni, da tudi novo instanco klasificiramo v razred Da . Tabeli spodaj prikazujeta rezultate klasifikacije pri razli\u010dnih nastavitvah - pri standardiziranih in nestandardiziranih podatkih, pri razli\u010dnih vrednostih \\(k\\) in pri evklidski ter manhattanski razdalji. Evklidska Manhattanska k Da Ne Rezultat Da Ne Rezultat 2 2 0 Da 2 0 Da 3 2 1 Da 2 1 Da 4 2 2 Da/Ne 2 2 Da/Ne 5 2 3 Ne 3 2 Da Klasifikacija s k najbli\u017ejih sosedov pri razli\u010dnih nastavitvah na nestandardiziranih podatkih. Evklidska Manhattanska k Da Ne Rezultat Da Ne Rezultat 2 2 0 Da 2 0 Da 3 3 0 Da 3 0 Da 4 4 0 Da 4 0 Da 5 5 0 Da 5 0 Da Klasifikacija s k najbli\u017ejih sosedov pri razli\u010dnih nastavitvah na standardiziranih podatkih. Zgornja tabela ka\u017ee zanimive rezultate. Najprej poglejmo nestandardizirane podatke. Tako pri evklidski, kot tudi pri manhattanski razdalji, se z ve\u010danjem \u0161tevila najbli\u017ejih instanc, ki se upo\u0161tevajo pri klasifikaciji, ve\u010da tudi negotovost, saj iz razreda Da pri upo\u0161tevanju le ene najbli\u017eje instance ( \\(k=1\\) ) preidemo do negotovosti, ko upo\u0161tevamo \u0161tiri najbli\u017eje instance ( \\(k=4\\) ), pa vse do spremembe odlo\u010ditve v razred Ne , ko upo\u0161tevamo pet najbli\u017ejih instanc ( \\(k=5\\) ) pri evklidski razdalji. Ti rezultati nam ka\u017eejo tudi razliko med evklidsko in manhattansko razdaljo, saj se odlo\u010ditve kon\u010dnega razreda instance ne skladajo pri \\(k=5\\) . Hkrati pa se moramo soo\u010diti \u0161e z negotovostjo pri \\(k=4\\) . Ko pridemo do neodlo\u010denega izida, se algoritem odlo\u010di za en naklju\u010den razred v vodstvu (tisti, ki ima manj\u0161i indeks), kar pa ni vedno najbolj\u0161a odlo\u010ditev. \u010ce bi ro\u010dno \u017eeleli izni\u010diti mo\u017enosti za neodlo\u010dene izide in naklju\u010dne odlo\u010ditve med njimi, bi algoritem preprosto zagnali \u0161e na drugih nastavitvah vrednosti \\(k\\) in pogledali, kak\u0161en je kon\u010den razred tedaj. Ko imamo opravek z binarno klasifikacijo (delitev instanc v dva razreda), pa se neodlo\u010denih izidov lahko znebimo z liho vrednostjo \\(k\\) . Po drugi strani pa ob pregledu rezultatov po standardizaciji vidimo ve\u010djo stabilnost, saj obstaja konsenz pri vseh nastavitvah \\(k\\) in pri obeh tipih razdalje. Ta pristop se tako izka\u017ee kot bolj robusten na manj\u0161e spremembe, pa tudi konceptualno je primernej\u0161i, saj vsi atributi instanc enakovredno vplivajo na odlo\u010ditev klasifikacije. Uporaba k najbli\u017ejih sosedov v Pythonu Algoritem k najbli\u017ejih sosedov je v knji\u017enici scikit-learn implementiran z razredom KNeighborsClassifier . \u017de pri inicializaciji primerka tega razreda dolo\u010dimo \\(k\\) \u0161tevilo najbli\u017ejih sosedov s parametrom n_neighbors in na\u010din ra\u010dunanja razdalje s parametrom metric . from sklearn.neighbors import KNeighborsClassifier klasif = KNeighborsClassifier ( n_neighbors = 3 , metric = 'manhattan' ) klasif . fit ( X_u , y_u ) napovedi = klasif . predict ( X_t ) Pri ra\u010dunanju razdalje lahko uporabimo evklidsko razdaljo z euclidean , mahattansko z manhattan in kosinusno razdaljo s cosine . S klicem metode fit in podajo podatkov instanc X_u in njihovih razredov y_u te shranimo in bodo slu\u017eili za izra\u010dun najbli\u017ejih sosedov. Metodo predict pa kli\u010demo s podajo mno\u017eice instanc X_t , ki jih \u017eelimo klasificirati. To je tudi standardni postopek uporabe drugih algoritmov klasifikacije, regresije in gru\u010denja v knji\u017enici scikit-learn : Nastavitve definiramo v konstruktorju. Model znanja zgradimo s fit(podatki_instanc, resitve_instanc) . Model znanja uporabimo s predict(podatki_novih_instanc) . Algoritmi pa lahko imajo tudi sebi specifi\u010dne metode. V primeru k najbli\u017ejih sosedov je njemu posebna metoda kneighbors , kateri podamo instance, za katere i\u0161\u010demo najbli\u017eje sosede, ter parameter n_neighbors , s katerim povemo, koliko najbli\u017ejih sosedov i\u0161\u010demo iz nabora vseh instanc podanih \u017ee prej v metodi fit . Rezultat sta dva seznama: seznam razdalj od izbranih instanc do najbli\u017ejih sosedov v nara\u0161\u010dajo\u010dem vrstnem redu glede na razdaljo, ter seznam indeksov najbli\u017ejih instanc, ponovno od najbli\u017ejega naprej. klasif . fit ( X_u , y_u ) razdalje , sosedi = klasif . kneighbors ( nove_instance , n_neighbors = 3 ) Sledi pregled uporabe klasifikacijskega algoritma k najbli\u017ejih sosedov za namen iskanja petih najbli\u017ejih sosedov eni instanci. Najprej s slede\u010do kodo nalo\u017eimo instance podatkovne mno\u017eice Iris , iz nje izberemo instanco v vrstici z indeksom 133 ter jo odstranimo iz podatkovne mno\u017eice. from sklearn.datasets import load_iris # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Izberemo eno instanco izbrana = 133 X_izbrana = podatki . data . iloc [ izbrana ,:] y_izbrana = podatki . target . iloc [ izbrana ] # Izbrano instanco izlo\u010dimo iz ostalih podatkov X_ostali = podatki . data . drop ( izbrana , axis = 0 ) y_ostali = podatki . target . drop ( izbrana ) Izbira vrednosti iz polja poteka s pomo\u010djo klica iloc[vrstica, stolpec] , kamor podamo indeks vrstice in indeks stolpca. \u010ce \u017eelimo izbrati celotno vrsto, podamo namesto indeksa kar dvopi\u010dje : . Tako s podatki.data.iloc[:, 12] izberemo stolpec z indeksom 12, s klicem podatki.data.iloc[8, :] pa izberemo vrstico z indeksom 8. \u010ce \u017eelimo izbrati ve\u010d vrednosti, pa namesto \u0161tevila na mestu vrstice in stolpca podamo polje indeksov. S klicem podatki.data.iloc[[2, 5, 8], :] izberemo vrstice s temi indeksi. In obratno, s klicem podatki.data.iloc[:, [3, 6, 9]] se vrnejo stolpci z indeksi 3, 6 in 9. Pri izbiri vrednosti iz vektorja podamo le eno vrednost, saj ima ta le eno dimenzijo. Tako nam klic podatki.target.iloc[[2, 5, 8]] vrne podatke z indeksi 2, 5 in 8. Z metodo podatki.data.drop(izbrana, axis=0) odstranimo instanco z indeksom izbrana iz podatkov podatki.data . Parameter axis dolo\u010da, po kateri osi izbri\u0161emo podatke - \u010de je podana 0, izbri\u0161emo vrstico, \u010de pa 1, pa izbri\u0161emo stolpec. Pri izbrisu iz podatki.target parametra axis ni potrebno podati, saj so podatki v obliki vektorja, ki ima le eno dimenzijo. Vizualizacija podatkov S pomo\u010djo knji\u017enice seaborn lahko enostavno vizualiziramo instance. Z dvema klicema metode scatterplot se en na drugega izri\u0161eta dva grafa raztrosa. S parametroma x in y podamo podatke, ki naj so izrisani na teh oseh. import seaborn as sns x_os , y_os = 0 , 1 # Izri\u0161emo ostale instance sns . scatterplot ( x = X_ostali . iloc [:, x_os ], y = X_ostali . iloc [:, y_os ], hue = podatki . target_names [ y_ostali ], palette = 'colorblind' ) # Izri\u0161emo eno izbrano instanco sns . scatterplot ( x = [ X_izbrana . iloc [ x_os ]], y = [ X_izbrana . iloc [ y_os ]], hue = [ 'Neznan' ], style = [ 'Neznan' ], markers = { 'Neznan' : '^' }) Instance podatkovne zbirke Iris. Horizontalna x os predstavlja prvi atribut, vertikalna y os pa drugi atribut podatkovne mno\u017eice. Barve lo\u010dijo razrede instanc, s trikotnikom pa je ozna\u010dena instanca, ki jo \u017eelimo klasificirati. Pomanjkljivost grafi\u010dne predstavitve podatkov je, da lahko izri\u0161emo instance glede na omejeno \u0161tevilo njihovih atributov. V na\u0161em primeru imamo dvodimenzionalen graf, kjer smo posamezne osi dolo\u010dili s spremenljivkama x_os in y_os . Parameter hue prejme podatke, ki bodo narisane ozna\u010dbe delili glede na barvo - v na\u0161em primeru so to podatki o razredu. Parameter style pa prejme podatke, ki dolo\u010dajo stil ozna\u010dbe, ki jih definiramo s parametrom markers . Barve ozna\u010db dolo\u010damo s podajanjem teme v parameter palette . U\u010denje in uporaba modela znanja U\u010denje modela in napoved razreda instance na indeksu 133 poteka na slede\u010d na\u010din. Najprej s konstruktorjem dolo\u010dimo vrednost \\(k\\) na pet najbli\u017ejih sosedov po izra\u010dunu manhattanske razdalje. Temu sledi klic metode fit , kateri podamo podatke instanc X_ostali in njihove razrede y_ostali . Ker metoda predict pri\u010dakuje ve\u010d instanc, dodamo instanco X_izbrana najprej v seznam in ta seznam podamo v klic metode predict([X_izbrana]) . \u010ce X_izbrana ne bi bil vektor, ampak polje, bi klic metode potekal brez ovijanja v seznam predict(X_izbrana) . from sklearn.neighbors import KNeighborsClassifier # Inicializiramo klasifikator knn = KNeighborsClassifier ( n_neighbors = 5 , metric = 'manhattan' ) # Shranimo instance za primerjavo knn . fit ( X_ostali , y_ostali ) # Napovemo razred izbrane instance napoved = knn . predict ([ X_izbrana ]) print ( f 'KNN je napovedal, da je instanca razreda { podatki . target_names [ napoved ] } .' ) print ( f 'Ta instanca je dejansko razreda { podatki . target_names [ y_izbrana ] } .' ) KNN je napovedal, da je instanca razreda ['versicolor']. Ta instanca je dejansko razreda virginica. Klasifikator je napovedal napa\u010den razred za to instanco. Preglejmo katerih pet instanc je po izra\u010dunu manhattanske razdalje najbli\u017eje na\u0161i izbrani instanci iz vrstice z indeksom 133. # Izbrani instanci najdemo pet najbli\u017ejih sosedov razdalje , sosedi = knn . kneighbors ([ X_izbrana ], n_neighbors = 5 ) print ( f 'Pet najbli\u017ejih: { sosedi } ' ) print ( f 'Razdalje od najbli\u017ejih do izbrane: { razdalje } ' ) Pet najbli\u017ejih: [[ 72 83 123 126 54]] Razdalje od najbli\u017ejih do izbrane: [[0.5 0.5 0.6 0.7 0.7]] Pet najbli\u017ejih instanc po manhattanski razdalji so instance z indeksi 72, 83, 123, 126 in 54. Rezultati razdalj pa so kar manhattanske razdalje teh sosedov do podane instance. Vpliv nastavitev na rezultate Poglejmo, kako se spremeni seznam petih najbli\u017ejih instanc, ko spreminjamo na\u010din izra\u010duna razdalje med instancami. for razdalja in [ 'euclidean' , 'manhattan' , 'cosine' ]: knn = KNeighborsClassifier ( n_neighbors = 5 , metric = razdalja ) knn . fit ( X_ostali , y_ostali ) razdalje , najblizje = knn . kneighbors ([ X_izbrana ], n_neighbors = 5 ) print ( f 'Najbli\u017eje instance po { razdalja } so { najblizje } ' ) print ( f 'Razdalje so { razdalje } ' ) Najbli\u017eje instance po euclidean so [[ 83 72 123 126 127]] Razdalje so [[0.33166248 0.36055513 0.37416574 0.43588989 0.45825757]] Najbli\u017eje instance po manhattan so [[ 72 83 123 126 54]] Razdalje so [[0.5 0.5 0.6 0.7 0.7]] Najbli\u017eje instance po cosine so [[125 129 90 131 83]] Razdalje so [[0.0001158 0.00022532 0.00033682 0.00034546 0.00039085]] Izra\u010dun petih najbli\u017ejih sosedov po evklidski in manhattanski razdalji vrne podobne rezultate. Instanca z indeksom 83 je najbli\u017eja izbrani po izra\u010dunu evklidske razdalje in je druga najbli\u017eja po manhattanski razdalji - mesto si izmenja z instanco 72. Tretje in \u010detrto mesto sta v obeh razdaljah zasedli instanci 123 in 125. Peto mesto ima v primeru evklidske razdalje instanca 127, v primeru manhattanske pa instanca 54. \u010ce sta instanci 127 in 54 druga\u010dnega razreda, lahko ta razlika vpliva na razred izbrane instance. Seznam najbli\u017ejih instanc po izra\u010dunu kosinusne razdalje pa je skorajda popolnoma druga\u010den od ostalih dveh - le instanca 83 je v vseh treh seznamih. Vse ostale \u0161tiri instance so v seznamu kosinusne razdalje druge v primerjavi s seznamoma evklidske in manhattanske razdalje. Ti rezultati nam ka\u017eejo, kako pomembna je odlo\u010ditev glede na\u010dina izra\u010duna razdalje. Slede\u010da slika ka\u017ee delitev obmo\u010dja razredov glede na na\u010din ra\u010dunanja razdalje. Pri kreaciji slike sta bila upo\u0161tevana prva dva atributa in vrednost \u0161tevila sosedov k je bila nastavljena na 5. Slika ka\u017ee, da ve\u010djih razlik med evklidsko in manhattansko razdaljo pri tej podatkovni mno\u017eici in nastavitvi k ni. Delitev obmo\u010dij je relativno jasna, z nekoliko ve\u010djim prekrivanjem v sredini, kjer so si instance razredov versicolor in virginica zelo podobne. Po drugi strani je razvidno, da delitev po izra\u010dunu glede na kosinusno razdaljo ni primerna za podano podatkovno mno\u017eico, saj sta obmo\u010dji versicolor in virginica preve\u010d prepleteni in je klasifikacija instanc v tem obmo\u010dju nestabilna. Delitev na obmo\u010dja razredov glede na na\u010din ra\u010dunanja razdalj med instancami. Poglejmo si \u0161e, kako vrednost k vpliva na kon\u010dno klasifikacijo izbrane instance. for k in [ 1 , 3 , 5 , 8 , 10 ]: knn = KNeighborsClassifier ( n_neighbors = k , metric = 'euclidean' ) knn . fit ( X_ostali , y_ostali ) napoved = knn . predict ([ X_izbrana ]) print ( f 'KNN z k= { k } je napovedal, da je izbrana instanca razreda { podatki . target_names [ napoved ] } ' ) KNN z k=1 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=3 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=5 je napovedal, da je izbrana instanca razreda ['virginica'] KNN z k=8 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=10 je napovedal, da je izbrana instanca razreda ['virginica'] Pri spreminjanju \u0161tevila najbli\u017ejih sosedov ne vidimo konsenza. Razred izbrane instance z indeksom 133 se namre\u010d spreminja iz (napa\u010dnega) razreda versicolor ob glasovanju enega, treh in osmih najbli\u017ejih sosedov, v (pravilen) razred virginica ob glasovanju petih ali desetih najbli\u017ejih sosedov. Ponovno je razvidno, da nastavitev igra vlogo pri napovedih algoritma. Vrednost k se najve\u010dkrat dolo\u010di po preizku\u0161anju, vsekakor pa mora biti vrednost smiselna (\u010de je k prevelik, bo v resnici algoritem vrnil najpogostej\u0161i razred). Preve\u010d optimiziranja nastavitev za namen bolj\u0161e klasifikacije ene instance je nesmiselno. Instanca indeksa 133 je bila izbrana namenoma, saj se napovedi njenega razreda zelo spreminjajo ob druga\u010dnih nastavitvah. Ve\u010dji del instanc dobi enako napoved razreda, ne glede na tip razdalje in k . To nam pove ve\u010d o tej instanci kot pa o samem algoritmu. Mogo\u010de je ta nekoliko nenavadna, ali pa je bila \u017ee v osnovi (s strani ekspertov) klasificirana napa\u010dno. Iz tega sledi, da je smiselno gledati rezultate in kakovost klasifikacije na ve\u010d instancah, ne le na eni - kaj pa \u010de je ta izbrana nenavadno. V naslednjem poglavju bomo spoznali na\u010dine ovrednotenja kakovosti klasifikacije in proces pravilne izbire instanc, s katerimi testiramo izbrani algoritem klasifikacije in njegovih nastavitev. Spodnja slika prikazuje razli\u010dna obmo\u010dja razredov glede na razli\u010dne vrednosti \u0161tevila sosedov k . Pri kreaciji slike sta bila ponovno upo\u0161tevana le prva dva atributa podatkov in ra\u010dunanje evklidskih razdalj. Iz slike je razvidno, da majhne vrednosti k prinesejo kar nekaj majhnih podro\u010dij klasifikacije. Po drugi strani pa je razvidno, da nastavitev \u0161tevila sosedov na 10 nekoliko pokvari delitev obmo\u010dja. Delitev na obmo\u010dja razredov glede na \u0161tevilo najbli\u017ejih sosedov k: 1, 3, 5, 8 in 10. Vpliv standardizacije podatkov na rezultate Poglejmo si, \u010de se klasifikacija kaj spremeni, \u010de uporabimo standardizirane podatke. from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import StandardScaler # Standardiziramo podatke standardizator = StandardScaler () standardizator . fit ( X_ostali ) X_ostali_s = standardizator . transform ( X_ostali ) X_izbrana_s = standardizator . transform ([ X_izbrana ]) for razdalja in [ 'euclidean' , 'manhattan' , 'cosine' ]: for k in [ 1 , 3 , 5 ]: knn = KNeighborsClassifier ( n_neighbors = k , metric = razdalja ) knn . fit ( X_ostali_s , y_ostali ) nap = knn . predict ( X_izbrana_s ) print ( f 'KNN metric= { razdalja } , k= { k } je napovedal razred { podatki . target_names [ nap ] } ' ) KNN metric=euclidean, k=1 je napovedal razred ['versicolor'] KNN metric=euclidean, k=3 je napovedal razred ['versicolor'] KNN metric=euclidean, k=5 je napovedal razred ['versicolor'] KNN metric=manhattan, k=1 je napovedal razred ['versicolor'] KNN metric=manhattan, k=3 je napovedal razred ['versicolor'] KNN metric=manhattan, k=5 je napovedal razred ['versicolor'] KNN metric=cosine, k=1 je napovedal razred ['versicolor'] KNN metric=cosine, k=3 je napovedal razred ['virginica'] KNN metric=cosine, k=5 je napovedal razred ['versicolor'] Objekt razreda StandardScaler najprej nau\u010dimo, kaj so povpre\u010dja in standardni odkloni podatkov s pomo\u010djo klica fit . Kasneje to uporabilo za transformacijo (standardizacijo) tako ostalih podatkov X_ostali , kakor tudi izbrane instance X_izbrana . Pri tem uporabimo klic metode transform . Napovedi so mnogo bolj robustne na spreminjanje nastavitev klasifikacijskega algoritma, ko imamo opravek s standardiziranimi podatki. Namre\u010d, najpogosteje napovedan razred je bil versicolor . \u010ceprav je napoved razreda napa\u010dna, imamo raje robustne napovedi, kot pa take, ki so preve\u010d odvisne od nastavitev algoritma. Enovit primer klasifikacije ve\u010d instanc Sledi enovit primer klasifikacije ve\u010d instanc iz Iris podatkovne zbirke, kjer so podatki tudi standardizirani. from sklearn.neighbors import KNeighborsClassifier from sklearn.datasets import load_iris import numpy as np from sklearn.preprocessing import StandardScaler # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Izberemo ve\u010d instanc izbrane = [ 1 , 31 , 61 , 91 , 121 ] X_izbrane = podatki . data . iloc [ izbrane ,:] y_izbrane = podatki . target . iloc [ izbrane ] # Izbrano instanco izlo\u010dimo iz ostalih podatkov X_ostali = podatki . data . drop ( izbrane , axis = 0 ) y_ostali = podatki . target . drop ( izbrane ) # Standardiziramo podatke standardizator = StandardScaler () standardizator . fit ( X_ostali ) X_ostali_s = standardizator . transform ( X_ostali ) X_izbrane_s = standardizator . transform ( X_izbrane ) # Zgradimo klasifikator in napovedmo razrede knn = KNeighborsClassifier ( n_neighbors = 3 , metric = 'euclidean' ) knn . fit ( X_ostali_s , y_ostali ) napovedi = knn . predict ( X_izbrane_s ) for i , napoved , dejansko in zip ( izbrane , napovedi , y_izbrane ): print ( f 'Instanca { i } je klasificirana kot { podatki . target_names [ napoved ] } dejansko pa je { podatki . target_names [ dejansko ] } .' ) Instanca 1 je klasificirana kot setosa dejansko pa je setosa. Instanca 31 je klasificirana kot setosa dejansko pa je setosa. Instanca 61 je klasificirana kot versicolor dejansko pa je versicolor. Instanca 91 je klasificirana kot versicolor dejansko pa je versicolor. Instanca 121 je klasificirana kot virginica dejansko pa je virginica. Aggarwal, C.C., Hinneburg, A. and Keim, D.A., 2001, January. On the surprising behavior of distance metrics in high dimensional space. In International conference on database theory (pp. 420-434). Springer, Berlin, Heidelberg. \u21a9 Aha, D.W., Kibler, D. and Albert, M.K., 1991. Instance-based learning algorithms. Machine learning, 6(1), pp.37-66. \u21a9 Goldberger, J., Hinton, G.E., Roweis, S. and Salakhutdinov, R.R., 2004. Neighbourhood components analysis. Advances in neural information processing systems, 17. \u21a9","title":"K najbli\u017ejih sosedov"},{"location":"pages/knjiga/05_knn/#k-najblizjih-sosedov","text":"Za\u010deli bomo s pregledom delovanja klasifikacije na podlagi podobnosti med instancami. Prvo vpra\u0161anje, ki se nam poraja, je, kako sploh dolo\u010dimo podobnost med instancami? Oziroma povedano druga\u010de, kdaj sta si dve instanci podobni in kdaj ne? Intuitivno si ljudje naredimo prvi vtis glede na podobnost z \u017ee poznanimi koncepti. Vidimo nov avto? Ta je podoben na\u0161emu avtu doma - torej je najverjetneje hiter, porabi veliko goriva in ni primeren za vo\u017enjo po slabi cesti. Izbira novih \u010devljev s primerjavo. Kaj pa, \u010de izbiramo nove zimske \u010devlje izmed nabora \u010devljev, ki jih trgovina ponuja? Vse \u010devlje v ponudbi primerjamo na podlagi izku\u0161enj z zimskimi \u010devlji, ki smo si jih lastili v preteklosti. \u017delimo si nove \u010devlje, ki so \u010dim bolj podobni prej\u0161njim, na \u017ealost obrabljenim. V\u0161e\u010d nam je bila njihova barva, prav tako nas niso \u017eulili pri dalj\u0161i hoji, na ledu nam nikoli ni drselo pa tudi za na fakulteto so bili primerni. Izmed vseh ponujenih \u010devljev najdemo najbli\u017eje na\u0161im prej\u0161njim - to ljudje naredimo intuitivno, hitro in brez nepotrebnih kalkulacij. Kako pa pripravimo ra\u010dunalnik, da bo videl podobnosti med predstavljenimi koncepti? Z izra\u010dunom razdalje med dvema konceptoma. Bolj sta si dve stvari podobni, manj\u0161a je razdalja med njima, ter obratno - manj sta si dve stvari podobni, ve\u010dja je razdalja med njima.","title":"K najbli\u017ejih sosedov"},{"location":"pages/knjiga/05_knn/#racunanje-razdalj","text":"Poglejmo si primer primerjave dveh \u010devljev. Primerjava dveh \u010devljev. Ta primer preslikajmo v tabelo, kjer je prva vrstica namenjena atributom prvih \u010devljev, druga vrstica je namenjena atributom drugih \u010devljev, v tretji vrstici pa so razlike med njima. Vi\u0161ina Te\u017ea Vodoodporni Barva \u010cevlji A 17 231 da rjavi \u010cevlji B 9 119 ne modri Razlika 8 112 ? ? Primer ra\u010dunanja razdalje med dvema \u010devljema. Razlike med \u0161tevilskimi atributi je enostavno izra\u010dunati; preprosto vzamemo vrednost \u0161tevilskih atributov prve instance in od\u0161tejemo vrednost atributov druge instance. V primeru kategori\u010dnih atributov pa ni mo\u017eno dolo\u010diti, kateri kategoriji sta si bli\u017eji in kateri sta si dlje. Ko moramo izra\u010dunati razdaljo med dvema kategorijama, preprosto uporabimo naslednje pravilo: \u010ce sta kategoriji enaki, je razdalja med njima 0. \u010ce sta kategoriji razli\u010dni, je razdalja med njima 1. Knji\u017enica scikit-learn pa ne razlikuje med tipi spremenljivk - vse spremenljivke obravnava kot \u0161tevilske oziroma razmernostne. Za na\u0161i dve kategori\u010dni spremenljivki vodoodpornosti in barve \u010devljev to predstavlja te\u017eavo, saj v trenutni obliki nista zapisani v obliki \u0161tevil.","title":"Ra\u010dunanje razdalj"},{"location":"pages/knjiga/05_knn/#indikacijski-atributi","text":"Za uporabo podatkov s knji\u017enico scikit-learn je podatke potrebno prilagoditi tako, da kategori\u010dne atribute spremenimo v \u0161tevilske. Napa\u010den pristop bi bil dolo\u010ditev \u0161tevila vsaki kategoriji. Pri barvi \u010devljev bi tako barva \u010drna postala \u0161tevilo 0, barva modra \u0161tevilo 1, barva rjava \u0161tevilo 2, in tako naprej. Ta pristop \u0161e vedno ni pravilen, saj algoritmi strojnega u\u010denja najve\u010dkrat operirajo s takimi vrednostmi - naklju\u010dna dolo\u010ditev \u0161tevilk kategorijam pa bi izra\u010dune pokvarila. Pri ra\u010dunanju razdalj bi tako razdalja med \u010drnimi in rjavimi \u010devlji bila 2, med \u010drnimi in modrimi pa le 1. To je nesmiselno, saj barv ne moremo postaviti v vrstni red. Posledi\u010dno se transformacije kategori\u010dnih atributov v \u0161tevilske atribute lotimo s pomo\u010djo kreacije indikacijskih atributov (angl. dummy attribute ). Iz enega kategori\u010dnega atributa tako nastane ve\u010d novih \u0161tevilskih atributov. Za vsako kategorijo enega kategori\u010dnega atributa tako nastane svoj \u0161tevilski atribut. \u010ce so \u010devlji lahko treh razli\u010dnih barv (\u010drni, modri, rjavi), potem nastanejo trije novi atributi: Barva (\u010drni) , Barva (modri) in Barva (rjavi) . Indikacijski atribut ima lahko le dve vrednosti: vrednost 0, \u010de kategorija ne dr\u017ei za instanco ter vrednost 1, \u010de kategorija dr\u017ei za instanco. Poglejmo si tabelo podatkov obeh \u010devljev po transformaciji kategori\u010dnih atributov v indikacijske atribute. Zdaj je primerjava mnogo bolj smiselna. Prav tako je opazno, da obstaja razlika tako v vodoodpornosti \u010devljev, kakor v barvi. Vi\u0161ina Te\u017ea Vod. Vod. Barva Barva Barva (da) (ne) (modri) (rjavi) (\u010drni) \u010cevlji A 17 231 1 0 0 0 1 \u010cevlji B 9 119 0 1 0 1 0 Razlika 8 112 1 -1 0 -1 1 Primer ra\u010dunanja razdalje z indikacijskimi atributi.","title":"Indikacijski atributi"},{"location":"pages/knjiga/05_knn/#kreacija-indikacijskih-atributov-v-pythonu","text":"Najenostavnej\u0161i postopek kreacije indikacijskih atributov je s pomo\u010djo pandas knji\u017enice. Struktura podatkov DataFrame ima \u017ee zabele\u017een tip vsakega stolpca, kar poenostavi avtomatsko transformacijo kategori\u010dnih atributov v indikacijske. Slede\u010da koda prikazuje kreacijo podatkov, iz katerih se bodo kreirali indikacijski atributi. import pandas as pd cevljiA = [ 17 , 231 , 'da' , 'rjavi' ] cevljiB = [ 9 , 119 , 'ne' , 'modri' ] cevljiC = [ 12 , 143 , 'ne' , '\u010drni' ] cevljiD = [ 8 , 112 , 'ne' , 'rjavi' ] cevljiE = [ 11 , 198 , 'da' , 'modri' ] cevljiF = [ 15 , 245 , 'da' , '\u010drni' ] # Z zdru\u017eitvijo instanc ustvarimo DataFrame podatki = pd . DataFrame ([ cevljiA , cevljiB , cevljiC , cevljiD , cevljiE , cevljiF ], columns = [ 'Vi\u0161ina' , 'Te\u017ea' , 'Vodood' , 'Barva' ], index = [ 'cevlji A' , 'cevlji B' , 'cevlji C' , 'cevlji D' , 'cevlji E' , 'cevlji F' ]) print ( podatki ) Vi\u0161ina Te\u017ea Vodood Barva cevlji A 17 231 da rjavi cevlji B 9 119 ne modri cevlji C 12 143 ne \u010drni cevlji D 8 112 ne rjavi cevlji E 11 198 da modri cevlji F 15 245 da \u010drni S pregledom vrednosti dtypes podatkov lahko ugotovimo kak\u0161nega tipa je posamezen atribut (stolpec). podatki . dtypes Vi\u0161ina int64 Te\u017ea int64 Vodood object Barva object dtype: object Atributa Vodood in Barva sta tipa object , kar pomeni, da sta obravnavana kot kategori\u010dni spremenljivki. Pred uporabo teh podatkov v procesu strojnega u\u010denja s knji\u017enico scikit-learn je potrebno spremeniti vse atribute object v \u0161tevilske. Knji\u017enica pandas ponuja metodo get_dummies() , ki pregleda podane podatke tipa DataFrame in vse atribute tipa object spremeni v indikacijske atribute. Izvorna spremenljivka se ne spremeni, temve\u010d se podatki z indikacijskimi atributi vrnejo kot rezultat metode. Pregled tipa atributov poka\u017ee, da so zdaj vsi atributi \u0161tevilskega tipa, s \u010dimer zadostimo uporabi v kombinaciji s scikit-learn knji\u017enico. # Vse kategori\u010dne (object) atribute spremenimo v indikacijske podatki_z_indikacijskimi = pd . get_dummies ( podatki ) podatki_z_indikacijskimi . dtypes Vi\u0161ina int64 Te\u017ea int64 Vodood_da uint8 Vodood_ne uint8 Barva_modri uint8 Barva_rjavi uint8 Barva_\u010drni uint8 dtype: object Pregled vsebine novih podatkov prika\u017ee novonastale indikacijske atribute. print ( podatki_z_indikacijskimi ) Vi\u0161ina Te\u017ea Vodood_da Vodovod_ne Barva_modri Barva_rjavi Barva_\u010drni cevlji A 17 231 1 0 0 1 0 cevlji B 9 119 0 1 1 0 0 cevlji C 12 143 0 1 0 0 1 cevlji D 8 112 0 1 0 1 0 cevlji E 11 198 1 0 1 0 0 cevlji F 15 245 1 0 0 0 1","title":"Kreacija indikacijskih atributov v Pythonu"},{"location":"pages/knjiga/05_knn/#skupna-razdalja","text":"Pri vmesnih izra\u010dunih razdalj \u0161e vedno ne pridemo do kon\u010dne skupne razdalje med dvema instancama. Za agregacijo vmesnih razdalj v eno mero razdalje pa lahko izberemo ve\u010d razli\u010dnih pristopov. Slede\u010da slika prikazuje ve\u010d vrst razdalj, ki bodo opisane v slede\u010dih sekcijah. Razli\u010dne razdalje med dvema to\u010dkama.","title":"Skupna razdalja"},{"location":"pages/knjiga/05_knn/#evklidska-razdalja","text":"Evklidsko razdaljo med dvema to\u010dkama v ravnini je definiral matematik Evklid. Deluje po principu Pitagorovega izreka, kjer se razdalja med dvema to\u010dkama oz. instancama ( \\(x_1\\) in \\(x_2\\) ) izra\u010duna kot koren vsote kvadratov vseh dimenzij. \\[\\begin{align*} \\begin{split} d_E\\left(x_1,x_2\\right)&=\\sqrt{\\left( x_1^{(1)}-x_2^{(1)}\\right)^2 + \\left( x_1^{(2)}-x_2^{(2)}\\right)^2 + \\dots \\left( x_1^{(l)}-x_2^{(l)}\\right)^2} \\\\ &=\\sqrt{\\sum \\nolimits _{i=1}^{l} \\left( x_1^{(i)}-x_2^{(i)}\\right)^2 } \\end{split} \\end{align*}\\] kjer je \\(l\\) \u0161tevilo atributov posamezne instance.","title":"Evklidska razdalja"},{"location":"pages/knjiga/05_knn/#mahattanska-razdalja","text":"Zelo pogosto pa nas zanima manhattanska razdalja, ki si zgled za ra\u010dunanje razdalje med dvema to\u010dkama vzame po postavitvi cest na otoku Manhattan, kot ka\u017ee slika spodaj. V ve\u010djem delu otoka so ceste postavljene vzdol\u017e otoka (avenije) in pre\u010dno po otoku (ulice). Pri ra\u010dunanju razdalje od ene to\u010dke do druge je tako potrebno v obzir vzeti dol\u017eine vseh cest med dvema to\u010dkama, pri tem pa ni mo\u017eno kraj\u0161ati poti z diagonalami. Razdalje na Manhattnu. Pri izra\u010dunu manhattanske razdalje tako ni najkraj\u0161a pot predstavljena kot diagonalna in najkraj\u0161a daljica med dvema instancama, ampak kot vsota vseh daljic, ki poteka vzdol\u017e vseh osi. \\[\\begin{align*} \\begin{split} d_M\\left(x_1,x_2\\right)&=\\left| x_1^{(1)}-x_2^{(1)}\\right| + \\left| x_1^{(2)}-x_2^{(2)}\\right| + \\dots \\left| x_1^{(l)}-x_2^{(l)}\\right| \\\\ &=\\sum \\nolimits _{i=1}^{l} \\left| x_1^{(i)}-x_2^{(i)}\\right| \\end{split} \\end{align*}\\]","title":"Mahattanska razdalja"},{"location":"pages/knjiga/05_knn/#kosinusna-razdalja","text":"Z manhattansko in evklidsko razdaljo pa pridemo do te\u017eav, ko imamo meritve, ki lahko imajo tako negativne kot pozitivne vrednosti. Primer take meritve bi bil letni zaslu\u017eek podjetja, saj je ta lahko tudi negativen (podjetje je na letni ravni imelo izgubo). Za primer vzemimo tri podjetja in njihove letne zaslu\u017eke ter spremembe dele\u017ea pokritega trga od prej\u0161njega leta, kot je na sliki. Primerjava evklidske, manhattanske in kosinusne razdalje. Tako evklidska kakor tudi manhattanska razdalja pravita, da sta podjetji A in C bli\u017eje kot pa podjetji A in B. \\[\\begin{align*} \\begin{split} d_E\\left(A,B\\right)&>d_E\\left(A,C\\right)\\\\ d_M\\left(A,B\\right)&>d_M\\left(A,C\\right)\\\\ d_C\\left(A,B\\right)&<d_M\\left(A,C\\right) \\end{split} \\end{align*}\\] To je v nasprotju z na\u0161o intuicijo: podjetje C je namre\u010d imelo izgubo v letnem prihodku in negativno spremembo pokritosti trga. Podjetji A in B pa sta imeli tako dobi\u010dek, kakor tudi se je njun dele\u017e pokritega trga pove\u010dal v primerjavi z lanskim letom. V takih situacijah sta evklidska in manhattanska razdalja neprimerni ter se uporabi kosinusna razdalja. Pri kosinusni razdalji je pomembna (1) smer vektorja posamezne instance ter (2) njegova dol\u017eina. Razliko med smerjo dveh vektorjev merimo s kotom \\(\\alpha\\) med vektorjema (instancama), ta pa je proporcionalna kosinusu kotov. Za instanci \\(x_1\\) in \\(x_2\\) velja slede\u010d izra\u010dun kosinusne razdalje. \\[\\begin{align*} \\begin{split} d_C\\left(x_1,x_2\\right)&=1- \\frac { x_1 \\cdot x_2}{||x_1|| \\cdot ||x_2||} \\\\ &=1-\\frac { \\sum _{i=1}^{l} x_1^{(i)} * x_2^{(i)}}{\\sqrt{\\sum _{i=1}^{l} x_1^{i~2}} * \\sqrt{\\sum _{i=1}^{l} x_2^{i~2}}} \\end{split} \\end{align*}\\] Izbira na\u010dina izra\u010duna razdalje je odvisna od problema. Najbolj intuitivna je vsekakor evklidska razdalja. Ve\u010dji je nabor atributov \\(l\\) , bolj je primerna manhattanska razdalja v primerjavi z evklidsko 1 . Kosinusna razdalja pa je primerna, ko nas bolj kot razdalje zanimajo podobnosti med instancami.","title":"Kosinusna razdalja"},{"location":"pages/knjiga/05_knn/#klasifikator-k-najblizjih-sosedov","text":"Pri nekaterih algoritmih klasifikacije ne nastane u\u010dni model, ampak se klasifikator odlo\u010da vsakokrat ob pogledu v \u017ee klasificirane instance. Taka vrsta u\u010denja se imenuje leno u\u010denje in algoritem klasifikacije k najbli\u017ejih sosedov (angl. k nearest neighbors ) je tipi\u010dni primer lenega algoritma u\u010denja. Sledi pregled delovanja tega algoritma k najbli\u017ejih sosedov.","title":"Klasifikator k najbli\u017ejih sosedov"},{"location":"pages/knjiga/05_knn/#delovanje-k-najblizjih-sosedov","text":"Algoritem k najbli\u017ejih sosedov ste\u010de po slede\u010dem postopku 2 , 3 . Za podano instanco \\(x_N\\) , ki jo \u017eelimo klasificirati, algoritem izra\u010duna razdalje do vseh \u017ee klasificiranih instanc. \u017de klasificirane instance razvrsti nara\u0161\u010dajo\u010de glede na razdaljo do instance \\(x_N\\) . V nadaljnji obravnavi upo\u0161teva le \\(k\\) prvih instanc v nara\u0161\u010dajo\u010dem seznamu - \\(k\\) najbli\u017ejih sosedov. Izra\u010duna pogostosti razredov iz nabora \\(k\\) najbli\u017ejih instanc kot je na sliki spodaj. Najpogostej\u0161i razred (modus) vrne kot rezultat klasifikacije instance \\(x_N\\) . \u010ce se ve\u010d razredov pojavlja z najve\u010djo pogostostjo, se izbere naklju\u010den razred izmed vseh najpogostej\u0161ih. Klasifikacija instance (moder trikotnik) s pomo\u010djo k najbli\u017ejih sosedov ob razli\u010dnih nastavitvah parametra k. Pri ra\u010dunanju razdalj je uporabljena evklidska razdalja. Sist. krvni tlak Tel. temp. Bolan Evklidska Manhattanska (mmHg) (\u00b0C) d rang d rang 82 38,0 Da 28,0 17 28,7 17 85 36,4 Da 25,1 16 27,3 16 89 36,7 Da 21,1 14 23,0 14 94 38,2 Da 16,0 11 16,5 11 95 38,2 Da 15,0 9 15,5 9 95 37,6 Ne 15,0 10 16,1 10 100 36,6 Ne 10,2 7 12,1 7 104 35,5 Ne 6,8 5 9,2 6 108 35,7 Ne 3,6 3 5,0 3 108 38,4 Da 2,0 2 2,3 2 109 39,4 Da 1,2 1 1,7 1 113 36,4 Ne 3,8 4 5,3 4 119 38,7 Da 9,0 6 9,0 5 124 37,5 Ne 14,1 8 15,2 8 128 37,8 Ne 18,0 12 18,9 12 129 38,8 Da 19,0 13 19,1 13 135 39,4 Da 25,0 15 25,7 15 140 36,6 Ne 30,1 18 32,1 18 145 36,0 Da 35,1 19 37,7 19 147 36,5 Da 37,1 20 39,2 20 U\u010dne instance. Sist. krvni tlak Tel. temp. Bolan (mmHg) (\u00b0C) 110 38,7 ? Nova instanca.","title":"Delovanje k najbli\u017ejih sosedov"},{"location":"pages/knjiga/05_knn/#primer-klasifikacije-pacientov","text":"Sledi primer izra\u010dunov algoritma k najbli\u017ejih sosedov po korakih na primeru diagnoze bolezni pacientov. Imamo u\u010dno mno\u017eico, kjer merimo sistoli\u010dni krvni tlak pacientov in njihovo telesno temperaturo. Med kartotekami imamo \u017ee zabele\u017eene podatke 20 prej\u0161njih pacientov in njihove diagnoze, ki so jih postavili zdravniki. Instance in nov pacient so prikazani v zgornji tabeli. V tabeli je prav tako podan izra\u010dun razdalj, evklidske in manhattanske. Izra\u010dun obeh razdalj med prvo instanco mno\u017eice in novo instanco pacienta poteka po slede\u010dem postopku. \\[\\begin{align*} \\begin{split} d_E\\left(x_N,x_1\\right)&=\\sqrt{\\left( 110-82\\right)^2 + \\left( 38,7 - 38,0\\right)^2}\\\\ &=\\sqrt{\\left(28\\right)^2 + \\left(0,7\\right)^2}\\\\ &=\\sqrt{784 + 0,49} =\\sqrt{784,49} = 28,0 \\\\ \\\\ d_M\\left(x_N,x_1\\right)&=\\left| 110-82\\right| + \\left| 38,7 - 38,0\\right|\\\\ &=\\left| 28\\right| + \\left| 0,7\\right|\\\\ &=28 + 0,7 =28,7 \\end{split} \\end{align*}\\] Razvidno je, da razlika v krvnem tlaku prevladuje pri izra\u010dunu razdalje. Do tega pride zaradi razli\u010dnih enot, posledica pa je, da razdalje atributov z ve\u010djimi \u0161tevilskimi vrednostmi prevladujejo nad atributi manj\u0161ih \u0161tevilskih vrednosti. \u010ce \u017eelimo prispevek atributov pri ra\u010dunanju skupne razdalje poenotiti, se je potrebno lotiti standardizacije podatkov, s \u010dimer postavimo vse meritve na enako zalogo vrednosti.","title":"Primer klasifikacije pacientov"},{"location":"pages/knjiga/05_knn/#standardizacija-podatkov","text":"Standardizacija (angl. standardization ) je postopek transformacije podatkov na tak na\u010din, da bodo ti po transformaciji imeli povpre\u010dje enako 0 in standardni odklon enak 1. Za standardizacijo podatkov \\(x\\) rabimo njihovo povpre\u010dje \\(\\mu\\) in standardni odklon \\(\\rho\\) , ki je definiran slede\u010de za vseh \\(n\\) instanc. \\[\\begin{align*} \\begin{split} {\\rho} &= \\sqrt{\\frac{(x_1-\\mu)^2+(x_1-\\mu)^2+\\dots+(x_n-\\mu)^2}{n}} \\end{split} \\end{align*}\\] Standardizirane vrednosti \\(z\\) izra\u010dunamo iz izvornih podatkov \\(x\\) slede\u010de. \\[\\begin{align*} \\begin{split} z &= \\frac{x-\\mu}{\\rho} \\end{split} \\end{align*}\\] Vsak atribut \\(x^1\\) , \\(x^2\\) , \\(\\dots\\) , \\(x^l\\) standardiziramo lo\u010deno - transformirano z njegovim povpre\u010djem in standardnim odklonom. Po standardizaciji vseh atributov (tudi indikacijskih) imajo vsi atributi povpre\u010dje v vrednosti 0 in standardni odklon 1. Razdalje posameznih atributov med instancami bodo tako v podobnem intervalu. Za standardizacijo podatkov pacientov tako potrebujemo podatke povpre\u010dja in standardnega odklona instanc, kar prikazuje spodnja ena\u010dba. \\[\\begin{align*} \\begin{split} \\mu\\left(\\text{Sistoli\u010dni krvni pritisk}\\right)&= 112,45\\\\ \\rho\\left(\\text{Sistoli\u010dni krvni pritisk}\\right)&= 19,58\\\\ \\mu\\left(\\text{Telesna temperatura}\\right)&= 37,42\\\\ \\rho\\left(\\text{Telesna temperatura}\\right)&= 1,17\\\\ \\end{split} \\end{align*}\\] Tabela prikazuje vrednosti atributov po standardizaciji. Sist. krvni tlak Tel. temp. Bolan Evklidska Manhattanska (mmHg) (\u00b0C) d rang d rang -1,55 0,49 Da 1,55 11 2,03 11 -1,40 -0,87 Da 2,34 15 3,24 17 -1,20 -0,61 Da 2,01 14 2,78 15 -0,94 0,66 Da 0,92 5 1,24 6 -0,89 0,66 Da 0,88 4 1,19 5 -0,89 0,15 Ne 1,21 8 1,70 8 -0,64 -0,70 Ne 1,86 12 2,30 13 -0,43 -1,64 Ne 2,74 19 3,03 16 -0,23 -1,47 Ne 2,56 17 2,66 14 -0,23 0,84 Da 0,28 1 0,36 1 -0,18 1,69 Da 0,60 3 0,65 3 0,03 -0,87 Ne 1,97 13 2,11 12 0,33 1,09 Da 0,46 2 0,46 2 0,59 0,07 Ne 1,25 9 1,74 9 0,79 0,32 Ne 1,20 7 1,69 7 0,85 1,18 Da 0,97 6 1,06 4 1,15 1,69 Da 1,41 10 1,87 10 1,41 -0,70 Ne 2,36 16 3,32 18 1,66 -1,21 Da 2,91 20 4,09 20 1,76 -0,78 Da 2,66 18 3,76 19 Standardizirane u\u010dne instance. Sist. krvni tlak Tel. temp. Bolan (mmHg) (\u00b0C) -0,13 1,09 ? Standardizirana nova instanca. Podatki po standardizaciji ka\u017eejo druga\u010dno sliko. Vrstni red najbli\u017ejih instanc podani instanci se spremeni. Instanca, ki je v tabeli ozna\u010dena, je pred standardizacijo bila \u010detrta najbli\u017eja podani instanci. Pred standardizacijo je namre\u010d bila razlika v temperaturi majhna v primerjavi z razliko v krvnem tlaku. Po standardizaciji pa je razlika v temperaturi mnogo ve\u010dja, kot razlika pri krvnem tlaku. To je prav, saj sta bili pred standardizacijo zalogi vrednosti obeh meritev druga\u010dni. Realni obseg telesne temperature \u010dloveka je pribli\u017eno med 35 in 41 \u00b0C, kar predstavlja maksimalno razliko 6 enot. \u0160est enot razlike pri krvnem tlaku pa ni niti pribli\u017eno realnemu obsegu sistoli\u010dnega krvnega tlaka, ki se lahko giblje v intervalu med 80 in 180 mmHg z maksimalno razliko kar 100 enot. Po standardizaciji se krvni tlak giblje med \\(-1,55\\) ter \\(1,76\\) z maksimalno razliko \\(3,31\\) . To je v skladu z obsegom telesne temperature, ki se po standardizaciji giblje med \\(-1,64\\) ter \\(1,69\\) z maksimalno razliko \\(3,33\\) . Standardizacija je le en postopek tehnike, ki jo imenujemo normalizacija podatkov . Alternative standardizaciji so min-max skaliranje , centriranje , rangiranje in drugi. Vsak pristop strojnega u\u010denja ni ob\u010dutljiv na intervale oz. skalo atributov. Med take \u0161tejemo algoritme kreacije odlo\u010ditvenih dreves in naivnega Bayesa. Dobra praksa je, da pri postopku podatkovnega rudarjenja podatke pred obdelavo vedno normaliziramo, saj se s tem izognemo morebitnemu zavajanju algoritmov. Negativna plat normalizacije podatkov pa je, da ti niso najve\u010dkrat enostavno interpretabilni. Kakor ka\u017eejo standardizirane vrednosti v zgornji tabeli, so vrednosti telesne temperature nesmiselne. To postane \u0161e posebej problemati\u010dno v primeru, ko gradimo model znanja, ki temelji na pravilih (odlo\u010ditvena pravila ali odlo\u010ditvena drevesa), saj bodo vrednosti v pravilih standardizirane.","title":"Standardizacija podatkov"},{"location":"pages/knjiga/05_knn/#standardizacija-v-pythonu","text":"Sledi primer, kjer za za\u010detek najprej pregledamo povpre\u010dne vrednosti in standardne odklone atributov pred standardizacijo. from sklearn.datasets import load_iris # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) print ( 'Povpre\u010dja pred standardizacijo:' ) print ( podatki . data . mean ( axis = 0 )) print ( 'Standardni odkloni pred standardizacijo:' ) print ( podatki . data . std ( axis = 0 )) Povpre\u010dja pred standardizacijo: sepal length (cm) 5.843333 sepal width (cm) 3.057333 petal length (cm) 3.758000 petal width (cm) 1.199333 dtype: float64 Standardni odkloni pred standardizacijo: sepal length (cm) 0.828066 sepal width (cm) 0.435866 petal length (cm) 1.765298 petal width (cm) 0.762238 dtype: float64 Sedaj pa te podatke standardiziramo in izpi\u0161emo povpre\u010dja ter standardne odklone novih vrednosti. Knji\u017enica scikit-learn ponuja kar nekaj na\u010dinov transformacije podatkov. Za standardizacijo se uporablja razred StandardScaler . S klicem metode fit se izra\u010dunata povpre\u010dje in standardni odklon iz podanih podatkov. Za transformacijo podatkov, pa se uporabi klic metode transform , kateri podamo podatke, ki jih \u017eelimo transformirati. from sklearn.preprocessing import StandardScaler # Inicializacija standardizatorja std = StandardScaler () # Izra\u010dunamo povpre\u010dja in standardne odklone atributov std . fit ( podatki . data ) # Standardizacija podatkov stand_podatki = std . transform ( podatki . data ) print ( 'Povpre\u010dja po standardizaciji:' ) print ( stand_podatki . mean ( axis = 0 )) print ( 'Standardni odkloni po standardizaciji:' ) print ( stand_podatki . std ( axis = 0 )) ovpre\u010dja po standardizaciji: -4.73695157e-16 -7.81597009e-16 -4.26325641e-16 -4.73695157e-16 Standardni odkloni po standardizaciji: 1. 1. 1. 1. Pregled rezultatov povpre\u010dij ka\u017ee, da so te zelo blizu vrednosti 0 ( \\(10^{-15}\\) ), standardni odkloni pa so enaki 1. \u010ce \u017eelimo proces izra\u010duna povpre\u010dij in standardnih odklonov ter proces transformacije podatkov zdru\u017eiti, lahko uporabimo metodo fit_transform , ki na podanih podatkih izra\u010duna vmesne vrednosti in jih vrne transformirane. stand_podatki = std . fit_transform ( podatki . data )","title":"Standardizacija v Pythonu"},{"location":"pages/knjiga/05_knn/#dolocitev-razreda-podane-instance","text":"Po izra\u010dunu razdalj in dolo\u010ditvi rangov glede na bli\u017eino do nove instance sledi dolo\u010ditev razreda (klasifikacija) nove instance. Pri tem procesu igra pomembno vlogo dolo\u010ditev vrednosti parametra \\(k\\) , ki nam pove, koliko najbli\u017ejih instanc upo\u0161tevamo pri klasifikaciji. Vseh \\(k\\) najbli\u017ejih instanc bo namre\u010d glasovalo za razred nove instance - vsaka instanca bo dala glas za razred, kateremu le-ta pripada. \u010ce je parameter \\(k=1\\) , potem vzamemo le najbli\u017ejo instanco in bo novi instanci dodeljen razred te instance. \u010ce se nave\u017eemo na primer iz tabele zgoraj, je pri izra\u010dunu obeh razdalj najbli\u017eja ista instanca - ta je razreda Da kar pomeni, da tudi novo instanco klasificiramo v razred Da . Tabeli spodaj prikazujeta rezultate klasifikacije pri razli\u010dnih nastavitvah - pri standardiziranih in nestandardiziranih podatkih, pri razli\u010dnih vrednostih \\(k\\) in pri evklidski ter manhattanski razdalji. Evklidska Manhattanska k Da Ne Rezultat Da Ne Rezultat 2 2 0 Da 2 0 Da 3 2 1 Da 2 1 Da 4 2 2 Da/Ne 2 2 Da/Ne 5 2 3 Ne 3 2 Da Klasifikacija s k najbli\u017ejih sosedov pri razli\u010dnih nastavitvah na nestandardiziranih podatkih. Evklidska Manhattanska k Da Ne Rezultat Da Ne Rezultat 2 2 0 Da 2 0 Da 3 3 0 Da 3 0 Da 4 4 0 Da 4 0 Da 5 5 0 Da 5 0 Da Klasifikacija s k najbli\u017ejih sosedov pri razli\u010dnih nastavitvah na standardiziranih podatkih. Zgornja tabela ka\u017ee zanimive rezultate. Najprej poglejmo nestandardizirane podatke. Tako pri evklidski, kot tudi pri manhattanski razdalji, se z ve\u010danjem \u0161tevila najbli\u017ejih instanc, ki se upo\u0161tevajo pri klasifikaciji, ve\u010da tudi negotovost, saj iz razreda Da pri upo\u0161tevanju le ene najbli\u017eje instance ( \\(k=1\\) ) preidemo do negotovosti, ko upo\u0161tevamo \u0161tiri najbli\u017eje instance ( \\(k=4\\) ), pa vse do spremembe odlo\u010ditve v razred Ne , ko upo\u0161tevamo pet najbli\u017ejih instanc ( \\(k=5\\) ) pri evklidski razdalji. Ti rezultati nam ka\u017eejo tudi razliko med evklidsko in manhattansko razdaljo, saj se odlo\u010ditve kon\u010dnega razreda instance ne skladajo pri \\(k=5\\) . Hkrati pa se moramo soo\u010diti \u0161e z negotovostjo pri \\(k=4\\) . Ko pridemo do neodlo\u010denega izida, se algoritem odlo\u010di za en naklju\u010den razred v vodstvu (tisti, ki ima manj\u0161i indeks), kar pa ni vedno najbolj\u0161a odlo\u010ditev. \u010ce bi ro\u010dno \u017eeleli izni\u010diti mo\u017enosti za neodlo\u010dene izide in naklju\u010dne odlo\u010ditve med njimi, bi algoritem preprosto zagnali \u0161e na drugih nastavitvah vrednosti \\(k\\) in pogledali, kak\u0161en je kon\u010den razred tedaj. Ko imamo opravek z binarno klasifikacijo (delitev instanc v dva razreda), pa se neodlo\u010denih izidov lahko znebimo z liho vrednostjo \\(k\\) . Po drugi strani pa ob pregledu rezultatov po standardizaciji vidimo ve\u010djo stabilnost, saj obstaja konsenz pri vseh nastavitvah \\(k\\) in pri obeh tipih razdalje. Ta pristop se tako izka\u017ee kot bolj robusten na manj\u0161e spremembe, pa tudi konceptualno je primernej\u0161i, saj vsi atributi instanc enakovredno vplivajo na odlo\u010ditev klasifikacije.","title":"Dolo\u010ditev razreda podane instance"},{"location":"pages/knjiga/05_knn/#uporaba-k-najblizjih-sosedov-v-pythonu","text":"Algoritem k najbli\u017ejih sosedov je v knji\u017enici scikit-learn implementiran z razredom KNeighborsClassifier . \u017de pri inicializaciji primerka tega razreda dolo\u010dimo \\(k\\) \u0161tevilo najbli\u017ejih sosedov s parametrom n_neighbors in na\u010din ra\u010dunanja razdalje s parametrom metric . from sklearn.neighbors import KNeighborsClassifier klasif = KNeighborsClassifier ( n_neighbors = 3 , metric = 'manhattan' ) klasif . fit ( X_u , y_u ) napovedi = klasif . predict ( X_t ) Pri ra\u010dunanju razdalje lahko uporabimo evklidsko razdaljo z euclidean , mahattansko z manhattan in kosinusno razdaljo s cosine . S klicem metode fit in podajo podatkov instanc X_u in njihovih razredov y_u te shranimo in bodo slu\u017eili za izra\u010dun najbli\u017ejih sosedov. Metodo predict pa kli\u010demo s podajo mno\u017eice instanc X_t , ki jih \u017eelimo klasificirati. To je tudi standardni postopek uporabe drugih algoritmov klasifikacije, regresije in gru\u010denja v knji\u017enici scikit-learn : Nastavitve definiramo v konstruktorju. Model znanja zgradimo s fit(podatki_instanc, resitve_instanc) . Model znanja uporabimo s predict(podatki_novih_instanc) . Algoritmi pa lahko imajo tudi sebi specifi\u010dne metode. V primeru k najbli\u017ejih sosedov je njemu posebna metoda kneighbors , kateri podamo instance, za katere i\u0161\u010demo najbli\u017eje sosede, ter parameter n_neighbors , s katerim povemo, koliko najbli\u017ejih sosedov i\u0161\u010demo iz nabora vseh instanc podanih \u017ee prej v metodi fit . Rezultat sta dva seznama: seznam razdalj od izbranih instanc do najbli\u017ejih sosedov v nara\u0161\u010dajo\u010dem vrstnem redu glede na razdaljo, ter seznam indeksov najbli\u017ejih instanc, ponovno od najbli\u017ejega naprej. klasif . fit ( X_u , y_u ) razdalje , sosedi = klasif . kneighbors ( nove_instance , n_neighbors = 3 ) Sledi pregled uporabe klasifikacijskega algoritma k najbli\u017ejih sosedov za namen iskanja petih najbli\u017ejih sosedov eni instanci. Najprej s slede\u010do kodo nalo\u017eimo instance podatkovne mno\u017eice Iris , iz nje izberemo instanco v vrstici z indeksom 133 ter jo odstranimo iz podatkovne mno\u017eice. from sklearn.datasets import load_iris # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Izberemo eno instanco izbrana = 133 X_izbrana = podatki . data . iloc [ izbrana ,:] y_izbrana = podatki . target . iloc [ izbrana ] # Izbrano instanco izlo\u010dimo iz ostalih podatkov X_ostali = podatki . data . drop ( izbrana , axis = 0 ) y_ostali = podatki . target . drop ( izbrana ) Izbira vrednosti iz polja poteka s pomo\u010djo klica iloc[vrstica, stolpec] , kamor podamo indeks vrstice in indeks stolpca. \u010ce \u017eelimo izbrati celotno vrsto, podamo namesto indeksa kar dvopi\u010dje : . Tako s podatki.data.iloc[:, 12] izberemo stolpec z indeksom 12, s klicem podatki.data.iloc[8, :] pa izberemo vrstico z indeksom 8. \u010ce \u017eelimo izbrati ve\u010d vrednosti, pa namesto \u0161tevila na mestu vrstice in stolpca podamo polje indeksov. S klicem podatki.data.iloc[[2, 5, 8], :] izberemo vrstice s temi indeksi. In obratno, s klicem podatki.data.iloc[:, [3, 6, 9]] se vrnejo stolpci z indeksi 3, 6 in 9. Pri izbiri vrednosti iz vektorja podamo le eno vrednost, saj ima ta le eno dimenzijo. Tako nam klic podatki.target.iloc[[2, 5, 8]] vrne podatke z indeksi 2, 5 in 8. Z metodo podatki.data.drop(izbrana, axis=0) odstranimo instanco z indeksom izbrana iz podatkov podatki.data . Parameter axis dolo\u010da, po kateri osi izbri\u0161emo podatke - \u010de je podana 0, izbri\u0161emo vrstico, \u010de pa 1, pa izbri\u0161emo stolpec. Pri izbrisu iz podatki.target parametra axis ni potrebno podati, saj so podatki v obliki vektorja, ki ima le eno dimenzijo.","title":"Uporaba k najbli\u017ejih sosedov v Pythonu"},{"location":"pages/knjiga/05_knn/#vizualizacija-podatkov","text":"S pomo\u010djo knji\u017enice seaborn lahko enostavno vizualiziramo instance. Z dvema klicema metode scatterplot se en na drugega izri\u0161eta dva grafa raztrosa. S parametroma x in y podamo podatke, ki naj so izrisani na teh oseh. import seaborn as sns x_os , y_os = 0 , 1 # Izri\u0161emo ostale instance sns . scatterplot ( x = X_ostali . iloc [:, x_os ], y = X_ostali . iloc [:, y_os ], hue = podatki . target_names [ y_ostali ], palette = 'colorblind' ) # Izri\u0161emo eno izbrano instanco sns . scatterplot ( x = [ X_izbrana . iloc [ x_os ]], y = [ X_izbrana . iloc [ y_os ]], hue = [ 'Neznan' ], style = [ 'Neznan' ], markers = { 'Neznan' : '^' }) Instance podatkovne zbirke Iris. Horizontalna x os predstavlja prvi atribut, vertikalna y os pa drugi atribut podatkovne mno\u017eice. Barve lo\u010dijo razrede instanc, s trikotnikom pa je ozna\u010dena instanca, ki jo \u017eelimo klasificirati. Pomanjkljivost grafi\u010dne predstavitve podatkov je, da lahko izri\u0161emo instance glede na omejeno \u0161tevilo njihovih atributov. V na\u0161em primeru imamo dvodimenzionalen graf, kjer smo posamezne osi dolo\u010dili s spremenljivkama x_os in y_os . Parameter hue prejme podatke, ki bodo narisane ozna\u010dbe delili glede na barvo - v na\u0161em primeru so to podatki o razredu. Parameter style pa prejme podatke, ki dolo\u010dajo stil ozna\u010dbe, ki jih definiramo s parametrom markers . Barve ozna\u010db dolo\u010damo s podajanjem teme v parameter palette .","title":"Vizualizacija podatkov"},{"location":"pages/knjiga/05_knn/#ucenje-in-uporaba-modela-znanja","text":"U\u010denje modela in napoved razreda instance na indeksu 133 poteka na slede\u010d na\u010din. Najprej s konstruktorjem dolo\u010dimo vrednost \\(k\\) na pet najbli\u017ejih sosedov po izra\u010dunu manhattanske razdalje. Temu sledi klic metode fit , kateri podamo podatke instanc X_ostali in njihove razrede y_ostali . Ker metoda predict pri\u010dakuje ve\u010d instanc, dodamo instanco X_izbrana najprej v seznam in ta seznam podamo v klic metode predict([X_izbrana]) . \u010ce X_izbrana ne bi bil vektor, ampak polje, bi klic metode potekal brez ovijanja v seznam predict(X_izbrana) . from sklearn.neighbors import KNeighborsClassifier # Inicializiramo klasifikator knn = KNeighborsClassifier ( n_neighbors = 5 , metric = 'manhattan' ) # Shranimo instance za primerjavo knn . fit ( X_ostali , y_ostali ) # Napovemo razred izbrane instance napoved = knn . predict ([ X_izbrana ]) print ( f 'KNN je napovedal, da je instanca razreda { podatki . target_names [ napoved ] } .' ) print ( f 'Ta instanca je dejansko razreda { podatki . target_names [ y_izbrana ] } .' ) KNN je napovedal, da je instanca razreda ['versicolor']. Ta instanca je dejansko razreda virginica. Klasifikator je napovedal napa\u010den razred za to instanco. Preglejmo katerih pet instanc je po izra\u010dunu manhattanske razdalje najbli\u017eje na\u0161i izbrani instanci iz vrstice z indeksom 133. # Izbrani instanci najdemo pet najbli\u017ejih sosedov razdalje , sosedi = knn . kneighbors ([ X_izbrana ], n_neighbors = 5 ) print ( f 'Pet najbli\u017ejih: { sosedi } ' ) print ( f 'Razdalje od najbli\u017ejih do izbrane: { razdalje } ' ) Pet najbli\u017ejih: [[ 72 83 123 126 54]] Razdalje od najbli\u017ejih do izbrane: [[0.5 0.5 0.6 0.7 0.7]] Pet najbli\u017ejih instanc po manhattanski razdalji so instance z indeksi 72, 83, 123, 126 in 54. Rezultati razdalj pa so kar manhattanske razdalje teh sosedov do podane instance.","title":"U\u010denje in uporaba modela znanja"},{"location":"pages/knjiga/05_knn/#vpliv-nastavitev-na-rezultate","text":"Poglejmo, kako se spremeni seznam petih najbli\u017ejih instanc, ko spreminjamo na\u010din izra\u010duna razdalje med instancami. for razdalja in [ 'euclidean' , 'manhattan' , 'cosine' ]: knn = KNeighborsClassifier ( n_neighbors = 5 , metric = razdalja ) knn . fit ( X_ostali , y_ostali ) razdalje , najblizje = knn . kneighbors ([ X_izbrana ], n_neighbors = 5 ) print ( f 'Najbli\u017eje instance po { razdalja } so { najblizje } ' ) print ( f 'Razdalje so { razdalje } ' ) Najbli\u017eje instance po euclidean so [[ 83 72 123 126 127]] Razdalje so [[0.33166248 0.36055513 0.37416574 0.43588989 0.45825757]] Najbli\u017eje instance po manhattan so [[ 72 83 123 126 54]] Razdalje so [[0.5 0.5 0.6 0.7 0.7]] Najbli\u017eje instance po cosine so [[125 129 90 131 83]] Razdalje so [[0.0001158 0.00022532 0.00033682 0.00034546 0.00039085]] Izra\u010dun petih najbli\u017ejih sosedov po evklidski in manhattanski razdalji vrne podobne rezultate. Instanca z indeksom 83 je najbli\u017eja izbrani po izra\u010dunu evklidske razdalje in je druga najbli\u017eja po manhattanski razdalji - mesto si izmenja z instanco 72. Tretje in \u010detrto mesto sta v obeh razdaljah zasedli instanci 123 in 125. Peto mesto ima v primeru evklidske razdalje instanca 127, v primeru manhattanske pa instanca 54. \u010ce sta instanci 127 in 54 druga\u010dnega razreda, lahko ta razlika vpliva na razred izbrane instance. Seznam najbli\u017ejih instanc po izra\u010dunu kosinusne razdalje pa je skorajda popolnoma druga\u010den od ostalih dveh - le instanca 83 je v vseh treh seznamih. Vse ostale \u0161tiri instance so v seznamu kosinusne razdalje druge v primerjavi s seznamoma evklidske in manhattanske razdalje. Ti rezultati nam ka\u017eejo, kako pomembna je odlo\u010ditev glede na\u010dina izra\u010duna razdalje. Slede\u010da slika ka\u017ee delitev obmo\u010dja razredov glede na na\u010din ra\u010dunanja razdalje. Pri kreaciji slike sta bila upo\u0161tevana prva dva atributa in vrednost \u0161tevila sosedov k je bila nastavljena na 5. Slika ka\u017ee, da ve\u010djih razlik med evklidsko in manhattansko razdaljo pri tej podatkovni mno\u017eici in nastavitvi k ni. Delitev obmo\u010dij je relativno jasna, z nekoliko ve\u010djim prekrivanjem v sredini, kjer so si instance razredov versicolor in virginica zelo podobne. Po drugi strani je razvidno, da delitev po izra\u010dunu glede na kosinusno razdaljo ni primerna za podano podatkovno mno\u017eico, saj sta obmo\u010dji versicolor in virginica preve\u010d prepleteni in je klasifikacija instanc v tem obmo\u010dju nestabilna. Delitev na obmo\u010dja razredov glede na na\u010din ra\u010dunanja razdalj med instancami. Poglejmo si \u0161e, kako vrednost k vpliva na kon\u010dno klasifikacijo izbrane instance. for k in [ 1 , 3 , 5 , 8 , 10 ]: knn = KNeighborsClassifier ( n_neighbors = k , metric = 'euclidean' ) knn . fit ( X_ostali , y_ostali ) napoved = knn . predict ([ X_izbrana ]) print ( f 'KNN z k= { k } je napovedal, da je izbrana instanca razreda { podatki . target_names [ napoved ] } ' ) KNN z k=1 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=3 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=5 je napovedal, da je izbrana instanca razreda ['virginica'] KNN z k=8 je napovedal, da je izbrana instanca razreda ['versicolor'] KNN z k=10 je napovedal, da je izbrana instanca razreda ['virginica'] Pri spreminjanju \u0161tevila najbli\u017ejih sosedov ne vidimo konsenza. Razred izbrane instance z indeksom 133 se namre\u010d spreminja iz (napa\u010dnega) razreda versicolor ob glasovanju enega, treh in osmih najbli\u017ejih sosedov, v (pravilen) razred virginica ob glasovanju petih ali desetih najbli\u017ejih sosedov. Ponovno je razvidno, da nastavitev igra vlogo pri napovedih algoritma. Vrednost k se najve\u010dkrat dolo\u010di po preizku\u0161anju, vsekakor pa mora biti vrednost smiselna (\u010de je k prevelik, bo v resnici algoritem vrnil najpogostej\u0161i razred). Preve\u010d optimiziranja nastavitev za namen bolj\u0161e klasifikacije ene instance je nesmiselno. Instanca indeksa 133 je bila izbrana namenoma, saj se napovedi njenega razreda zelo spreminjajo ob druga\u010dnih nastavitvah. Ve\u010dji del instanc dobi enako napoved razreda, ne glede na tip razdalje in k . To nam pove ve\u010d o tej instanci kot pa o samem algoritmu. Mogo\u010de je ta nekoliko nenavadna, ali pa je bila \u017ee v osnovi (s strani ekspertov) klasificirana napa\u010dno. Iz tega sledi, da je smiselno gledati rezultate in kakovost klasifikacije na ve\u010d instancah, ne le na eni - kaj pa \u010de je ta izbrana nenavadno. V naslednjem poglavju bomo spoznali na\u010dine ovrednotenja kakovosti klasifikacije in proces pravilne izbire instanc, s katerimi testiramo izbrani algoritem klasifikacije in njegovih nastavitev. Spodnja slika prikazuje razli\u010dna obmo\u010dja razredov glede na razli\u010dne vrednosti \u0161tevila sosedov k . Pri kreaciji slike sta bila ponovno upo\u0161tevana le prva dva atributa podatkov in ra\u010dunanje evklidskih razdalj. Iz slike je razvidno, da majhne vrednosti k prinesejo kar nekaj majhnih podro\u010dij klasifikacije. Po drugi strani pa je razvidno, da nastavitev \u0161tevila sosedov na 10 nekoliko pokvari delitev obmo\u010dja. Delitev na obmo\u010dja razredov glede na \u0161tevilo najbli\u017ejih sosedov k: 1, 3, 5, 8 in 10.","title":"Vpliv nastavitev na rezultate"},{"location":"pages/knjiga/05_knn/#vpliv-standardizacije-podatkov-na-rezultate","text":"Poglejmo si, \u010de se klasifikacija kaj spremeni, \u010de uporabimo standardizirane podatke. from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import StandardScaler # Standardiziramo podatke standardizator = StandardScaler () standardizator . fit ( X_ostali ) X_ostali_s = standardizator . transform ( X_ostali ) X_izbrana_s = standardizator . transform ([ X_izbrana ]) for razdalja in [ 'euclidean' , 'manhattan' , 'cosine' ]: for k in [ 1 , 3 , 5 ]: knn = KNeighborsClassifier ( n_neighbors = k , metric = razdalja ) knn . fit ( X_ostali_s , y_ostali ) nap = knn . predict ( X_izbrana_s ) print ( f 'KNN metric= { razdalja } , k= { k } je napovedal razred { podatki . target_names [ nap ] } ' ) KNN metric=euclidean, k=1 je napovedal razred ['versicolor'] KNN metric=euclidean, k=3 je napovedal razred ['versicolor'] KNN metric=euclidean, k=5 je napovedal razred ['versicolor'] KNN metric=manhattan, k=1 je napovedal razred ['versicolor'] KNN metric=manhattan, k=3 je napovedal razred ['versicolor'] KNN metric=manhattan, k=5 je napovedal razred ['versicolor'] KNN metric=cosine, k=1 je napovedal razred ['versicolor'] KNN metric=cosine, k=3 je napovedal razred ['virginica'] KNN metric=cosine, k=5 je napovedal razred ['versicolor'] Objekt razreda StandardScaler najprej nau\u010dimo, kaj so povpre\u010dja in standardni odkloni podatkov s pomo\u010djo klica fit . Kasneje to uporabilo za transformacijo (standardizacijo) tako ostalih podatkov X_ostali , kakor tudi izbrane instance X_izbrana . Pri tem uporabimo klic metode transform . Napovedi so mnogo bolj robustne na spreminjanje nastavitev klasifikacijskega algoritma, ko imamo opravek s standardiziranimi podatki. Namre\u010d, najpogosteje napovedan razred je bil versicolor . \u010ceprav je napoved razreda napa\u010dna, imamo raje robustne napovedi, kot pa take, ki so preve\u010d odvisne od nastavitev algoritma.","title":"Vpliv standardizacije podatkov na rezultate"},{"location":"pages/knjiga/05_knn/#enovit-primer-klasifikacije-vec-instanc","text":"Sledi enovit primer klasifikacije ve\u010d instanc iz Iris podatkovne zbirke, kjer so podatki tudi standardizirani. from sklearn.neighbors import KNeighborsClassifier from sklearn.datasets import load_iris import numpy as np from sklearn.preprocessing import StandardScaler # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Izberemo ve\u010d instanc izbrane = [ 1 , 31 , 61 , 91 , 121 ] X_izbrane = podatki . data . iloc [ izbrane ,:] y_izbrane = podatki . target . iloc [ izbrane ] # Izbrano instanco izlo\u010dimo iz ostalih podatkov X_ostali = podatki . data . drop ( izbrane , axis = 0 ) y_ostali = podatki . target . drop ( izbrane ) # Standardiziramo podatke standardizator = StandardScaler () standardizator . fit ( X_ostali ) X_ostali_s = standardizator . transform ( X_ostali ) X_izbrane_s = standardizator . transform ( X_izbrane ) # Zgradimo klasifikator in napovedmo razrede knn = KNeighborsClassifier ( n_neighbors = 3 , metric = 'euclidean' ) knn . fit ( X_ostali_s , y_ostali ) napovedi = knn . predict ( X_izbrane_s ) for i , napoved , dejansko in zip ( izbrane , napovedi , y_izbrane ): print ( f 'Instanca { i } je klasificirana kot { podatki . target_names [ napoved ] } dejansko pa je { podatki . target_names [ dejansko ] } .' ) Instanca 1 je klasificirana kot setosa dejansko pa je setosa. Instanca 31 je klasificirana kot setosa dejansko pa je setosa. Instanca 61 je klasificirana kot versicolor dejansko pa je versicolor. Instanca 91 je klasificirana kot versicolor dejansko pa je versicolor. Instanca 121 je klasificirana kot virginica dejansko pa je virginica. Aggarwal, C.C., Hinneburg, A. and Keim, D.A., 2001, January. On the surprising behavior of distance metrics in high dimensional space. In International conference on database theory (pp. 420-434). Springer, Berlin, Heidelberg. \u21a9 Aha, D.W., Kibler, D. and Albert, M.K., 1991. Instance-based learning algorithms. Machine learning, 6(1), pp.37-66. \u21a9 Goldberger, J., Hinton, G.E., Roweis, S. and Salakhutdinov, R.R., 2004. Neighbourhood components analysis. Advances in neural information processing systems, 17. \u21a9","title":"Enovit primer klasifikacije ve\u010d instanc"},{"location":"pages/knjiga/06_kakovost_klasifikacije/","text":"Kakovost klasifikacije Pri iskanju najprimernej\u0161ega klasifikatorja je potrebno evalvirati vsako izmed metod. Mno\u017eica podatkov, na podlagi katere smo model znanja zgradili, mora vsebovati tudi re\u0161itve (dejanske razrede) instanc, zato lahko pravilnost klasifikacije modela znanja preverimo kar s primerjavo napovedi in dejanskih razredov. Tak pristop ni optimalen, saj lahko vodi v prenasi\u010denje (angl. overfitting ). Pri prenasi\u010denju algoritem vrne model, ki uspe\u0161no klasificira podane podatke, vendar je kakovost klasificiranja na novih podatkih tipi\u010dno zelo slaba. Pravimo, da se je model preve\u010d prilegel u\u010dnim podatkom in ni dovolj splo\u0161en za dano problematiko. Prenasi\u010deni modeli imajo visoko stopnjo variance (angl. variance ), kar pomeni, da se preve\u010d prilagajajo majhnemu nihanju in naklju\u010dnemu \u0161umu v podatkih. Prenasi\u010denost \u010ce primerjamo prenasi\u010dene modele z u\u010denjem ljudi, bi lahko rekli, da pride do prenasi\u010denja, \u010de se ljudje nau\u010dimo snov dobesedno, ampak brez pravega razumevanja konceptov. \u010ce bi se u\u010dili mno\u017eenja \u0161tevil, bi se s prenasi\u010denjem nau\u010dili na pamet le rezultate mno\u017eenja \u0161tevil do \\(10\\) (ker so le ti v u\u010dbeniku). Ker pa nismo dojeli koncepta mno\u017eenja, ne bi poznali rezultata mno\u017eenja dveh \u0161e prej nevidenih \u0161tevil (primer: \\(12*27\\) ). Nasprotje prenasi\u010denju pa je nenasi\u010denost (angl. underfitting ). Nenasi\u010deni modeli imajo visoko pristranskost (angl. bias ). To pomeni, da je model preve\u010d splo\u0161en, saj ne zazna pomembnih relacij ali zakonitosti. Prikaz variance in pristranskosti. Nenasi\u010denost \u010ce se ponovno vrnemo na analogijo u\u010denja mno\u017eenja dveh \u0161tevil, bi pri\u0161li do nenasi\u010denosti, ko ne bi v celoti dojeli koncepta mno\u017eenja dveh \u0161tevil - znali bi mno\u017eiti dve celo\u0161tevilski pozitivni \u0161tevili, ne pa realnih \u0161tevil (primer: \\(6,4*1,34\\) ) ali celo negativnih celih \u0161tevil (primer: \\(-7 * 21\\) ). Praviloma imajo klasifikatorji z visoko pristranskostjo nizko stopnjo variance in obratno. Vedno \u017eelimo najti klasifikator s pravim ravnovesjem med pristranskostjo in varianco, a ta meja je arbitrarna in jo dolo\u010dimo sami. Delitev podatkov Da se izognemo prekomernemu prileganju podatkom, je potrebno podatke razdeliti na ve\u010d delov. Na enem izmed delov se klasifikator u\u010di (klasifikacijski algoritem zgradi klasifikacijski model), na drugih podatkih (na takih, ki niso sodelovali pri procesu u\u010denja) pa se preveri kakovost dobljenega modela. U\u010dna in testna mno\u017eica Nadaljujmo z analogijo u\u010denja mno\u017eenja dveh \u0161tevil. U\u010dno mno\u017eico dojemimo kot u\u010dno gradivo, ki ga uporabljamo za u\u010denje na izpit. \u010ce bi na izpitu dobili enake primere mno\u017eenja dveh \u0161tevil, kot smo jih \u017ee videli v u\u010dnem gradivu, bi preverjali le, \u010de imamo dober spomin, ne pa tudi, \u010de smo dejansko dojeli postopek mno\u017eenja dveh \u0161tevil. Tako naj bo izpit sestavljen iz popolnoma novih primerov mno\u017eenja dveh \u0161tevil, kar pa v na\u0161em postopku ponazorimo z lo\u010deno testno mno\u017eico. \u010ce je klasifikator dejansko izlu\u0161\u010dil vzorce iz podatkov, ne bi smel imeti te\u017eav pri klasifikaciji novih podatkov. \u010ce se je u\u010dne podatke nau\u010dil le na pamet, potem pa ne bo dobro klasificiral testnih podatkov (bo padel na izpitu). U\u010dna in testna mno\u017eica Osnovna razdelitev izvorne mno\u017eice je z delitvijo na dve podmno\u017eici: u\u010dno mno\u017eico (angl. train dataset ali learn dataset ) in testno mno\u017eico (angl. test dataset ) 1 . Na u\u010dni mno\u017eici le u\u010dimo klasifikatorje, ki pa jih nato testiramo in evalviramo le na testni mno\u017eici - podatki iz testne mno\u017eice ne smejo sodelovati pri gradnji klasifikacijskih modelov. Kljub deljenju na u\u010dno in testno mno\u017eico lahko dobimo prenasi\u010den model, a je mo\u017enost za to mnogo manj\u0161a. U\u010dno mno\u017eico \\(X\\) smo definirali \u017ee prej, testna mno\u017eica \\(X'\\) pa je definirana spodaj. \\[\\begin{align*} \\label{eq:testna_mnozica} \\begin{split} X' &= \\left\\lbrace \\left( x'_1,y'_1 \\right),\\left( x'_2,y'_2 \\right), \\dots ,\\left( x'_m,y'_m \\right) \\right\\rbrace \\\\ y'_i &\\in \\left\\lbrace razred_1, razred_2, \\dots , razred_k \\right\\rbrace \\\\ m &= \\text{\u0161tevilo instanc v testni mno\u017eici} \\\\ \\end{split} \\end{align*}\\] Klasifikacijski model vsaki testni instanci dolo\u010di razred \\(\\hat{y}\\) , ki pa ni nujno enak dejanskemu razredu testne instance \\(y'\\) - v tem primeru naredi napako pri klasifikaciji instance. Cilj je najti tak klasifikacijski model, ki pravilno klasificira \u010dim ve\u010d testnih instanc (idealno vse). Delitev na u\u010dno in testno mno\u017eico v Pythonu V kodi pa razdelimo podatke na u\u010dno in testno mno\u017eico na slede\u010d na\u010din. from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Razdelimo podatke na u\u010dne in testne X_u , X_t , y_u , y_t = train_test_split ( podatki . data , podatki . target , train_size = 0.7 , random_state = 42 ) print ( f 'Velikost u\u010dne mno\u017eice: { X_u . shape } ' ) print ( f 'Velikost u\u010dnih razredov: { y_u . shape } ' ) print ( f 'Velikost testne mno\u017eice: { X_t . shape } ' ) print ( f 'Velikost testnih razredov: { y_t . shape } ' ) Velikost u\u010dne mno\u017eice: (105, 4) Velikost u\u010dnih razredov: (105,) Velikost testne mno\u017eice: (45, 4) Velikost testnih razredov: (45,) S klicem train_test_split podane podatke razdelimo na u\u010dno in testno mno\u017eico na tak na\u010din, da zadostimo slede\u010dim kriterijem. \u010ce podamo parameter train_size in je vrednost tega celo \u0161tevilo, bo u\u010dna mno\u017eica \u0161tela prav toliko instanc. Primer: train_size=10 pove, da bo u\u010dna mno\u017eica \u0161tela to\u010dno 10 instanc. \u010ce podamo parameter train_size in je vrednost tega realno \u0161tevilo med 0 in 1, bo u\u010dna mno\u017eica v velikosti dele\u017ea izvorne podane mno\u017eice. Primer: train_size=0.8 pove, da bo u\u010dna mno\u017eica 80 % velikosti izvorne mno\u017eice. \u010ce podamo parameter test_size in je vrednost tega celo \u0161tevilo, bo testna mno\u017eica \u0161tela prav toliko instanc. Primer: test_size=10 pove, da bo testna mno\u017eica \u0161tela 10 instanc. \u010ce podamo parameter test_size in je vrednost tega realno \u0161tevilo med 0 in 1, bo testna mno\u017eica v velikosti dele\u017ea izvorne podane mno\u017eice. Primer: test_size=0.2 pove, da bo testna mno\u017eica 20 % velikosti izvorne mno\u017eice. Klicu train_test_split smo podali tako izvorno mno\u017eico atributov podatki.data , kakor tudi izvorne razrede podatki.target . S tem zagotovimo, da dobimo vrnjene razdeljene tako atribute instanc (u\u010dne X_u in testne X_t ), kakor tudi re\u0161itve (u\u010dne y_u in testne y_t ). Validacijska mno\u017eica \u010ce se ukvarjamo s specifi\u010dnim problemom podatkovnega rudarjenja, se pri gradnji klasifikacijskih modelov sre\u010damo z optimizacijo parametrov klasifikatorja (na primer definiranja \u0161tevila sosedov \\(k\\) pri klasifikatorju \\(k\\) najbli\u017ejih sosedov). \u010ce parametre prilagajamo glede na rezultate klasifikatorjev na testni mno\u017eici, delamo napako, saj tako testni podatki sodelujejo pri gradnji optimalnega modela. V tem primeru potrebujemo \u0161e validacijsko mno\u017eico (angl. validation dataset ), na podlagi katere preizku\u0161amo in optimiziramo parametre, \u0161ele na koncu pa zgrajen model testiramo na testni mno\u017eici 2 . Slika spodaj prikazuje razdelitev izvorne podatkovne mno\u017eice na te tri dele: u\u010dno mno\u017eico, validacijsko mno\u017eico in testno mno\u017eico. Iz procesa je razvidno, da se nastali modeli znanja najprej preizkusijo na validacijski mno\u017eici ter se njihova kakovost ovrednoti na tej mno\u017eici. \u0160ele izbrani (po navadi najbolj\u0161i) model se uporabi za kon\u010dno ovrednotenje na testni mno\u017eici. Delitev mno\u017eice na u\u010dno, validacijsko in testno ter njihova uporaba. Delitev na u\u010dno, validacijsko in testno mno\u017eico v Pythonu V paketu scikit-learn ne obstaja klic, s katerim bi dosegli enostavno delitev na tri dele, zato je potrebno, da smo nekoliko zviti. V prvem klicu train_test_split smo pridobili testno mno\u017eico X_t , y_t ter za\u010dasno u\u010dno mno\u017eico X_u1 , y_u1 . Razmerje med njima je \\(4:1\\) v prid za\u010dasne u\u010dne mno\u017eice. V drugem klicu train_test_split pa za\u010dasno u\u010dno mno\u017eico razdelimo na dejansko u\u010dno X_u , y_u ter validacijsko mno\u017eico X_v , y_v v razmerju \\(3:1\\) . Kon\u010dno razmerje med u\u010dno mno\u017eico, validacijsko mno\u017eico in testno mno\u017eico je tako \\(3:1:1\\) . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Razdelimo podatke na 'za\u010dasno u\u010dne' in testne X_u1 , X_t , y_u1 , y_t = train_test_split ( podatki . data , podatki . target , train_size = 0.8 , random_state = 42 ) # 'Za\u010dasno u\u010dne' podatke razdelimo na 'dejanske u\u010dne' in validacijske X_u , X_v , y_u , y_v = train_test_split ( X_u1 , y_u1 , train_size = 0.75 , random_state = 42 ) Poglejmo kak\u0161ne so mno\u017eice po razdelitvi. print ( f 'Velikost u\u010dne mno\u017eice: { X_u . shape } ' ) print ( f 'Velikost u\u010dnih razredov: { y_u . shape } ' ) print ( f 'Velikost validacijske mno\u017eice: { X_v . shape } ' ) print ( f 'Velikost validacijskih razredov: { y_v . shape } ' ) print ( f 'Velikost testne mno\u017eice: { X_t . shape } ' ) print ( f 'Velikost testnih razredov: { y_t . shape } ' ) Velikost u\u010dne mno\u017eice: (90, 4) Velikost u\u010dnih razredov: (90,) Velikost validacijske mno\u017eice: (30, 4) Velikost validacijskih razredov: (30,) Velikost testne mno\u017eice: (30, 4) Velikost testnih razredov: (30,) Navzkri\u017ena validacija Pri deljenju podatkov na u\u010dno, testno in validacijsko mno\u017eico pa lahko zgolj po naklju\u010dju razdelimo podatke v u\u010dno, validacijsko in testno mno\u017eico, tako da se zgodi ena izmed slede\u010dih stvari: kak\u0161en vzorec ni prisoten (v celoti ali v dovolj veliki meri) v u\u010dni mno\u017eici in pri u\u010denju klasifikatorja ta ni zajet v modelu; se kak\u0161en, v realnosti neprisoten, vzorec poka\u017ee v u\u010dni mno\u017eici in se ta zapi\u0161e v model klasifikacije, v realnosti pa ni uporaben. Navzkri\u017ena validacija Nadaljujemo z analogijo u\u010denja mno\u017eenja \u0161tevil. Pri u\u010denju matemati\u010dne naloge (z re\u0161itvami) razdelimo na dva dela: tiste primere, na katerih se u\u010dimo (u\u010dno mno\u017eico) in tiste, na katerih se pred izpitom preverimo, \u010de vemo dovolj (validacijska mno\u017eica; izpit bo pa testna mno\u017eica). \u010ce smo po naklju\u010dju razdelili naloge mno\u017eenja tako, da se u\u010dimo le iz najla\u017ejih (recimo le mno\u017eenje celih pozitivnih \u0161tevil), dolo\u010denega naprednega znanja ne bomo osvojili (mno\u017eenja negativnih ali realnih \u0161tevil). Da zmanj\u0161amo mo\u017enost za ponesre\u010deno razdelitev mno\u017eice, to razdelimo na ve\u010d na\u010dinov - na \\(n\\) na\u010dinov. Tako iz ene mno\u017eice dobimo \\(n\\) u\u010dnih mno\u017eic in \\(n\\) testnih mno\u017eic. Ta proces imenujemo navzkri\u017ena validacija (angl. cross-validation ) in ste\u010de tako, da celotno mno\u017eico razdelimo na \\(n\\) delov - na \\(n\\) rezov (angl. folds ) 3 . Vsak rez je enkrat v vlogi testne mno\u017eice, v vseh ostalih primerih pa je v kombinaciji z ostalimi rezi del u\u010dne mno\u017eice. Posledi\u010dno je tudi vsaka instanca enkrat v testni mno\u017eici, v ostalih primerih pa je vedno v u\u010dni mno\u017eici. Slede\u010da slika prikazuje razdelitev celotne mno\u017eice na \u0161tiri reze za namen navzkri\u017ene validacije. Stratificirana navzkri\u017ena validacija. Vertikalna Y os prikazuje pripadnost instanc bodisi v u\u010dno (modra) ali testno (rde\u010da) mno\u017eico pri posameznem rezu. Pri naklju\u010dni delitvi na reze se zna zgoditi, da bodo razmerja med razredi razli\u010dna med rezi. Tega se \u017eelimo izogniti, saj lahko nastane situacija, ko so vse instance enega razreda le v enem rezu. Ko bo ta rez v vlogi testnih instanc, klasifikacijski algoritem nima mo\u017enosti, da pravilno klasificira instance tega manj\u0161inskega razreda - saj jih nikoli ne vidi v \u010dasu u\u010denja. S postopkom stratifikacije (angl. stratification ) poskrbimo, da se mno\u017eica preme\u0161a na tak na\u010din, da so razmerja razredov v vseh rezih kar se da enaka. V Pythonu razdelimo mno\u017eico s stratificirano navzkri\u017eno validacijo slede\u010de. from sklearn.datasets import load_iris from sklearn.model_selection import StratifiedKFold # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Stratificirana navzkri\u017ena validacija s \u0161tirimi rezi skf = StratifiedKFold ( n_splits = 4 ) rez = 1 # S tem bomo \u0161teli reze instanca = 13 # Pregledovali bomo instanco na mestu 13 print ( f 'Zanima nas instanca { instanca } s podatki:' ) print ( podatki . data . iloc [ instanca ,:]) for ucne , testne in skf . split ( podatki . data , podatki . target ): if instanca in ucne : print ( f 'Instanca { instanca } je v { rez } . delitvi v u\u010dni mno\u017eici.' ) elif instanca in testne : print ( f 'Instanca { instanca } je v { rez } . delitvi v testni mno\u017eici.' ) rez = rez + 1 Zanima nas instanca 13 s podatki: sepal length (cm) 4.3 sepal width (cm) 3.0 petal length (cm) 1.1 petal width (cm) 0.1 Name: 13, dtype: float64 Instanca 13 je v 1. delitvi v u\u010dni mno\u017eici. Instanca 13 je v 2. delitvi v testni mno\u017eici. Instanca 13 je v 3. delitvi v u\u010dni mno\u017eici. Instanca 13 je v 4. delitvi v u\u010dni mno\u017eici. Z uporabo razreda StratifiedKFold in njegove metode split razdelimo podane podatke na reze. Z vrednostjo n_splits ob inicializaciji podamo, na koliko rezov delimo mno\u017eico. V for zanki tako na vsaki u\u010dni mno\u017eici nau\u010dimo svoj model klasifikacije, ki ga evalviramo na testni mno\u017eici tistega reza. Tako dobimo \\(n\\) klasifikacijskih modelov, vsak se je nau\u010dil na nekoliko druga\u010dni u\u010dni mno\u017eici in vsak se je evalviral na popolnoma druga\u010dni testni mno\u017eici. Iz tega sledi, da dobimo tudi \\(n\\) rezultatov klasifikacije na testnih mno\u017eicah. Da pa ocenimo celokupno kakovost podanega klasifikacijskega algoritma, pa vse rezultate preprosto povpre\u010dimo. Sledi poglavje, ki pregleda, kako sploh merimo kakovost klasifikacije, razdeljeno glede na \u0161tevilo razredov, v katere model klasifikacije razvr\u0161\u010da instance. Metrike klasifikacije Ko imamo napovedi modela, lahko ovrednotimo oz. evalviramo kakovost te napovedi. To naredimo s pomo\u010djo klasifikacijskih metrik (angl. classification metrics ) 4 , 5 , 6 , 7 . Sledi pregled razli\u010dnih klasifikacijskih metrik, najprej, ko imamo opravka s klasifikacijo v dva razreda (dvorazredna ali binarna klasifikacija) in nato, ko imamo opravka z ve\u010d kot dvema razredoma (ve\u010drazredna klasifikacija). Binarna klasifikacija Pri binarnih klasifikatorjih gradimo model za klasifikacijo instanc v dva razreda. Ko imamo model zgrajen, ga ocenimo s pomo\u010djo testnih podatkov in iz rezultatov zgradimo matriko zmede ali kontingen\u010dno tabelo (angl. confusion matrix ali contingency table ), ki je prikazana v spodnji tabeli. Napovadan A Napovedan B Dejanski A TP FN Dejanski B FP TN Matrika zmede za dva razreda. V danem primeru klasificiramo podatke v dva razreda: A in B. Vsaka celica v tabeli prikazuje \u0161tevilo instanc. Vrstice matrike ka\u017eejo dejansko pripadnost instanc v razrede, stolpci pa povedo razporeditev instanc v razrede s pomo\u010djo klasifikatorja. Enemu razredu pripi\u0161emo lastnost pozitivnega razreda, drugemu pa lastnost negativnega razreda. Ta dolo\u010ditev je arbitrarna in je odvisna od problematike re\u0161evanja - pravilne dolo\u010ditve ni in bi tudi druga delitev v pozitivne in negativne vrnila prave rezultate. from sklearn.metrics import confusion_matrix # Podamo dejanske razrede in napovedi klasifikatorja y_dejanski = [ 'A' , 'A' , 'B' , 'B' , 'A' , 'B' ] y_napovedan = [ 'A' , 'A' , 'B' , 'A' , 'A' , 'B' ] # Izra\u010dunamo matriko zmede mat_zmede = confusion_matrix ( y_dejanski , y_napovedan , labels = [ 'A' , 'B' ]) print ( mat_zmede ) [[3 0] [1 2]] Klicu confusion_matrix podamo vsaj dve vrednosti: polje dejanskih razredov in polje napovedi. Pomembno je, da so elementi v obeh poljih v enakem vrstnem redu (prvi element v obeh poljih ka\u017ee tako dejanski razred kot napoved prvega elementa). S parametrom labels pa podamo vrstni red razredov pri kreaciji matrike zmede - \u010de tega ne podamo, bo vrstni red razredov po abecednem vrstnem redu. V primeru iz kode vidimo, da je klasifikacijski model za tri instance razreda A povedal, da so res razreda A; za dve instanci razreda B je povedal, da sta res razreda B; ter za eno instanco razreda B je napa\u010dno povedal, da je ta razreda A. \u010ce privzamemo, da je razred A pozitiven, matrika zmede binarne klasifikacije vsebuje \u0161tiri vrednosti, ki ka\u017eejo koli\u010dino instanc glede na rezultate klasifikacije: TP - pravilno klasificirane kot pozitivne (angl. true positives ), FP - napa\u010dno klasificirane kot pozitivne (angl. false positives ), TN - pravilno klasificirane kot negativne (angl. true negatives ), FN - napa\u010dno klasificirane kot negativne (angl. false negatives ). Raz\u0161irimo prej\u0161nji primer z naslednjo kodo, ki nam vrne te \u0161tiri vrednosti (deluje le pri binarni klasifikaciji). Klicu metode confusion_matrix dodamo \u0161e klic metode ravel , ki dobljeno metriko zlo\u017ei v enodimenzionalno polje (najprej vse iz prve vrstice, nato vse iz druge vrstice ...). # Izra\u010dunamo vrednosti napovedi tp , fn , fp , tn = confusion_matrix ( y_dejanski , y_napovedan , labels = [ 'A' , 'B' ]) . ravel () print ( f 'TP: { tp } ' ) print ( f 'TN: { tn } ' ) print ( f 'FP: { fp } ' ) print ( f 'FN: { fn } ' ) TP: 3 TN: 2 FP: 1 FN: 0 S pomo\u010djo teh \u0161tirih vrednosti lahko izra\u010dunamo razli\u010dne metrike klasifikacije (angl. classification metrics ) oziroma vrednosti, ki nam povedo, kako kakovosten je klasifikacijski model. Teh metrik je ve\u010d in vsaka slu\u017ei svojemu namenu - obstajajo tudi razli\u010dni pogledi, kaj sploh pomeni kakovosten model znanja. To\u010dnost klasifikacije in dele\u017e napake Prva taka metrika je to\u010dnost klasifikacije (angl. accuracy ), ki prikazuje, kolik\u0161en dele\u017een instanc je klasificiran v dejanski razred - dele\u017e pravilno klasificiranih instanc. Formula to\u010dnosti je prikazana v slede\u010di ena\u010dbi in se izra\u010duna kot ulomek \u0161tevila pravilno klasificiranih instanc ( \\(TP + TN\\) ) deljeno s \u0161tevilom vseh klasificiranih instanc ( \\(TP + FP + TN + FN\\) ). Metriko to\u010dnost \u017eelimo maksimirati in te\u017eimo k temu, da je \u010dim bli\u017eje vrednosti 1 8 . \\[\\begin{equation*} to\\check{c}nost = \\frac{TP + TN}{TP + FP + TN + FN} \\end{equation*}\\] Metrika, ki ka\u017ee nasprotno vrednost to\u010dnosti, je dele\u017e napake (angl. error rate ); pravimo tudi, da je obratno sorazmerna kot to\u010dnost. Predstavlja dele\u017e nepravilno klasificiranih instanc, kar je prikazano v spodnji ena\u010dbi. Lahko pa dele\u017e napake izra\u010dunamo tudi preprosto kot \\(1 - to\\check{c}nost\\) . Dele\u017e napake je metrika, ki jo minimiziramo in katere idealna vrednost je \\(0\\) . \\[\\begin{equation*} dele\\check{z}\\:napake = \\frac{FP + FN}{TP + FP + TN + FN} \\end{equation*}\\] Prika\u017eimo \u0161e izra\u010dun to\u010dnosti, dele\u017ea napake in \u0161tevila pravilno klasificiranih instanc v Pythonu s klicem accuracy_score . from sklearn.metrics import accuracy_score # Podamo dejanske razrede in napovedi klasifikatorja y_dejanski = [ 'A' , 'A' , 'B' , 'B' , 'A' , 'B' ] y_napovedan = [ 'A' , 'A' , 'B' , 'A' , 'A' , 'B' ] # Izra\u010dunamo to\u010dnost tocnost = accuracy_score ( y_dejanski , y_napovedan ) delez_napake = 1 - tocnost # Izra\u010dunamo \u0161tevilo pravilno klasificiranih (TP+TN) st_pravilnih = accuracy_score ( y_dejanski , y_napovedan , normalize = False ) print ( f 'To\u010dnost: { tocnost } ' ) print ( f 'Dele\u017e napake: { delez_napake } ' ) print ( f '\u0160tevilo pravilnih: { st_pravilnih } ' ) To\u010dnost: 0.8333333333333334 Dele\u017e napake: 0.16666666666666663 \u0160tevilo pravilnih: 5 Priklic in preciznost Naslednji dve metriki, ki ju bomo obravnavali skupaj, sta priklic in preciznost. Obe metriki imata vrednost v intervalu \\([0,1]\\) , kjer je 0 najslab\u0161a vrednost, 1 pa najbolj\u0161a. Metrika priklic (angl. recall ) nam pove dele\u017e pozitivnih instanc, ki so pravilno klasificirane v pozitivni razred. V\u010dasih priklic imenujemo tudi senzitivnost (angl. sensitivity ) ali dele\u017e pravilno klasificiranih pozitivnih instanc (angl. true positive rate ). Izra\u010dun priklica je prikazan v slede\u010di ena\u010dbi. \\[\\begin{equation*} priklic = \\frac{TP}{TP + FN} \\end{equation*}\\] Poglejmo si uporabo priklica v Pythonu. from sklearn.metrics import recall_score # Podamo dejanske razrede in napovedi klasifikatorja y_dejanski = [ 'A' , 'A' , 'B' , 'B' , 'A' , 'B' ] y_napovedan = [ 'A' , 'A' , 'B' , 'A' , 'A' , 'B' ] # Izra\u010dunamo priklic obeh razredov priklic_A = recall_score ( y_dejanski , y_napovedan , pos_label = 'A' ) priklic_B = recall_score ( y_dejanski , y_napovedan , pos_label = 'B' ) print ( f 'Priklic A: { priklic_A } ' ) print ( f 'Priklic B: { priklic_B } ' ) Priklic A: 1.0 Priklic B: 0.6666666666666666 Podobno kot pri izra\u010dunu to\u010dnosti, tudi pri klicu metode recall_score podamo najprej polje z dejanskimi razredi, \u010demur pa sledi polje z napovedanimi razredi. Pri binarni klasifikaciji moramo podati tudi parameter pos_label , s katerim dolo\u010dimo razred, za katerega ra\u010dunamo priklic. Priklic oz. senzitivnost testa O senzitivnosti (ali ob\u010dutljivosti) testa v praksi zelo pogosto govorimo, ko imamo opravka s testom, ki ugotavlja prisotnost ali odsotnost dolo\u010denega obolenja (npr. bolezni, ki jo povzro\u010da virus). Senzitivnost takega testa nam pove kak\u0161en je dele\u017e bolnih, ki jih je test na\u0161el. Predstavljajmo si primer, ko imamo 100 pacientov, ki so dejansko bolni. Test je za dejansko bolne pokazal, da jih je 80 bolnih, za ostalih 20 pa je test (napa\u010dno) trdil, da niso bolni. Tak test ima \\(80 %\\) senzitivnost oz. priklic. Druga metrika je preciznost (angl. precision in nam pove, kolik\u0161en dele\u017e instanc, klasificiranih v pozitivni razred, je dejansko pripadnikov pozitivnega razreda. V\u010dasih metriko priklic imenujemo zaupanje (angl. confidence ) ali to\u010dnost pozitivno klasificiranih pozitivnih instanc (angl. true positive accuracy ). \\[\\begin{equation*} preciznost = \\frac{TP}{TP + FP} \\end{equation*}\\] Slede\u010da koda prikazuje uporabo klica precision_score . from sklearn.metrics import precision_score # Izra\u010dunamo preciznost obeh razredov preciznost_A = precision_score ( y_dejanski , y_napovedan , pos_label = 'A' ) preciznost_B = precision_score ( y_dejanski , y_napovedan , pos_label = 'B' ) print ( f 'Preciznost A: { preciznost_A } ' ) print ( f 'Preciznost B: { preciznost_B } ' ) Preciznost A: 0.75 Preciznost B: 1.0 Ponovno se vrnimo na primer testa za ugotavljanje dolo\u010denega obolenja. Imamo 100 pacientov, ki so dejansko bolni. Test je za 80 izmed teh pokazal, da so bolni, za \u0161e 10 dodatnih (dejansko zdravih) pacientov pa je prav tako (napa\u010dno) pokazal, da so bolni. Tak test ima \\(88,89 %\\) preciznosti oz. zaupanja. F-mera Iz prej\u0161njih sekcij je razvidno, da obstaja nabor metrik za merjenje kakovosti binarne klasifikacije, od katerih ima vsaka svoj namen. Ko \u017eelimo meriti kakovost klasifikacijskega modela v splo\u0161nem, pa uporaba ve\u010djega \u0161tevila metrik ni prakti\u010dna. V ta namen uporabljamo metriko, imenovano F-mera (angl. F-score ali F-measure ), ki zdru\u017ei metriki priklic in preciznost v harmoni\u010dni sredini, kot je prikazano v ena\u010dbi spodaj. Metriki preciznost in priklic nista neposredno povezani in prikazujeta razli\u010dne informacije. Visoka vrednost ene ne pomeni nujno nizke vrednosti druge, zato stremimo k temu, da bi obe metriki bili dovolj visoki. Prednost F-mere je, da zdru\u017ei obe omenjeni metriki skupaj v eno \u0161tevilo, kjer se takoj opazi nizka vrednost katerekoli izmed njiju. \\[\\begin{align*} \\begin{split} F\\text{-}mera_\\beta &= (1 + \\beta^2) \\cdot \\frac{preciznost \\cdot priklic}{(\\beta^2 \\cdot preciznost) + priklic} \\\\[.3cm] &= \\frac{(1 + \\beta^2) \\cdot TP}{(1 + \\beta^2) \\cdot TP + \\beta^2 \\cdot FN + FP} \\end{split} \\end{align*}\\] Ta ena\u010dba prikazuje splo\u0161no obliko F-mere, kjer lahko posamezno metriko (preciznost ali priklic) ute\u017eimo pri kon\u010dnem izra\u010dunu. Vlogo ute\u017ei igra vrednost \\(\\beta\\) , ki je vi\u0161ja pri ve\u010dji pomembnosti metrike preciznost in ni\u017eja, ko damo ve\u010dji poudarek metriki priklica. Tradicionalna oblika metrike F-mera je, ko sta obe vrednosti enako ute\u017eeni in imata uravnote\u017eeno vlogo pri izra\u010dunu (ko je \\(\\beta=1\\) ). Najve\u010dkrat se uporablja prav ta, ki se v\u010dasih imenuje tudi F \\(_{1}\\) -mera in je prikazana s slede\u010do ena\u010dbo. \\[\\begin{equation*} F_1\\text{-}mera = 2 \\cdot \\frac{preciznost \\cdot priklic}{preciznost + priklic} \\end{equation*}\\] \u010ce uporabljamo pri izra\u010dunu F-mere vrednost \\(\\beta\\) , ki ni enaka \\(1\\) , se to zapi\u0161e kot podpisana vrednost (F \\(_2\\) -mera ali F \\(_{0,5}\\) -mera). \u010ce pa je \\(\\beta\\) enak vrednosti \\(1\\) , lahko podpisano vrednost izpustimo in zapi\u0161emo le F-mera. Slede\u010da Python koda prikazuje izra\u010dun F-mere z in brez podane \\(\\beta\\) vrednosti. Koda raz\u0161irja prej\u0161nje primere, kjer so dejanski in napovedani razredi \u017ee definirani. from sklearn.metrics import f1_score , fbeta_score # Izra\u010dunamo F-mero in F-meta (s podano beto) razreda A f1_score_A = f1_score ( y_dejanski , y_napovedan , pos_label = 'A' ) f_beta_score_A = fbeta_score ( y_dejanski , y_napovedan , pos_label = 'A' , beta = 2 ) print ( f 'F1-mera A: { f1_score_A } ' ) print ( f 'F-mera (beta=2) A: { f_beta_score_A } ' ) F1-mera A: 0.8571428571428571 F-mera (beta=2) B: 0.9375 Klasifikacija v ve\u010d razredov V prej\u0161nji sekciji smo naredili pregled metrik za ocenjevanje kakovosti modelov klasifikacije v dva razreda. Mnogokrat pa imajo podatki ve\u010d mo\u017enih razredov, zato evalvacija teh modelov z metrikami, neprilagojenimi za ve\u010d razredov, postane te\u017eavna. Kreacija matrike zmede poteka po enakem postopku kot pri binarni klasifikaciji, kjer v vrstice bele\u017eimo dejanske, v stolpce pa napovedane razrede instanc, kot ka\u017ee slede\u010da tabela. Napovadan A Napovedan B Napovedan C Dejanski A 50 1 2 Dejanski B 4 20 0 Dejanski C 5 1 10 Matrika zmede za ve\u010d razredov. Najpreprostej\u0161a metrika za izra\u010dun ne glede na \u0161tevilo razredov je to\u010dnost, saj \u0161e vedno velja, da \u0161tevilo pravilno klasificiranih instanc delimo s \u0161tevilom vseh instanc. Podobno velja tudi za dele\u017e napake, le da \u0161tevilo pravilno klasificiranih zamenjamo s \u0161tevilom nepravilno klasificiranih primerkov. Te\u017eava nastopi pri poimenovanju pozitivnih in negativnih razredov, saj smo prisiljeni oznako pozitivni ali negativni dodeliti ve\u010d razredom. Vse metrike, ki vsebujejo \u0161tevilo pozitivnih ali negativnih instanc, se morajo prilagoditi uporabi klasifikacije v ve\u010d razredov na slede\u010d na\u010din. Te metrike ra\u010dunamo v ve\u010d korakih, kjer v vsakem koraku dobi naziv pozitivnega razreda drug razred, ostali pa predstavljajo negativni razred. Kon\u010dno vrednost metrik dobimo na ve\u010d na\u010dinov: z makro agregacijo, z ute\u017eenim povpre\u010denjem , ali z mikro agregacijo 9 . Makro agregacija metrik Pri makro agregaciji metrik klasifikacije izdelamo ve\u010d matrik zmede ( \\(m\\) matrik zmede, \u010de imamo \\(m\\) razredov), in sicer tako, da je v vsaki metriki zmede drug razred ozna\u010den kot pozitivni razred. Za vsako matriko zmede izra\u010dunamo izbrano metriko (dobimo \\(m\\) metrik klasifikacije). Kon\u010dno vrednost metrike klasifikacije pa na koncu izra\u010dunamo tako, da povpre\u010dimo vseh \\(m\\) metrik. Slede\u010de ena\u010dbe prikazujejo makro ( \\(Ma\\) ) izra\u010dun nekaterih metrik klasifikacije. \\[\\begin{align*} priklic_{Ma} &= \\frac{\\sum_{i=1}^{m} \\frac{TP_i}{TP_i + FN_i}}{m} = \\frac{\\sum_{i=1}^{m} priklic_{i}}{m} \\\\[.2cm] preciznost_{Ma} &= \\frac{\\sum_{i=1}^{m} \\frac{TP_i}{TP_i + FP_i}}{m} = \\frac{\\sum_{i=1}^{m} preciznost_{i}}{m} \\\\[.2cm] F_1\\text{-}mera_{Ma} &= \\frac{\\sum_{i=1}^{m} F_1\\text{-}mera_{i}}{m} \\end{align*}\\] \u010ceprav se metrika to\u010dnost izra\u010duna za m razredov enako kot za binarni problem klasifikacije, pa lahko izra\u010dunamo metriko povpre\u010dna to\u010dnost (angl. average class accuracy ). Ta metrika izra\u010duna najprej to\u010dnost za vsak posamezni razred, nato pa povpre\u010di izra\u010dunane vrednosti. Po enakem postopku izra\u010dunamo tudi povpre\u010dno stopnjo napake (angl. average class error rate ), le da povpre\u010dimo dele\u017ee napak razredov. \\[\\begin{align*} povpre\\check{c}na\\:to\\check{c}nost &= \\frac{\\sum_{i=1}^{m} \\frac{TP_i+TN_i}{TP_i + FP_i + TN_i + FN_i}}{m} \\\\[.2cm] povpre\\check{c}en\\:dele\\check{z}\\:napake &= \\frac{\\sum_{i=1}^{m} \\frac{FP_i+FN_i}{TP_i + FP_i + TN_i + FN_i}}{m} \\end{align*}\\] Ute\u017eena agregacija metrik Preprosto povpre\u010denje metrik vsakega razreda najve\u010dkrat ni smiselno. Ko imamo neuravnote\u017eeno mno\u017eico (tj. mno\u017eico, ko je razmerje med razredi zelo neenakomerno - \u010dez \\(8:1\\) ), povpre\u010denje kakovosti klasifikacije instanc pogostega razreda in kakovosti instanc redkih razredov ni smiselno. Namre\u010d, ve\u010d instanc imamo, ve\u010d mo\u017enosti ima klasifikacijski algoritem, da izlu\u0161\u010di vzorce, ki so zna\u010dilni za instance tega razreda. To privede do klasifikacijskih modelov, ki delujejo mnogo bolj\u0161e pri klasifikaciji instanc bolj pogostega razreda v primerjavi s klasifikacijo instanc redkih razredov. \u010ce kakovost klasifikacije povpre\u010dimo (kot je to pri makro agregaciji), kakovost klasifikacije vseh razredov enakomerno prispeva h kon\u010dni metriki. To je odli\u010dno, \u010de \u017eelimo ve\u010dji poudarek na kakovosti manj\u0161inskih instanc. \u010ce pa ne \u017eelimo, da instance v manj\u0161ini prevzamejo neenakomerno ve\u010djo vlogo pri ocenjevanju kakovosti, pa lahko posamezne metrike pred povpre\u010denjem ute\u017eimo. Pri ute\u017eevanju metriko posameznega razreda \\(i\\) zmno\u017eimo z dele\u017eem instanc \\(u_i\\) razreda \\(i\\) , zmno\u017eene metrike pa nato se\u0161tejemo v skupno mero. Dele\u017e razreda \\(u_i\\) je preprost odstotek instanc tega razreda \\(i\\) v testni mno\u017eici. \\[\\begin{align*} priklic_{Ua} &= \\sum_{i=1}^{m} u_i \\cdot \\frac{TP_i}{TP_i + FN_i} = \\sum_{i=1}^{m} u_i \\cdot priklic_{i} \\\\[.2cm] preciznost_{Ua} &= \\sum_{i=1}^{m} u_i \\cdot \\frac{TP_i}{TP_i + FP_i} = \\sum_{i=1}^{m} u_i \\cdot preciznost_{i} \\\\[.2cm] F_1\\text{-}mera_{Ua} &= \\sum_{i=1}^{m} u_i \\cdot F_1\\text{-}mera_{i} \\end{align*}\\] Mikro agregacija metrik Pri mikro agregaciji metrik za\u010dnemo podobno kot pri makro agregaciji; zgradimo \\(m\\) matrik zmede, nadaljnji postopek pa je druga\u010den. Namesto povpre\u010denja razli\u010dnih metrik se\u0161tejemo posamezne vrednosti iz matrik zmede. Primer: vse \\(TP_i\\) se\u0161tejemo skupaj, da dobimo \\(TP_{Mi}\\) , in tega vstavimo v ena\u010dbo za izra\u010dun mikro agregiranih metrik. Ena\u010dbe spodaj prikazujejo na\u010din izra\u010duna posameznih metrik na mikro ( \\(Mi\\) ) na\u010din. \\[\\begin{align*} priklic_{Mi} &= \\frac{\\sum_{i=1}^{m} TP_i}{\\sum_{i=1}^{m} (TP_i + FN_i)} \\\\[.2cm] preciznost_{Mi} &= \\frac{\\sum_{i=1}^{m} TP_i}{\\sum_{i=1}^{m} (TP_i + FP_i)} \\\\[.2cm] F_1\\text{-}mera_{Mi} &= 2 \\cdot \\frac{preciznost_{Mi} \\cdot priklic_{Mi}}{preciznost_{Mi} + priklic_{Mi}} \\end{align*}\\] Posebnih mikro agregacij to\u010dnosti in dele\u017ea napake ni, saj v tem primeru dobimo \u017ee prej definirano to\u010dnost in dele\u017e napake. Agregacija metrik v Pythonu Z uporabo knji\u017enice scikit-learn in \u017ee do sedaj predstavljenih metod accuracy_score , precision_score , recall_score in f1_score enostavno agregiramo metrike vseh razredov v eno vrednost. Pri agregaciji nam slu\u017ei parameter teh metod average , s katerim podamo na\u010din agregacije. from sklearn.metrics import precision_score , recall_score , f1_score # Izra\u010dunamo preciznost z makro agregacijo preciznost_makro = precision_score ( y_dejanski , y_napovedan , average = 'macro' ) # Izra\u010dunamo ute\u017een priklic priklic_utezen = recall_score ( y_dejanski , y_napovedan , average = 'weighted' ) # Izra\u010dunamo mikro F-mero f_mera_mikro = f1_score ( y_dejanski , y_napovedan , average = 'micro' ) # Izra\u010dunamo lo\u010deno preciznost za vsak razred preciznost_vseh = precision_score ( y_dejanski , y_napovedan , average = None ) print ( f 'Makro preciznost: { preciznost_makro } ' ) print ( f 'Ute\u017een priklic: { priklic_utezen } ' ) print ( f 'Mikro F-mera: { f_mera_mikro } ' ) print ( f 'Preciznost vseh razredov: { preciznost_vseh } ' ) Makro preciznost: 0.875 Ute\u017een priklic: 0.8333333333333334 Mikro F-mera: 0.8333333333333334 Preciznost vseh razredov: [0.75 1. ] Vrednost parametra average dolo\u010da na\u010din agregacije: 'macro' se uporabi za makro agregacijo. 'weighted' se uporabi za ute\u017eeno agregacijo, kjer so ute\u017ei sestavljene iz razmerja razredov v podani testni mno\u017eici. 'micro' se uporabi za mikro agregacijo. None se uporabi, ko \u017eelimo lo\u010den izpis metrike za vsak razred posebej, brez agregacije. Lever, J., 2016. Classification evaluation: It is important to understand both what a classification metric expresses and what it hides. Nature Methods, 13(8), pp.603-605. \u21a9 Guyon, I., 1997. A scaling law for the validation-set training-set size ratio. AT&T Bell Laboratories, 1(11). \u21a9 Schaffer, C., 1993. Selecting a classification method by cross-validation. Machine Learning, 13(1), pp.135-143. \u21a9 Hossin, M. and Sulaiman, M.N., 2015. A review on evaluation metrics for data classification evaluations. International journal of data mining & knowledge management process, 5(2), p.1. \u21a9 Powers, D.M., 2020. Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation. arXiv preprint arXiv:2010.16061. \u21a9 Sokolova, M., Japkowicz, N. and Szpakowicz, S., 2006, December. Beyond accuracy, F-score and ROC: a family of discriminant measures for performance evaluation. In Australasian joint conference on artificial intelligence (pp. 1015-1021). Springer, Berlin, Heidelberg. \u21a9 Sokolova, M. and Lapalme, G., 2009. A systematic analysis of performance measures for classification tasks. Information processing & management, 45(4), pp.427-437. \u21a9 Dem\u0161ar, J., 2006. Statistical comparisons of classifiers over multiple data sets. The Journal of Machine Learning Research, 7, pp.1-30. \u21a9 Jurman, G. and Furlanello, C., 2010. A unifying view for performance measures in multi-class prediction. arXiv preprint arXiv:1008.2908. \u21a9","title":"Kakovost klasifikacije"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#kakovost-klasifikacije","text":"Pri iskanju najprimernej\u0161ega klasifikatorja je potrebno evalvirati vsako izmed metod. Mno\u017eica podatkov, na podlagi katere smo model znanja zgradili, mora vsebovati tudi re\u0161itve (dejanske razrede) instanc, zato lahko pravilnost klasifikacije modela znanja preverimo kar s primerjavo napovedi in dejanskih razredov. Tak pristop ni optimalen, saj lahko vodi v prenasi\u010denje (angl. overfitting ). Pri prenasi\u010denju algoritem vrne model, ki uspe\u0161no klasificira podane podatke, vendar je kakovost klasificiranja na novih podatkih tipi\u010dno zelo slaba. Pravimo, da se je model preve\u010d prilegel u\u010dnim podatkom in ni dovolj splo\u0161en za dano problematiko. Prenasi\u010deni modeli imajo visoko stopnjo variance (angl. variance ), kar pomeni, da se preve\u010d prilagajajo majhnemu nihanju in naklju\u010dnemu \u0161umu v podatkih. Prenasi\u010denost \u010ce primerjamo prenasi\u010dene modele z u\u010denjem ljudi, bi lahko rekli, da pride do prenasi\u010denja, \u010de se ljudje nau\u010dimo snov dobesedno, ampak brez pravega razumevanja konceptov. \u010ce bi se u\u010dili mno\u017eenja \u0161tevil, bi se s prenasi\u010denjem nau\u010dili na pamet le rezultate mno\u017eenja \u0161tevil do \\(10\\) (ker so le ti v u\u010dbeniku). Ker pa nismo dojeli koncepta mno\u017eenja, ne bi poznali rezultata mno\u017eenja dveh \u0161e prej nevidenih \u0161tevil (primer: \\(12*27\\) ). Nasprotje prenasi\u010denju pa je nenasi\u010denost (angl. underfitting ). Nenasi\u010deni modeli imajo visoko pristranskost (angl. bias ). To pomeni, da je model preve\u010d splo\u0161en, saj ne zazna pomembnih relacij ali zakonitosti. Prikaz variance in pristranskosti. Nenasi\u010denost \u010ce se ponovno vrnemo na analogijo u\u010denja mno\u017eenja dveh \u0161tevil, bi pri\u0161li do nenasi\u010denosti, ko ne bi v celoti dojeli koncepta mno\u017eenja dveh \u0161tevil - znali bi mno\u017eiti dve celo\u0161tevilski pozitivni \u0161tevili, ne pa realnih \u0161tevil (primer: \\(6,4*1,34\\) ) ali celo negativnih celih \u0161tevil (primer: \\(-7 * 21\\) ). Praviloma imajo klasifikatorji z visoko pristranskostjo nizko stopnjo variance in obratno. Vedno \u017eelimo najti klasifikator s pravim ravnovesjem med pristranskostjo in varianco, a ta meja je arbitrarna in jo dolo\u010dimo sami.","title":"Kakovost klasifikacije"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#delitev-podatkov","text":"Da se izognemo prekomernemu prileganju podatkom, je potrebno podatke razdeliti na ve\u010d delov. Na enem izmed delov se klasifikator u\u010di (klasifikacijski algoritem zgradi klasifikacijski model), na drugih podatkih (na takih, ki niso sodelovali pri procesu u\u010denja) pa se preveri kakovost dobljenega modela. U\u010dna in testna mno\u017eica Nadaljujmo z analogijo u\u010denja mno\u017eenja dveh \u0161tevil. U\u010dno mno\u017eico dojemimo kot u\u010dno gradivo, ki ga uporabljamo za u\u010denje na izpit. \u010ce bi na izpitu dobili enake primere mno\u017eenja dveh \u0161tevil, kot smo jih \u017ee videli v u\u010dnem gradivu, bi preverjali le, \u010de imamo dober spomin, ne pa tudi, \u010de smo dejansko dojeli postopek mno\u017eenja dveh \u0161tevil. Tako naj bo izpit sestavljen iz popolnoma novih primerov mno\u017eenja dveh \u0161tevil, kar pa v na\u0161em postopku ponazorimo z lo\u010deno testno mno\u017eico. \u010ce je klasifikator dejansko izlu\u0161\u010dil vzorce iz podatkov, ne bi smel imeti te\u017eav pri klasifikaciji novih podatkov. \u010ce se je u\u010dne podatke nau\u010dil le na pamet, potem pa ne bo dobro klasificiral testnih podatkov (bo padel na izpitu).","title":"Delitev podatkov"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#ucna-in-testna-mnozica","text":"Osnovna razdelitev izvorne mno\u017eice je z delitvijo na dve podmno\u017eici: u\u010dno mno\u017eico (angl. train dataset ali learn dataset ) in testno mno\u017eico (angl. test dataset ) 1 . Na u\u010dni mno\u017eici le u\u010dimo klasifikatorje, ki pa jih nato testiramo in evalviramo le na testni mno\u017eici - podatki iz testne mno\u017eice ne smejo sodelovati pri gradnji klasifikacijskih modelov. Kljub deljenju na u\u010dno in testno mno\u017eico lahko dobimo prenasi\u010den model, a je mo\u017enost za to mnogo manj\u0161a. U\u010dno mno\u017eico \\(X\\) smo definirali \u017ee prej, testna mno\u017eica \\(X'\\) pa je definirana spodaj. \\[\\begin{align*} \\label{eq:testna_mnozica} \\begin{split} X' &= \\left\\lbrace \\left( x'_1,y'_1 \\right),\\left( x'_2,y'_2 \\right), \\dots ,\\left( x'_m,y'_m \\right) \\right\\rbrace \\\\ y'_i &\\in \\left\\lbrace razred_1, razred_2, \\dots , razred_k \\right\\rbrace \\\\ m &= \\text{\u0161tevilo instanc v testni mno\u017eici} \\\\ \\end{split} \\end{align*}\\] Klasifikacijski model vsaki testni instanci dolo\u010di razred \\(\\hat{y}\\) , ki pa ni nujno enak dejanskemu razredu testne instance \\(y'\\) - v tem primeru naredi napako pri klasifikaciji instance. Cilj je najti tak klasifikacijski model, ki pravilno klasificira \u010dim ve\u010d testnih instanc (idealno vse).","title":"U\u010dna in testna mno\u017eica"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#delitev-na-ucno-in-testno-mnozico-v-pythonu","text":"V kodi pa razdelimo podatke na u\u010dno in testno mno\u017eico na slede\u010d na\u010din. from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Razdelimo podatke na u\u010dne in testne X_u , X_t , y_u , y_t = train_test_split ( podatki . data , podatki . target , train_size = 0.7 , random_state = 42 ) print ( f 'Velikost u\u010dne mno\u017eice: { X_u . shape } ' ) print ( f 'Velikost u\u010dnih razredov: { y_u . shape } ' ) print ( f 'Velikost testne mno\u017eice: { X_t . shape } ' ) print ( f 'Velikost testnih razredov: { y_t . shape } ' ) Velikost u\u010dne mno\u017eice: (105, 4) Velikost u\u010dnih razredov: (105,) Velikost testne mno\u017eice: (45, 4) Velikost testnih razredov: (45,) S klicem train_test_split podane podatke razdelimo na u\u010dno in testno mno\u017eico na tak na\u010din, da zadostimo slede\u010dim kriterijem. \u010ce podamo parameter train_size in je vrednost tega celo \u0161tevilo, bo u\u010dna mno\u017eica \u0161tela prav toliko instanc. Primer: train_size=10 pove, da bo u\u010dna mno\u017eica \u0161tela to\u010dno 10 instanc. \u010ce podamo parameter train_size in je vrednost tega realno \u0161tevilo med 0 in 1, bo u\u010dna mno\u017eica v velikosti dele\u017ea izvorne podane mno\u017eice. Primer: train_size=0.8 pove, da bo u\u010dna mno\u017eica 80 % velikosti izvorne mno\u017eice. \u010ce podamo parameter test_size in je vrednost tega celo \u0161tevilo, bo testna mno\u017eica \u0161tela prav toliko instanc. Primer: test_size=10 pove, da bo testna mno\u017eica \u0161tela 10 instanc. \u010ce podamo parameter test_size in je vrednost tega realno \u0161tevilo med 0 in 1, bo testna mno\u017eica v velikosti dele\u017ea izvorne podane mno\u017eice. Primer: test_size=0.2 pove, da bo testna mno\u017eica 20 % velikosti izvorne mno\u017eice. Klicu train_test_split smo podali tako izvorno mno\u017eico atributov podatki.data , kakor tudi izvorne razrede podatki.target . S tem zagotovimo, da dobimo vrnjene razdeljene tako atribute instanc (u\u010dne X_u in testne X_t ), kakor tudi re\u0161itve (u\u010dne y_u in testne y_t ).","title":"Delitev na u\u010dno in testno mno\u017eico v Pythonu"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#validacijska-mnozica","text":"\u010ce se ukvarjamo s specifi\u010dnim problemom podatkovnega rudarjenja, se pri gradnji klasifikacijskih modelov sre\u010damo z optimizacijo parametrov klasifikatorja (na primer definiranja \u0161tevila sosedov \\(k\\) pri klasifikatorju \\(k\\) najbli\u017ejih sosedov). \u010ce parametre prilagajamo glede na rezultate klasifikatorjev na testni mno\u017eici, delamo napako, saj tako testni podatki sodelujejo pri gradnji optimalnega modela. V tem primeru potrebujemo \u0161e validacijsko mno\u017eico (angl. validation dataset ), na podlagi katere preizku\u0161amo in optimiziramo parametre, \u0161ele na koncu pa zgrajen model testiramo na testni mno\u017eici 2 . Slika spodaj prikazuje razdelitev izvorne podatkovne mno\u017eice na te tri dele: u\u010dno mno\u017eico, validacijsko mno\u017eico in testno mno\u017eico. Iz procesa je razvidno, da se nastali modeli znanja najprej preizkusijo na validacijski mno\u017eici ter se njihova kakovost ovrednoti na tej mno\u017eici. \u0160ele izbrani (po navadi najbolj\u0161i) model se uporabi za kon\u010dno ovrednotenje na testni mno\u017eici. Delitev mno\u017eice na u\u010dno, validacijsko in testno ter njihova uporaba.","title":"Validacijska mno\u017eica"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#delitev-na-ucno-validacijsko-in-testno-mnozico-v-pythonu","text":"V paketu scikit-learn ne obstaja klic, s katerim bi dosegli enostavno delitev na tri dele, zato je potrebno, da smo nekoliko zviti. V prvem klicu train_test_split smo pridobili testno mno\u017eico X_t , y_t ter za\u010dasno u\u010dno mno\u017eico X_u1 , y_u1 . Razmerje med njima je \\(4:1\\) v prid za\u010dasne u\u010dne mno\u017eice. V drugem klicu train_test_split pa za\u010dasno u\u010dno mno\u017eico razdelimo na dejansko u\u010dno X_u , y_u ter validacijsko mno\u017eico X_v , y_v v razmerju \\(3:1\\) . Kon\u010dno razmerje med u\u010dno mno\u017eico, validacijsko mno\u017eico in testno mno\u017eico je tako \\(3:1:1\\) . from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Razdelimo podatke na 'za\u010dasno u\u010dne' in testne X_u1 , X_t , y_u1 , y_t = train_test_split ( podatki . data , podatki . target , train_size = 0.8 , random_state = 42 ) # 'Za\u010dasno u\u010dne' podatke razdelimo na 'dejanske u\u010dne' in validacijske X_u , X_v , y_u , y_v = train_test_split ( X_u1 , y_u1 , train_size = 0.75 , random_state = 42 ) Poglejmo kak\u0161ne so mno\u017eice po razdelitvi. print ( f 'Velikost u\u010dne mno\u017eice: { X_u . shape } ' ) print ( f 'Velikost u\u010dnih razredov: { y_u . shape } ' ) print ( f 'Velikost validacijske mno\u017eice: { X_v . shape } ' ) print ( f 'Velikost validacijskih razredov: { y_v . shape } ' ) print ( f 'Velikost testne mno\u017eice: { X_t . shape } ' ) print ( f 'Velikost testnih razredov: { y_t . shape } ' ) Velikost u\u010dne mno\u017eice: (90, 4) Velikost u\u010dnih razredov: (90,) Velikost validacijske mno\u017eice: (30, 4) Velikost validacijskih razredov: (30,) Velikost testne mno\u017eice: (30, 4) Velikost testnih razredov: (30,)","title":"Delitev na u\u010dno, validacijsko in testno mno\u017eico v Pythonu"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#navzkrizna-validacija","text":"Pri deljenju podatkov na u\u010dno, testno in validacijsko mno\u017eico pa lahko zgolj po naklju\u010dju razdelimo podatke v u\u010dno, validacijsko in testno mno\u017eico, tako da se zgodi ena izmed slede\u010dih stvari: kak\u0161en vzorec ni prisoten (v celoti ali v dovolj veliki meri) v u\u010dni mno\u017eici in pri u\u010denju klasifikatorja ta ni zajet v modelu; se kak\u0161en, v realnosti neprisoten, vzorec poka\u017ee v u\u010dni mno\u017eici in se ta zapi\u0161e v model klasifikacije, v realnosti pa ni uporaben. Navzkri\u017ena validacija Nadaljujemo z analogijo u\u010denja mno\u017eenja \u0161tevil. Pri u\u010denju matemati\u010dne naloge (z re\u0161itvami) razdelimo na dva dela: tiste primere, na katerih se u\u010dimo (u\u010dno mno\u017eico) in tiste, na katerih se pred izpitom preverimo, \u010de vemo dovolj (validacijska mno\u017eica; izpit bo pa testna mno\u017eica). \u010ce smo po naklju\u010dju razdelili naloge mno\u017eenja tako, da se u\u010dimo le iz najla\u017ejih (recimo le mno\u017eenje celih pozitivnih \u0161tevil), dolo\u010denega naprednega znanja ne bomo osvojili (mno\u017eenja negativnih ali realnih \u0161tevil). Da zmanj\u0161amo mo\u017enost za ponesre\u010deno razdelitev mno\u017eice, to razdelimo na ve\u010d na\u010dinov - na \\(n\\) na\u010dinov. Tako iz ene mno\u017eice dobimo \\(n\\) u\u010dnih mno\u017eic in \\(n\\) testnih mno\u017eic. Ta proces imenujemo navzkri\u017ena validacija (angl. cross-validation ) in ste\u010de tako, da celotno mno\u017eico razdelimo na \\(n\\) delov - na \\(n\\) rezov (angl. folds ) 3 . Vsak rez je enkrat v vlogi testne mno\u017eice, v vseh ostalih primerih pa je v kombinaciji z ostalimi rezi del u\u010dne mno\u017eice. Posledi\u010dno je tudi vsaka instanca enkrat v testni mno\u017eici, v ostalih primerih pa je vedno v u\u010dni mno\u017eici. Slede\u010da slika prikazuje razdelitev celotne mno\u017eice na \u0161tiri reze za namen navzkri\u017ene validacije. Stratificirana navzkri\u017ena validacija. Vertikalna Y os prikazuje pripadnost instanc bodisi v u\u010dno (modra) ali testno (rde\u010da) mno\u017eico pri posameznem rezu. Pri naklju\u010dni delitvi na reze se zna zgoditi, da bodo razmerja med razredi razli\u010dna med rezi. Tega se \u017eelimo izogniti, saj lahko nastane situacija, ko so vse instance enega razreda le v enem rezu. Ko bo ta rez v vlogi testnih instanc, klasifikacijski algoritem nima mo\u017enosti, da pravilno klasificira instance tega manj\u0161inskega razreda - saj jih nikoli ne vidi v \u010dasu u\u010denja. S postopkom stratifikacije (angl. stratification ) poskrbimo, da se mno\u017eica preme\u0161a na tak na\u010din, da so razmerja razredov v vseh rezih kar se da enaka. V Pythonu razdelimo mno\u017eico s stratificirano navzkri\u017eno validacijo slede\u010de. from sklearn.datasets import load_iris from sklearn.model_selection import StratifiedKFold # Nalo\u017eimo podatke podatki = load_iris ( as_frame = True ) # Stratificirana navzkri\u017ena validacija s \u0161tirimi rezi skf = StratifiedKFold ( n_splits = 4 ) rez = 1 # S tem bomo \u0161teli reze instanca = 13 # Pregledovali bomo instanco na mestu 13 print ( f 'Zanima nas instanca { instanca } s podatki:' ) print ( podatki . data . iloc [ instanca ,:]) for ucne , testne in skf . split ( podatki . data , podatki . target ): if instanca in ucne : print ( f 'Instanca { instanca } je v { rez } . delitvi v u\u010dni mno\u017eici.' ) elif instanca in testne : print ( f 'Instanca { instanca } je v { rez } . delitvi v testni mno\u017eici.' ) rez = rez + 1 Zanima nas instanca 13 s podatki: sepal length (cm) 4.3 sepal width (cm) 3.0 petal length (cm) 1.1 petal width (cm) 0.1 Name: 13, dtype: float64 Instanca 13 je v 1. delitvi v u\u010dni mno\u017eici. Instanca 13 je v 2. delitvi v testni mno\u017eici. Instanca 13 je v 3. delitvi v u\u010dni mno\u017eici. Instanca 13 je v 4. delitvi v u\u010dni mno\u017eici. Z uporabo razreda StratifiedKFold in njegove metode split razdelimo podane podatke na reze. Z vrednostjo n_splits ob inicializaciji podamo, na koliko rezov delimo mno\u017eico. V for zanki tako na vsaki u\u010dni mno\u017eici nau\u010dimo svoj model klasifikacije, ki ga evalviramo na testni mno\u017eici tistega reza. Tako dobimo \\(n\\) klasifikacijskih modelov, vsak se je nau\u010dil na nekoliko druga\u010dni u\u010dni mno\u017eici in vsak se je evalviral na popolnoma druga\u010dni testni mno\u017eici. Iz tega sledi, da dobimo tudi \\(n\\) rezultatov klasifikacije na testnih mno\u017eicah. Da pa ocenimo celokupno kakovost podanega klasifikacijskega algoritma, pa vse rezultate preprosto povpre\u010dimo. Sledi poglavje, ki pregleda, kako sploh merimo kakovost klasifikacije, razdeljeno glede na \u0161tevilo razredov, v katere model klasifikacije razvr\u0161\u010da instance.","title":"Navzkri\u017ena validacija"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#metrike-klasifikacije","text":"Ko imamo napovedi modela, lahko ovrednotimo oz. evalviramo kakovost te napovedi. To naredimo s pomo\u010djo klasifikacijskih metrik (angl. classification metrics ) 4 , 5 , 6 , 7 . Sledi pregled razli\u010dnih klasifikacijskih metrik, najprej, ko imamo opravka s klasifikacijo v dva razreda (dvorazredna ali binarna klasifikacija) in nato, ko imamo opravka z ve\u010d kot dvema razredoma (ve\u010drazredna klasifikacija).","title":"Metrike klasifikacije"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#binarna-klasifikacija","text":"Pri binarnih klasifikatorjih gradimo model za klasifikacijo instanc v dva razreda. Ko imamo model zgrajen, ga ocenimo s pomo\u010djo testnih podatkov in iz rezultatov zgradimo matriko zmede ali kontingen\u010dno tabelo (angl. confusion matrix ali contingency table ), ki je prikazana v spodnji tabeli. Napovadan A Napovedan B Dejanski A TP FN Dejanski B FP TN Matrika zmede za dva razreda. V danem primeru klasificiramo podatke v dva razreda: A in B. Vsaka celica v tabeli prikazuje \u0161tevilo instanc. Vrstice matrike ka\u017eejo dejansko pripadnost instanc v razrede, stolpci pa povedo razporeditev instanc v razrede s pomo\u010djo klasifikatorja. Enemu razredu pripi\u0161emo lastnost pozitivnega razreda, drugemu pa lastnost negativnega razreda. Ta dolo\u010ditev je arbitrarna in je odvisna od problematike re\u0161evanja - pravilne dolo\u010ditve ni in bi tudi druga delitev v pozitivne in negativne vrnila prave rezultate. from sklearn.metrics import confusion_matrix # Podamo dejanske razrede in napovedi klasifikatorja y_dejanski = [ 'A' , 'A' , 'B' , 'B' , 'A' , 'B' ] y_napovedan = [ 'A' , 'A' , 'B' , 'A' , 'A' , 'B' ] # Izra\u010dunamo matriko zmede mat_zmede = confusion_matrix ( y_dejanski , y_napovedan , labels = [ 'A' , 'B' ]) print ( mat_zmede ) [[3 0] [1 2]] Klicu confusion_matrix podamo vsaj dve vrednosti: polje dejanskih razredov in polje napovedi. Pomembno je, da so elementi v obeh poljih v enakem vrstnem redu (prvi element v obeh poljih ka\u017ee tako dejanski razred kot napoved prvega elementa). S parametrom labels pa podamo vrstni red razredov pri kreaciji matrike zmede - \u010de tega ne podamo, bo vrstni red razredov po abecednem vrstnem redu. V primeru iz kode vidimo, da je klasifikacijski model za tri instance razreda A povedal, da so res razreda A; za dve instanci razreda B je povedal, da sta res razreda B; ter za eno instanco razreda B je napa\u010dno povedal, da je ta razreda A. \u010ce privzamemo, da je razred A pozitiven, matrika zmede binarne klasifikacije vsebuje \u0161tiri vrednosti, ki ka\u017eejo koli\u010dino instanc glede na rezultate klasifikacije: TP - pravilno klasificirane kot pozitivne (angl. true positives ), FP - napa\u010dno klasificirane kot pozitivne (angl. false positives ), TN - pravilno klasificirane kot negativne (angl. true negatives ), FN - napa\u010dno klasificirane kot negativne (angl. false negatives ). Raz\u0161irimo prej\u0161nji primer z naslednjo kodo, ki nam vrne te \u0161tiri vrednosti (deluje le pri binarni klasifikaciji). Klicu metode confusion_matrix dodamo \u0161e klic metode ravel , ki dobljeno metriko zlo\u017ei v enodimenzionalno polje (najprej vse iz prve vrstice, nato vse iz druge vrstice ...). # Izra\u010dunamo vrednosti napovedi tp , fn , fp , tn = confusion_matrix ( y_dejanski , y_napovedan , labels = [ 'A' , 'B' ]) . ravel () print ( f 'TP: { tp } ' ) print ( f 'TN: { tn } ' ) print ( f 'FP: { fp } ' ) print ( f 'FN: { fn } ' ) TP: 3 TN: 2 FP: 1 FN: 0 S pomo\u010djo teh \u0161tirih vrednosti lahko izra\u010dunamo razli\u010dne metrike klasifikacije (angl. classification metrics ) oziroma vrednosti, ki nam povedo, kako kakovosten je klasifikacijski model. Teh metrik je ve\u010d in vsaka slu\u017ei svojemu namenu - obstajajo tudi razli\u010dni pogledi, kaj sploh pomeni kakovosten model znanja.","title":"Binarna klasifikacija"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#tocnost-klasifikacije-in-delez-napake","text":"Prva taka metrika je to\u010dnost klasifikacije (angl. accuracy ), ki prikazuje, kolik\u0161en dele\u017een instanc je klasificiran v dejanski razred - dele\u017e pravilno klasificiranih instanc. Formula to\u010dnosti je prikazana v slede\u010di ena\u010dbi in se izra\u010duna kot ulomek \u0161tevila pravilno klasificiranih instanc ( \\(TP + TN\\) ) deljeno s \u0161tevilom vseh klasificiranih instanc ( \\(TP + FP + TN + FN\\) ). Metriko to\u010dnost \u017eelimo maksimirati in te\u017eimo k temu, da je \u010dim bli\u017eje vrednosti 1 8 . \\[\\begin{equation*} to\\check{c}nost = \\frac{TP + TN}{TP + FP + TN + FN} \\end{equation*}\\] Metrika, ki ka\u017ee nasprotno vrednost to\u010dnosti, je dele\u017e napake (angl. error rate ); pravimo tudi, da je obratno sorazmerna kot to\u010dnost. Predstavlja dele\u017e nepravilno klasificiranih instanc, kar je prikazano v spodnji ena\u010dbi. Lahko pa dele\u017e napake izra\u010dunamo tudi preprosto kot \\(1 - to\\check{c}nost\\) . Dele\u017e napake je metrika, ki jo minimiziramo in katere idealna vrednost je \\(0\\) . \\[\\begin{equation*} dele\\check{z}\\:napake = \\frac{FP + FN}{TP + FP + TN + FN} \\end{equation*}\\] Prika\u017eimo \u0161e izra\u010dun to\u010dnosti, dele\u017ea napake in \u0161tevila pravilno klasificiranih instanc v Pythonu s klicem accuracy_score . from sklearn.metrics import accuracy_score # Podamo dejanske razrede in napovedi klasifikatorja y_dejanski = [ 'A' , 'A' , 'B' , 'B' , 'A' , 'B' ] y_napovedan = [ 'A' , 'A' , 'B' , 'A' , 'A' , 'B' ] # Izra\u010dunamo to\u010dnost tocnost = accuracy_score ( y_dejanski , y_napovedan ) delez_napake = 1 - tocnost # Izra\u010dunamo \u0161tevilo pravilno klasificiranih (TP+TN) st_pravilnih = accuracy_score ( y_dejanski , y_napovedan , normalize = False ) print ( f 'To\u010dnost: { tocnost } ' ) print ( f 'Dele\u017e napake: { delez_napake } ' ) print ( f '\u0160tevilo pravilnih: { st_pravilnih } ' ) To\u010dnost: 0.8333333333333334 Dele\u017e napake: 0.16666666666666663 \u0160tevilo pravilnih: 5","title":"To\u010dnost klasifikacije in dele\u017e napake"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#priklic-in-preciznost","text":"Naslednji dve metriki, ki ju bomo obravnavali skupaj, sta priklic in preciznost. Obe metriki imata vrednost v intervalu \\([0,1]\\) , kjer je 0 najslab\u0161a vrednost, 1 pa najbolj\u0161a. Metrika priklic (angl. recall ) nam pove dele\u017e pozitivnih instanc, ki so pravilno klasificirane v pozitivni razred. V\u010dasih priklic imenujemo tudi senzitivnost (angl. sensitivity ) ali dele\u017e pravilno klasificiranih pozitivnih instanc (angl. true positive rate ). Izra\u010dun priklica je prikazan v slede\u010di ena\u010dbi. \\[\\begin{equation*} priklic = \\frac{TP}{TP + FN} \\end{equation*}\\] Poglejmo si uporabo priklica v Pythonu. from sklearn.metrics import recall_score # Podamo dejanske razrede in napovedi klasifikatorja y_dejanski = [ 'A' , 'A' , 'B' , 'B' , 'A' , 'B' ] y_napovedan = [ 'A' , 'A' , 'B' , 'A' , 'A' , 'B' ] # Izra\u010dunamo priklic obeh razredov priklic_A = recall_score ( y_dejanski , y_napovedan , pos_label = 'A' ) priklic_B = recall_score ( y_dejanski , y_napovedan , pos_label = 'B' ) print ( f 'Priklic A: { priklic_A } ' ) print ( f 'Priklic B: { priklic_B } ' ) Priklic A: 1.0 Priklic B: 0.6666666666666666 Podobno kot pri izra\u010dunu to\u010dnosti, tudi pri klicu metode recall_score podamo najprej polje z dejanskimi razredi, \u010demur pa sledi polje z napovedanimi razredi. Pri binarni klasifikaciji moramo podati tudi parameter pos_label , s katerim dolo\u010dimo razred, za katerega ra\u010dunamo priklic. Priklic oz. senzitivnost testa O senzitivnosti (ali ob\u010dutljivosti) testa v praksi zelo pogosto govorimo, ko imamo opravka s testom, ki ugotavlja prisotnost ali odsotnost dolo\u010denega obolenja (npr. bolezni, ki jo povzro\u010da virus). Senzitivnost takega testa nam pove kak\u0161en je dele\u017e bolnih, ki jih je test na\u0161el. Predstavljajmo si primer, ko imamo 100 pacientov, ki so dejansko bolni. Test je za dejansko bolne pokazal, da jih je 80 bolnih, za ostalih 20 pa je test (napa\u010dno) trdil, da niso bolni. Tak test ima \\(80 %\\) senzitivnost oz. priklic. Druga metrika je preciznost (angl. precision in nam pove, kolik\u0161en dele\u017e instanc, klasificiranih v pozitivni razred, je dejansko pripadnikov pozitivnega razreda. V\u010dasih metriko priklic imenujemo zaupanje (angl. confidence ) ali to\u010dnost pozitivno klasificiranih pozitivnih instanc (angl. true positive accuracy ). \\[\\begin{equation*} preciznost = \\frac{TP}{TP + FP} \\end{equation*}\\] Slede\u010da koda prikazuje uporabo klica precision_score . from sklearn.metrics import precision_score # Izra\u010dunamo preciznost obeh razredov preciznost_A = precision_score ( y_dejanski , y_napovedan , pos_label = 'A' ) preciznost_B = precision_score ( y_dejanski , y_napovedan , pos_label = 'B' ) print ( f 'Preciznost A: { preciznost_A } ' ) print ( f 'Preciznost B: { preciznost_B } ' ) Preciznost A: 0.75 Preciznost B: 1.0 Ponovno se vrnimo na primer testa za ugotavljanje dolo\u010denega obolenja. Imamo 100 pacientov, ki so dejansko bolni. Test je za 80 izmed teh pokazal, da so bolni, za \u0161e 10 dodatnih (dejansko zdravih) pacientov pa je prav tako (napa\u010dno) pokazal, da so bolni. Tak test ima \\(88,89 %\\) preciznosti oz. zaupanja.","title":"Priklic in preciznost"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#f-mera","text":"Iz prej\u0161njih sekcij je razvidno, da obstaja nabor metrik za merjenje kakovosti binarne klasifikacije, od katerih ima vsaka svoj namen. Ko \u017eelimo meriti kakovost klasifikacijskega modela v splo\u0161nem, pa uporaba ve\u010djega \u0161tevila metrik ni prakti\u010dna. V ta namen uporabljamo metriko, imenovano F-mera (angl. F-score ali F-measure ), ki zdru\u017ei metriki priklic in preciznost v harmoni\u010dni sredini, kot je prikazano v ena\u010dbi spodaj. Metriki preciznost in priklic nista neposredno povezani in prikazujeta razli\u010dne informacije. Visoka vrednost ene ne pomeni nujno nizke vrednosti druge, zato stremimo k temu, da bi obe metriki bili dovolj visoki. Prednost F-mere je, da zdru\u017ei obe omenjeni metriki skupaj v eno \u0161tevilo, kjer se takoj opazi nizka vrednost katerekoli izmed njiju. \\[\\begin{align*} \\begin{split} F\\text{-}mera_\\beta &= (1 + \\beta^2) \\cdot \\frac{preciznost \\cdot priklic}{(\\beta^2 \\cdot preciznost) + priklic} \\\\[.3cm] &= \\frac{(1 + \\beta^2) \\cdot TP}{(1 + \\beta^2) \\cdot TP + \\beta^2 \\cdot FN + FP} \\end{split} \\end{align*}\\] Ta ena\u010dba prikazuje splo\u0161no obliko F-mere, kjer lahko posamezno metriko (preciznost ali priklic) ute\u017eimo pri kon\u010dnem izra\u010dunu. Vlogo ute\u017ei igra vrednost \\(\\beta\\) , ki je vi\u0161ja pri ve\u010dji pomembnosti metrike preciznost in ni\u017eja, ko damo ve\u010dji poudarek metriki priklica. Tradicionalna oblika metrike F-mera je, ko sta obe vrednosti enako ute\u017eeni in imata uravnote\u017eeno vlogo pri izra\u010dunu (ko je \\(\\beta=1\\) ). Najve\u010dkrat se uporablja prav ta, ki se v\u010dasih imenuje tudi F \\(_{1}\\) -mera in je prikazana s slede\u010do ena\u010dbo. \\[\\begin{equation*} F_1\\text{-}mera = 2 \\cdot \\frac{preciznost \\cdot priklic}{preciznost + priklic} \\end{equation*}\\] \u010ce uporabljamo pri izra\u010dunu F-mere vrednost \\(\\beta\\) , ki ni enaka \\(1\\) , se to zapi\u0161e kot podpisana vrednost (F \\(_2\\) -mera ali F \\(_{0,5}\\) -mera). \u010ce pa je \\(\\beta\\) enak vrednosti \\(1\\) , lahko podpisano vrednost izpustimo in zapi\u0161emo le F-mera. Slede\u010da Python koda prikazuje izra\u010dun F-mere z in brez podane \\(\\beta\\) vrednosti. Koda raz\u0161irja prej\u0161nje primere, kjer so dejanski in napovedani razredi \u017ee definirani. from sklearn.metrics import f1_score , fbeta_score # Izra\u010dunamo F-mero in F-meta (s podano beto) razreda A f1_score_A = f1_score ( y_dejanski , y_napovedan , pos_label = 'A' ) f_beta_score_A = fbeta_score ( y_dejanski , y_napovedan , pos_label = 'A' , beta = 2 ) print ( f 'F1-mera A: { f1_score_A } ' ) print ( f 'F-mera (beta=2) A: { f_beta_score_A } ' ) F1-mera A: 0.8571428571428571 F-mera (beta=2) B: 0.9375","title":"F-mera"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#klasifikacija-v-vec-razredov","text":"V prej\u0161nji sekciji smo naredili pregled metrik za ocenjevanje kakovosti modelov klasifikacije v dva razreda. Mnogokrat pa imajo podatki ve\u010d mo\u017enih razredov, zato evalvacija teh modelov z metrikami, neprilagojenimi za ve\u010d razredov, postane te\u017eavna. Kreacija matrike zmede poteka po enakem postopku kot pri binarni klasifikaciji, kjer v vrstice bele\u017eimo dejanske, v stolpce pa napovedane razrede instanc, kot ka\u017ee slede\u010da tabela. Napovadan A Napovedan B Napovedan C Dejanski A 50 1 2 Dejanski B 4 20 0 Dejanski C 5 1 10 Matrika zmede za ve\u010d razredov. Najpreprostej\u0161a metrika za izra\u010dun ne glede na \u0161tevilo razredov je to\u010dnost, saj \u0161e vedno velja, da \u0161tevilo pravilno klasificiranih instanc delimo s \u0161tevilom vseh instanc. Podobno velja tudi za dele\u017e napake, le da \u0161tevilo pravilno klasificiranih zamenjamo s \u0161tevilom nepravilno klasificiranih primerkov. Te\u017eava nastopi pri poimenovanju pozitivnih in negativnih razredov, saj smo prisiljeni oznako pozitivni ali negativni dodeliti ve\u010d razredom. Vse metrike, ki vsebujejo \u0161tevilo pozitivnih ali negativnih instanc, se morajo prilagoditi uporabi klasifikacije v ve\u010d razredov na slede\u010d na\u010din. Te metrike ra\u010dunamo v ve\u010d korakih, kjer v vsakem koraku dobi naziv pozitivnega razreda drug razred, ostali pa predstavljajo negativni razred. Kon\u010dno vrednost metrik dobimo na ve\u010d na\u010dinov: z makro agregacijo, z ute\u017eenim povpre\u010denjem , ali z mikro agregacijo 9 .","title":"Klasifikacija v ve\u010d razredov"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#makro-agregacija-metrik","text":"Pri makro agregaciji metrik klasifikacije izdelamo ve\u010d matrik zmede ( \\(m\\) matrik zmede, \u010de imamo \\(m\\) razredov), in sicer tako, da je v vsaki metriki zmede drug razred ozna\u010den kot pozitivni razred. Za vsako matriko zmede izra\u010dunamo izbrano metriko (dobimo \\(m\\) metrik klasifikacije). Kon\u010dno vrednost metrike klasifikacije pa na koncu izra\u010dunamo tako, da povpre\u010dimo vseh \\(m\\) metrik. Slede\u010de ena\u010dbe prikazujejo makro ( \\(Ma\\) ) izra\u010dun nekaterih metrik klasifikacije. \\[\\begin{align*} priklic_{Ma} &= \\frac{\\sum_{i=1}^{m} \\frac{TP_i}{TP_i + FN_i}}{m} = \\frac{\\sum_{i=1}^{m} priklic_{i}}{m} \\\\[.2cm] preciznost_{Ma} &= \\frac{\\sum_{i=1}^{m} \\frac{TP_i}{TP_i + FP_i}}{m} = \\frac{\\sum_{i=1}^{m} preciznost_{i}}{m} \\\\[.2cm] F_1\\text{-}mera_{Ma} &= \\frac{\\sum_{i=1}^{m} F_1\\text{-}mera_{i}}{m} \\end{align*}\\] \u010ceprav se metrika to\u010dnost izra\u010duna za m razredov enako kot za binarni problem klasifikacije, pa lahko izra\u010dunamo metriko povpre\u010dna to\u010dnost (angl. average class accuracy ). Ta metrika izra\u010duna najprej to\u010dnost za vsak posamezni razred, nato pa povpre\u010di izra\u010dunane vrednosti. Po enakem postopku izra\u010dunamo tudi povpre\u010dno stopnjo napake (angl. average class error rate ), le da povpre\u010dimo dele\u017ee napak razredov. \\[\\begin{align*} povpre\\check{c}na\\:to\\check{c}nost &= \\frac{\\sum_{i=1}^{m} \\frac{TP_i+TN_i}{TP_i + FP_i + TN_i + FN_i}}{m} \\\\[.2cm] povpre\\check{c}en\\:dele\\check{z}\\:napake &= \\frac{\\sum_{i=1}^{m} \\frac{FP_i+FN_i}{TP_i + FP_i + TN_i + FN_i}}{m} \\end{align*}\\]","title":"Makro agregacija metrik"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#utezena-agregacija-metrik","text":"Preprosto povpre\u010denje metrik vsakega razreda najve\u010dkrat ni smiselno. Ko imamo neuravnote\u017eeno mno\u017eico (tj. mno\u017eico, ko je razmerje med razredi zelo neenakomerno - \u010dez \\(8:1\\) ), povpre\u010denje kakovosti klasifikacije instanc pogostega razreda in kakovosti instanc redkih razredov ni smiselno. Namre\u010d, ve\u010d instanc imamo, ve\u010d mo\u017enosti ima klasifikacijski algoritem, da izlu\u0161\u010di vzorce, ki so zna\u010dilni za instance tega razreda. To privede do klasifikacijskih modelov, ki delujejo mnogo bolj\u0161e pri klasifikaciji instanc bolj pogostega razreda v primerjavi s klasifikacijo instanc redkih razredov. \u010ce kakovost klasifikacije povpre\u010dimo (kot je to pri makro agregaciji), kakovost klasifikacije vseh razredov enakomerno prispeva h kon\u010dni metriki. To je odli\u010dno, \u010de \u017eelimo ve\u010dji poudarek na kakovosti manj\u0161inskih instanc. \u010ce pa ne \u017eelimo, da instance v manj\u0161ini prevzamejo neenakomerno ve\u010djo vlogo pri ocenjevanju kakovosti, pa lahko posamezne metrike pred povpre\u010denjem ute\u017eimo. Pri ute\u017eevanju metriko posameznega razreda \\(i\\) zmno\u017eimo z dele\u017eem instanc \\(u_i\\) razreda \\(i\\) , zmno\u017eene metrike pa nato se\u0161tejemo v skupno mero. Dele\u017e razreda \\(u_i\\) je preprost odstotek instanc tega razreda \\(i\\) v testni mno\u017eici. \\[\\begin{align*} priklic_{Ua} &= \\sum_{i=1}^{m} u_i \\cdot \\frac{TP_i}{TP_i + FN_i} = \\sum_{i=1}^{m} u_i \\cdot priklic_{i} \\\\[.2cm] preciznost_{Ua} &= \\sum_{i=1}^{m} u_i \\cdot \\frac{TP_i}{TP_i + FP_i} = \\sum_{i=1}^{m} u_i \\cdot preciznost_{i} \\\\[.2cm] F_1\\text{-}mera_{Ua} &= \\sum_{i=1}^{m} u_i \\cdot F_1\\text{-}mera_{i} \\end{align*}\\]","title":"Ute\u017eena agregacija metrik"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#mikro-agregacija-metrik","text":"Pri mikro agregaciji metrik za\u010dnemo podobno kot pri makro agregaciji; zgradimo \\(m\\) matrik zmede, nadaljnji postopek pa je druga\u010den. Namesto povpre\u010denja razli\u010dnih metrik se\u0161tejemo posamezne vrednosti iz matrik zmede. Primer: vse \\(TP_i\\) se\u0161tejemo skupaj, da dobimo \\(TP_{Mi}\\) , in tega vstavimo v ena\u010dbo za izra\u010dun mikro agregiranih metrik. Ena\u010dbe spodaj prikazujejo na\u010din izra\u010duna posameznih metrik na mikro ( \\(Mi\\) ) na\u010din. \\[\\begin{align*} priklic_{Mi} &= \\frac{\\sum_{i=1}^{m} TP_i}{\\sum_{i=1}^{m} (TP_i + FN_i)} \\\\[.2cm] preciznost_{Mi} &= \\frac{\\sum_{i=1}^{m} TP_i}{\\sum_{i=1}^{m} (TP_i + FP_i)} \\\\[.2cm] F_1\\text{-}mera_{Mi} &= 2 \\cdot \\frac{preciznost_{Mi} \\cdot priklic_{Mi}}{preciznost_{Mi} + priklic_{Mi}} \\end{align*}\\] Posebnih mikro agregacij to\u010dnosti in dele\u017ea napake ni, saj v tem primeru dobimo \u017ee prej definirano to\u010dnost in dele\u017e napake.","title":"Mikro agregacija metrik"},{"location":"pages/knjiga/06_kakovost_klasifikacije/#agregacija-metrik-v-pythonu","text":"Z uporabo knji\u017enice scikit-learn in \u017ee do sedaj predstavljenih metod accuracy_score , precision_score , recall_score in f1_score enostavno agregiramo metrike vseh razredov v eno vrednost. Pri agregaciji nam slu\u017ei parameter teh metod average , s katerim podamo na\u010din agregacije. from sklearn.metrics import precision_score , recall_score , f1_score # Izra\u010dunamo preciznost z makro agregacijo preciznost_makro = precision_score ( y_dejanski , y_napovedan , average = 'macro' ) # Izra\u010dunamo ute\u017een priklic priklic_utezen = recall_score ( y_dejanski , y_napovedan , average = 'weighted' ) # Izra\u010dunamo mikro F-mero f_mera_mikro = f1_score ( y_dejanski , y_napovedan , average = 'micro' ) # Izra\u010dunamo lo\u010deno preciznost za vsak razred preciznost_vseh = precision_score ( y_dejanski , y_napovedan , average = None ) print ( f 'Makro preciznost: { preciznost_makro } ' ) print ( f 'Ute\u017een priklic: { priklic_utezen } ' ) print ( f 'Mikro F-mera: { f_mera_mikro } ' ) print ( f 'Preciznost vseh razredov: { preciznost_vseh } ' ) Makro preciznost: 0.875 Ute\u017een priklic: 0.8333333333333334 Mikro F-mera: 0.8333333333333334 Preciznost vseh razredov: [0.75 1. ] Vrednost parametra average dolo\u010da na\u010din agregacije: 'macro' se uporabi za makro agregacijo. 'weighted' se uporabi za ute\u017eeno agregacijo, kjer so ute\u017ei sestavljene iz razmerja razredov v podani testni mno\u017eici. 'micro' se uporabi za mikro agregacijo. None se uporabi, ko \u017eelimo lo\u010den izpis metrike za vsak razred posebej, brez agregacije. Lever, J., 2016. Classification evaluation: It is important to understand both what a classification metric expresses and what it hides. Nature Methods, 13(8), pp.603-605. \u21a9 Guyon, I., 1997. A scaling law for the validation-set training-set size ratio. AT&T Bell Laboratories, 1(11). \u21a9 Schaffer, C., 1993. Selecting a classification method by cross-validation. Machine Learning, 13(1), pp.135-143. \u21a9 Hossin, M. and Sulaiman, M.N., 2015. A review on evaluation metrics for data classification evaluations. International journal of data mining & knowledge management process, 5(2), p.1. \u21a9 Powers, D.M., 2020. Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation. arXiv preprint arXiv:2010.16061. \u21a9 Sokolova, M., Japkowicz, N. and Szpakowicz, S., 2006, December. Beyond accuracy, F-score and ROC: a family of discriminant measures for performance evaluation. In Australasian joint conference on artificial intelligence (pp. 1015-1021). Springer, Berlin, Heidelberg. \u21a9 Sokolova, M. and Lapalme, G., 2009. A systematic analysis of performance measures for classification tasks. Information processing & management, 45(4), pp.427-437. \u21a9 Dem\u0161ar, J., 2006. Statistical comparisons of classifiers over multiple data sets. The Journal of Machine Learning Research, 7, pp.1-30. \u21a9 Jurman, G. and Furlanello, C., 2010. A unifying view for performance measures in multi-class prediction. arXiv preprint arXiv:1008.2908. \u21a9","title":"Agregacija metrik v Pythonu"},{"location":"pages/knjiga/09_algoritmi/","text":"Nadzorovano u\u010denje Regresija Klasifikacija k najbli\u017ejih sosedov","title":"09 algoritmi"},{"location":"pages/knjiga/09_o_gradivu/","text":"O gradivu Naslov Strojno u\u010denje Podnaslov S Pythonom do prvega klasifikatorja Avtorja Sa\u0161o Karakati\u010d Univerza v Mariboru, Fakulteta za elektrotehniko, ra\u010dunalni\u0161tvo in informatiko Iztok Fister Ml. Univerza v Mariboru, Fakulteta za elektrotehniko, ra\u010dunalni\u0161tvo in informatiko Recenzija Niko Luka\u010d Univerza v Mariboru, Fakulteta za elektrotehniko, ra\u010dunalni\u0161tvo in informatiko Branko Kav\u0161ek Univerza na Primorskem, Fakulteta za matematiko, naravoslovje in informacijske tehnologije Lektoriranje Nu\u0161a Grah Tehni\u010dna urednika Sa\u0161o Karakati\u010d Univerza v Mariboru, Fakulteta za elektrotehniko, >ra\u010dunalni\u0161tvo in informatiko Jan Per\u0161a Univerza v Mariboru, Univerzitetna zalo\u017eba Oblikovanje ovitka Jan Per\u0161a Univerza v Mariboru, Univerzitetna zalo\u017eba Zalo\u017enik Univerza v Mariboru, Univerzitetna zalo\u017eba Slom\u0161kov trg 15, 2000 Maribor, Slovenija https://press.um.si , zalozba@um.si Izdajatelj Univerza v Mariboru, Fakulteta za elektrotehniko, ra\u010dunalni\u0161tvo in informatiko Koro\u0161ka cesta 46, 2000 Maribor, Slovenija http://feri.um.si , feri@um.si Izdaja Prva Vrsta publikacije E-knjiga Izdano Maribor, Slovenija, januar 2022 Dostopno na http://press.um.si/index.php/ump/catalog/book/643 Licenca CC BY-NC-ND 4.0 \u00a9 Univerza v Mariboru, Univerzitetna zalo\u017eba Navajanje gradiva V besedlu BibTeX Karakati\u010d, S. in Fister Ml., I. (2022). Strojno u\u010denje: s Pythonom do prvega klasifikatorja. Maribor: Univerzitetna zalo\u017eba. doi: 10.18690/um.feri.1.2022 @book{karakatic_strojnoucenje_2022, title = \"Strojno u\u010denje: s Pythonom do prvega klasifikatorja.\", author = \"Karakati\u010d, Sa\u0161o and Fister Jr., Iztok\", year = 2022, publisher = \"Univerzitetna zalo\u017eba\", address = \"Maribor\", doi = \"10.18690/um.feri.1.2022\" }","title":"O gradivu"},{"location":"pages/knjiga/09_o_gradivu/#o-gradivu","text":"Naslov Strojno u\u010denje Podnaslov S Pythonom do prvega klasifikatorja Avtorja Sa\u0161o Karakati\u010d Univerza v Mariboru, Fakulteta za elektrotehniko, ra\u010dunalni\u0161tvo in informatiko Iztok Fister Ml. Univerza v Mariboru, Fakulteta za elektrotehniko, ra\u010dunalni\u0161tvo in informatiko Recenzija Niko Luka\u010d Univerza v Mariboru, Fakulteta za elektrotehniko, ra\u010dunalni\u0161tvo in informatiko Branko Kav\u0161ek Univerza na Primorskem, Fakulteta za matematiko, naravoslovje in informacijske tehnologije Lektoriranje Nu\u0161a Grah Tehni\u010dna urednika Sa\u0161o Karakati\u010d Univerza v Mariboru, Fakulteta za elektrotehniko, >ra\u010dunalni\u0161tvo in informatiko Jan Per\u0161a Univerza v Mariboru, Univerzitetna zalo\u017eba Oblikovanje ovitka Jan Per\u0161a Univerza v Mariboru, Univerzitetna zalo\u017eba Zalo\u017enik Univerza v Mariboru, Univerzitetna zalo\u017eba Slom\u0161kov trg 15, 2000 Maribor, Slovenija https://press.um.si , zalozba@um.si Izdajatelj Univerza v Mariboru, Fakulteta za elektrotehniko, ra\u010dunalni\u0161tvo in informatiko Koro\u0161ka cesta 46, 2000 Maribor, Slovenija http://feri.um.si , feri@um.si Izdaja Prva Vrsta publikacije E-knjiga Izdano Maribor, Slovenija, januar 2022 Dostopno na http://press.um.si/index.php/ump/catalog/book/643 Licenca CC BY-NC-ND 4.0 \u00a9 Univerza v Mariboru, Univerzitetna zalo\u017eba","title":"O gradivu"},{"location":"pages/knjiga/09_o_gradivu/#navajanje-gradiva","text":"V besedlu BibTeX Karakati\u010d, S. in Fister Ml., I. (2022). Strojno u\u010denje: s Pythonom do prvega klasifikatorja. Maribor: Univerzitetna zalo\u017eba. doi: 10.18690/um.feri.1.2022 @book{karakatic_strojnoucenje_2022, title = \"Strojno u\u010denje: s Pythonom do prvega klasifikatorja.\", author = \"Karakati\u010d, Sa\u0161o and Fister Jr., Iztok\", year = 2022, publisher = \"Univerzitetna zalo\u017eba\", address = \"Maribor\", doi = \"10.18690/um.feri.1.2022\" }","title":"Navajanje gradiva"}]}